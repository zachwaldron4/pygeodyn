{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63a6e7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:27.283998Z",
     "start_time": "2024-12-02T23:39:26.701256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n"
     ]
    }
   ],
   "source": [
    "print('begin')\n",
    "scale_cadence = 3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "from pygeodyn.pygeodyn_plot_scalingfactors import *\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fedd53b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:27.293165Z",
     "start_time": "2024-12-02T23:39:27.286531Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_cleanformat_axes(fig, font_dict):\n",
    "    rownum, colnum = fig._get_subplot_rows_columns()\n",
    "\n",
    "    for i in rownum:\n",
    "        if len(rownum)==1:\n",
    "            L_ticklabel = True\n",
    "        else:\n",
    "            if i < len(rownum):\n",
    "                L_ticklabel = False\n",
    "            else:\n",
    "                L_ticklabel = True\n",
    "        fig.update_xaxes(### LINE at axis border\n",
    "                          showline=True,\n",
    "                          showticklabels=L_ticklabel,\n",
    "    #                       tickformat= '%m/%d',\n",
    "                          linecolor='black',\n",
    "                          linewidth=1,\n",
    "                         ### Major ticks\n",
    "                          ticks='inside',\n",
    "                          tickfont=font_dict,\n",
    "                          mirror=True,\n",
    "    #                       tickwidth=2,\n",
    "    #                       ticklen=9,\n",
    "                          tickcolor='grey',\n",
    "    #                       tick0=\"2018-11-9\" ,\n",
    "    #                       dtick=86400000.0*1,    # milliseconds in a day, every 7 days\n",
    "                          #### Minor Ticks\n",
    "                           minor=dict(\n",
    "                             dtick=86400000.0, # milliseconds in a day\n",
    "                             tickwidth=1,\n",
    "                             ticklen=4,\n",
    "                             tickcolor='grey',\n",
    "                             ticks='inside'),\n",
    "                          ### GRID\n",
    "                           gridcolor='gainsboro',\n",
    "                           gridwidth=1,\n",
    "                           layer='above traces',\n",
    "                           tickangle=0,\n",
    "                           row=i, col=1)\n",
    "        fig.update_yaxes(showline=True,      # add line at x=0\n",
    "                             showticklabels=True,\n",
    "                             linecolor='black',  # line color\n",
    "                             linewidth=1,        # line size\n",
    "                         ticks='inside',     # ticks outside axis\n",
    "                         tickfont=font_dict, # tick label font\n",
    "                         mirror='allticks',  # add ticks to top/right axes\n",
    "                         tickwidth=1,      # tick width\n",
    "                         tickcolor='black',  # tick color\n",
    "                         gridcolor='gainsboro',\n",
    "                         gridwidth=1,\n",
    "                         layer='above traces',\n",
    "                         row=i, col=1)\n",
    "    return(fig)\n",
    "\n",
    "from pygeodyn.pygeodyn_plot_scalingfactors import *\n",
    "\n",
    "coldict = {}\n",
    "coldict['msis2']     = \"#2ca02c\"  # 'tab:green'\n",
    "coldict['dtm2020_o'] = \"#d62728\"  # 'tab:red'\n",
    "coldict['jb2008']    = \"orange\"   \n",
    "coldict['hasdm_oc']  = \"#1f77b4\"     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a8662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T21:51:24.227143Z",
     "start_time": "2024-09-24T21:51:24.221396Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb163e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f73b32e2",
   "metadata": {},
   "source": [
    "## GFO Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3ce2b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:27.300962Z",
     "start_time": "2024-12-02T23:39:27.296256Z"
    }
   },
   "outputs": [],
   "source": [
    "month_list = ['jun']#,\n",
    "gfo_acc_file = 'gfo_acc_june.csv'\n",
    "ice_pod_file = 'icesat2_june.csv'\n",
    "sp104_pod_file = 'spire104_june.csv'\n",
    "sp113_pod_file = 'spire113_june.csv'\n",
    "sp106_pod_file = 'spire106_june.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e705c797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:31.918874Z",
     "start_time": "2024-12-02T23:39:27.304069Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:40: SyntaxWarning:\n",
      "\n",
      "invalid escape sequence '\\s'\n",
      "\n",
      "<>:40: SyntaxWarning:\n",
      "\n",
      "invalid escape sequence '\\s'\n",
      "\n",
      "/tmp/ipykernel_9984/3599698310.py:40: SyntaxWarning:\n",
      "\n",
      "invalid escape sequence '\\s'\n",
      "\n",
      "  gfo_acc_june.csv.bz2: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "must bunzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfo_acc_june.csv  exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  gfo_acc_june.csv:  3.322:1,  2.408 bits/byte, 69.90% saved, 24257014 in, 7302431 out.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(gfo_acc_file) :\n",
    "    print(gfo_acc_file, ' exists' )\n",
    "\n",
    "elif os.path.exists(gfo_acc_file+'.bz2') :\n",
    "    \n",
    "    print('must bunzip')\n",
    "    os.system('bunzip2 -v '+ gfo_acc_file+'.bz2')\n",
    "# gfo_6month.csv.bz2\n",
    "else:\n",
    "    gfo_bigdf = {}\n",
    "\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        if month=='jun':\n",
    "            m_num = 6\n",
    "            y_num = 2022\n",
    "\n",
    "        path_gfo     = \"/data/SatDragModelValidation/data/inputs/raw_inputdata/data_GRACEFO_accelerometer/\"\n",
    "        filename_gfo = f'GC_DNS_ACC_{y_num}_{m_num:02d}_v02c.txt'\n",
    "\n",
    "        datapath_gfo     = path_gfo + filename_gfo\n",
    "\n",
    "        headers = [\n",
    "            'date',        #         Date (yyyy-mm-dd)\n",
    "            'time',        #         Time (hh:mm:ss.sss)\n",
    "            'time_system', #         Time system: UTC or GPS (differs per mission)\n",
    "            'alt',         #  f10.3  Altitude (m), GRS80\n",
    "            'lon',         #   f8.3  Geodetic longitude (deg), GRS80\n",
    "            'lat',         #   f7.3  Geodetic latitude (deg), GRS80\n",
    "            'lst',         #   f6.3  Local solar time (h)\n",
    "            'arglat',      #   f7.3  Argument of latitude (deg)\n",
    "            'dens_x',      #  e15.8  Density derived from accelerometer measurements (kg/m3)\n",
    "            'dens_mean',   #  e15.8  Running orbit average of density (kg/m3)\n",
    "            'flag_den',    #   f4.1  Flag for density: 0 = nominal data, 1 = anomalous data (-)\n",
    "            'flag_orbitavg',#   f4.1  Flag for running orbit average density: 0 = nominal data, 1 = anomalous data (-)\n",
    "                    ]\n",
    "\n",
    "        print(month)\n",
    "        gfo_bigdf[month] = pd.read_csv(datapath_gfo, \n",
    "                skiprows = 38, \n",
    "                sep = '\\s+',\n",
    "                names = headers,\n",
    "                           )\n",
    "\n",
    "        #Convert date from GPS to UTC\n",
    "        date = pd.to_datetime(gfo_bigdf[month]['date']  \\\n",
    "                            + gfo_bigdf[month]['time'], \\\n",
    "                                    format='%Y-%m-%d%H:%M:%S.000') - pd.to_timedelta(18,'s')\n",
    "\n",
    "        gfo_bigdf[month].insert(0, 'Date', date)\n",
    "\n",
    "        del gfo_bigdf[month]['date'], gfo_bigdf[month]['time'], date\n",
    "        del gfo_bigdf[month]['time_system']\n",
    "        del gfo_bigdf[month]['dens_mean']\n",
    "        del gfo_bigdf[month]['flag_den']\n",
    "        del gfo_bigdf[month]['flag_orbitavg']\n",
    "\n",
    "\n",
    "    #     resid_meas_summry = pd.concat([ gfo_bigdf, resid_meas_summry_iter])\n",
    "\n",
    "    gfo_acc_df = pd.concat([ gfo_bigdf[month] for month in month_list]  )\n",
    "    gfo_acc_df = gfo_acc_df.reset_index(drop=True)\n",
    "\n",
    "    \n",
    "\n",
    "if os.path.exists(gfo_acc_file) :\n",
    "    print(gfo_acc_file, ' exists' )\n",
    "    \n",
    "    \n",
    "    gfo_acc_df = pd.read_csv(gfo_acc_file, \n",
    "                    sep = ',',\n",
    "                    )\n",
    "\n",
    "    os.system('bzip2 -v '+ gfo_acc_file)\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(f\"---Calculating Grace-FO normalization\")\n",
    "    D500_gfo = normalize_density_msis2( gfo_acc_df , 'GRACE-FO Acc', 500)\n",
    "    gfo_acc_df['D500_gfo'] = D500_gfo\n",
    "    #### save to a csv\n",
    "    gfo_acc_df.to_csv(gfo_acc_file, index=False)  \n",
    "\n",
    "    \n",
    "\n",
    "# gfo_df2 =     gfo_df.query(\"Date >= '2018-10-14' and Date < '2018-12-30'\")\n",
    "# del gfo_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "715a3f69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:31.927452Z",
     "start_time": "2024-12-02T23:39:31.923809Z"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a4026",
   "metadata": {},
   "source": [
    "## ICEsat-2 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b935df38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.529836Z",
     "start_time": "2024-12-02T23:39:31.930125Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m run_dict\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imonth,month \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(month_list):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m run_list:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsis2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     26\u001b[0m             run_dict[month\u001b[38;5;241m+\u001b[39mi]\u001b[38;5;241m=\u001b[39m{}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_list' is not defined"
     ]
    }
   ],
   "source": [
    "if os.path.exists(ice_pod_file):\n",
    "    print(ice_pod_file, ' exists' )\n",
    "\n",
    "    \n",
    "elif os.path.exists(ice_pod_file+'.bz2') :\n",
    "    \n",
    "    print('must bunzip')\n",
    "    os.system('bunzip2 -v '+ ice_pod_file+'.bz2')\n",
    "    \n",
    "else:\n",
    "\n",
    "\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from gc import collect as gc_collect\n",
    "    import pickle \n",
    "    from datetime import datetime,timedelta\n",
    "\n",
    "    scale_cadence = 3\n",
    "\n",
    "    dir_modeldat='/data/SatDragModelValidation/data/inputs/atmos_models'\n",
    "    run_dict={}\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        for i in run_list:\n",
    "            if i =='msis2':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 5\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='dtm2020_o':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 3\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='jb2008':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 1\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='hasdm_oc':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 2 \n",
    "                run_dict[month+i]['model_path'] = dir_modeldat+'/hasdm/sethasdm_orbitclouds' #HASDM_OrbitCloud_2018313.01.csv\n",
    "\n",
    "    print(run_dict)\n",
    "\n",
    "\n",
    "\n",
    "    dir_save    =  '/data/SatDragModelValidation/data/outputs_clean/'\\\n",
    "                 + 'icesat2/O2R2023_longimeperiod/ScaledDensitiesJune2022/'\n",
    "    obj = {}\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        for i,model in enumerate(run_list):\n",
    "            pickleName = f'_{month}_DRIA_scale{scale_cadence}.pkl'\n",
    "\n",
    "            ### Load the data if the pickles exist\n",
    "            print()\n",
    "            print()\n",
    "            gc_collect()\n",
    "\n",
    "            pickle_file = dir_save+model+pickleName\n",
    "            \n",
    "            obj[month+model] = pd.read_pickle(pickle_file)\n",
    "\n",
    "#             filehandler = open(pickle_file, 'rb') \n",
    "#             obj[month+model] = pickle.load(filehandler)\n",
    "#             filehandler.close()\n",
    "            print('Loaded data from pickle... ',  month+model)\n",
    "\n",
    "\n",
    "    ### Save space if doing density retrieval\n",
    "    for model in run_dict.keys():\n",
    "        del obj[model]['OrbitResids']\n",
    "        del obj[model]['Trajectory_orbfil']\n",
    "\n",
    "    gc_collect()\n",
    "    \n",
    "    \n",
    "    ## MAKE SCALING FACTORS\n",
    "    \n",
    "    satid = 1807001\n",
    "    wgts = {}\n",
    "\n",
    "    for model in run_dict.keys():\n",
    "        wgts[model] = {}\n",
    "        ScalingFactors  = []\n",
    "        ScalingFactor_times = []\n",
    "\n",
    "        for ii,arc in enumerate(obj[model]['global_params']['arc_input']):\n",
    "            epochstart = obj[model]['global_params']['prms']['epoch_start'][ii]\n",
    "            hrs = pd.to_datetime(epochstart, format='%Y-%m-%d %H:%M:%S').hour\n",
    "            frachours =(hrs/24)\n",
    "            #\n",
    "            if len(arc) == 9:\n",
    "                maneuv_indicator = arc[8]\n",
    "            else:\n",
    "                maneuv_indicator = ''\n",
    "            arc_type = obj[model]['global_params']['prms']['arc_type']\n",
    "            if arc_type == \"Nominal30hr_and_AB\":\n",
    "                arc_name =arc[:8]+ maneuv_indicator\n",
    "            else:\n",
    "                arc_name =arc[:8]+('%.3f'%frachours).lstrip('0')+ maneuv_indicator\n",
    "            ### Collect the weights for the ensemble average\n",
    "            inv_rms          = 1/obj[model]['Statistics'][arc_name]['T_RMS'].values[0]\n",
    "            wgts[model][arc_name] = inv_rms#/sum_wgts\n",
    "\n",
    "            iters = int(obj[model]['run_parameters'+arc_name]['total_iterations']) \n",
    "            for iit, itime in enumerate(obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'].keys()):\n",
    "                if iit == 0 or iit==9:\n",
    "                    pass\n",
    "                else:\n",
    "                    CURRENT_VALUE = obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'][itime]['CURRENT_VALUE']\n",
    "                    APRIORI_VALUE = obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'][itime]['APRIORI_VALUE']\n",
    "                    ScalingFactors.append(CURRENT_VALUE/APRIORI_VALUE)\n",
    "                    ScalingFactor_times.append(itime)\n",
    "        run_dict[model]['ScalingFactor_times'] = ScalingFactor_times\n",
    "        run_dict[model]['ScalingFactors']      = ScalingFactors\n",
    "        run_dict[model]['Weight'] = wgts[model]\n",
    "\n",
    "\n",
    "\n",
    "    ###### Scale the densities\n",
    "    models_dens = {}\n",
    "    for monthmodel in run_dict.keys():\n",
    "        print(f\"---Making continuous scaled rho for {monthmodel}\")\n",
    "        models_dens[monthmodel] = get_continuous_scaled_densities(obj, run_dict, monthmodel, scale_cadence)\n",
    "\n",
    "        del obj[monthmodel]\n",
    "        \n",
    "    gc_collect()               \n",
    "    ## Retrieve scaled ensemble weighted average\n",
    "    ##     'Rho_x' denotes the ensemble weighted avg\n",
    "    print(f\"---Making ensemble avg\")\n",
    "#     for monthmodel in run_dict.keys():\n",
    "#         month = monthmodel[:3]\n",
    "    models_dens =  calc_rho_ScaledEnsembleWgtAvg(models_dens, run_dict, month_list, run_list)\n",
    "\n",
    "\n",
    "    ### Clear up space \n",
    "    del obj\n",
    "    for monthmodel in run_dict.keys():\n",
    "        print(monthmodel)\n",
    "        if 'Rho_x' not in monthmodel:\n",
    "            del models_dens[monthmodel]\n",
    "\n",
    "    gc_collect()      \n",
    "    \n",
    "    \n",
    "    \n",
    "if os.path.exists(ice_pod_file) :\n",
    "    print(ice_pod_file, ' exists' )\n",
    "    \n",
    "    ice_pod_df = pd.read_csv(ice_pod_file, \n",
    "                    sep = ',',\n",
    "                    )\n",
    "\n",
    "#     os.system('bzip2 -v '+ gfo_pod_df)\n",
    "\n",
    "else:\n",
    "    \n",
    "    ice_pod_bigdf = {}\n",
    "    print(f\"---Calculating ICE_pod normalization\")\n",
    "    \n",
    "    for imonth,month in enumerate(month_list):\n",
    "        print(month)\n",
    "        \n",
    "        D500_ice_pod = normalize_density_msis2( models_dens[month+'Rho_x'], 'ICESat-2', 500)\n",
    "        models_dens[month+'Rho_x']['D500_ice_pod'] = D500_ice_pod\n",
    "        \n",
    "        ice_pod_bigdf[month] = pd.DataFrame.from_dict(models_dens[month+'Rho_x'])\n",
    "        \n",
    "        del models_dens[month+'Rho_x']\n",
    "\n",
    "    print('--concatenating and saving')\n",
    "\n",
    "    ice_pod_df = pd.concat([ ice_pod_bigdf[month] for month in month_list]  )\n",
    "    ice_pod_df = ice_pod_df.reset_index(drop=True)\n",
    "    ice_pod_df.to_csv(ice_pod_file, index=False)  \n",
    "\n",
    "    ### Need to set this up to only write to file once. and have that write include the full dataset. Maybe concat the dataframes by month first\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6ace9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.534607Z",
     "start_time": "2024-12-02T23:39:32.534589Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datetime import datetime,timedelta\n",
    "# import numpy as np\n",
    "\n",
    "# timestart = pd.to_datetime('2022-06-05 00:00:00', format='%Y-%m-%d %H:%M:%S')\n",
    "# timeend   = pd.to_datetime('2022-06-08 00:00:00', format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# arcs_yyyyddd         = []\n",
    "# epochstart = []\n",
    "# epochstop   = []\n",
    "\n",
    "# itime = timestart\n",
    "# while itime < timeend:\n",
    "               \n",
    "#     itime_0 = itime\n",
    "#     itime = itime + pd.to_timedelta(24,'h')\n",
    "    \n",
    "#     arcs_yyyyddd.append(itime_0.strftime('%Y.%j'))\n",
    "#     epochstart.append(itime_0.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "#     epochstop.append(itime.strftime(  '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# #     print(' ',itime_0.strftime('%Y.%j'),itime_0, ' to ', itime)        \n",
    "\n",
    "\n",
    "# ##### update the epoch start to be 3-hours before and epoch end to be 3 hours later\n",
    "# input_arcs       = []\n",
    "# input_epochstart = []\n",
    "# input_epochstop   = []\n",
    "\n",
    "\n",
    "# for i in epochstart:\n",
    "#     itime = pd.to_datetime(i) - pd.to_timedelta(3,'h')\n",
    "#     input_epochstart.append(itime.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# for i in epochstop:\n",
    "#     itime = pd.to_datetime(i) + pd.to_timedelta(3,'h')\n",
    "#     input_epochstop.append(itime.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb79ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:16:53.859807Z",
     "start_time": "2024-09-20T18:16:53.853602Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcd0d870",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9c077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.536003Z",
     "start_time": "2024-12-02T23:39:32.535986Z"
    }
   },
   "outputs": [],
   "source": [
    "from pygeodyn.PYGEODYN import Pygeodyn\n",
    "import datetime\n",
    "\n",
    "\n",
    "scaling_cadence = 3\n",
    "scale_cadence = scaling_cadence\n",
    "index_buffarc_start = 0\n",
    "run_list = [\n",
    "                'jb2008',\n",
    "                'msis2',\n",
    "                'dtm2020_o',\n",
    "#                 'hasdm_oc'\n",
    "           ]\n",
    "\n",
    "month_list = ['jun']\n",
    "\n",
    "dir_modeldat='/data/SatDragModelValidation/data/inputs/atmos_models'\n",
    "run_dict={}\n",
    "for i in run_list:\n",
    "    if i =='msis2':\n",
    "        run_dict[i]={}\n",
    "        run_dict[i]['num'] = 5\n",
    "        run_dict[i]['model_path'] = None\n",
    "    if i =='dtm2020_o':\n",
    "        run_dict[i]={}\n",
    "        run_dict[i]['num'] = 3\n",
    "        run_dict[i]['model_path'] = None\n",
    "    if i =='jb2008':\n",
    "        run_dict[i]={}\n",
    "        run_dict[i]['num'] = 1\n",
    "        run_dict[i]['model_path'] = None\n",
    "    if i =='hasdm_oc':\n",
    "        run_dict[i]={}\n",
    "        run_dict[i]['num'] = 2 \n",
    "        run_dict[i]['model_path'] = dir_modeldat+'/hasdm/sethasdm_orbitclouds' #HASDM_OrbitCloud_2018313.01.csv\n",
    "            \n",
    "print(run_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985aefae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.537593Z",
     "start_time": "2024-12-02T23:39:32.537577Z"
    }
   },
   "outputs": [],
   "source": [
    "# run_dict['jb2008']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4e8bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T18:42:36.566726Z",
     "start_time": "2024-09-25T18:42:36.566717Z"
    },
    "scrolled": true
   },
   "source": [
    "## Spire POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8473bea1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T20:33:56.155890Z",
     "start_time": "2024-10-23T20:33:56.152753Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa8bd4e8",
   "metadata": {},
   "source": [
    "### Spire 113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe196c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.540926Z",
     "start_time": "2024-12-02T23:39:32.540910Z"
    }
   },
   "outputs": [],
   "source": [
    "satnum = '113'\n",
    "\n",
    "if os.path.exists(sp113_pod_file):\n",
    "    print(sp113_pod_file, ' exists' )\n",
    "\n",
    "    \n",
    "elif os.path.exists(sp113_pod_file+'.bz2') :\n",
    "    \n",
    "    print('must bunzip')\n",
    "    os.system('bunzip2 -v '+ sp113_pod_file+'.bz2')\n",
    "    \n",
    "else:\n",
    "\n",
    "\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from gc import collect as gc_collect\n",
    "    import pickle \n",
    "    from datetime import datetime,timedelta\n",
    "\n",
    "\n",
    "    dir_modeldat='/data/SatDragModelValidation/data/inputs/atmos_models'\n",
    "    run_dict={}\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        for i in run_list:\n",
    "            if i =='msis2':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 5\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='dtm2020_o':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 3\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='jb2008':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 1\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='hasdm_oc':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 2 \n",
    "                run_dict[month+i]['model_path'] = dir_modeldat+'/hasdm/sethasdm_orbitclouds' \n",
    "\n",
    "    print(run_dict)\n",
    "\n",
    "\n",
    "\n",
    "    dir_save    =  '/data/SatDragModelValidation/data/outputs_clean/'\\\n",
    "                 + 'spire'+satnum+'/'\n",
    "    obj = {}\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        for i,model in enumerate(run_list):\n",
    "            pickleName = f'_DRIA_scale{scale_cadence}.pkl'\n",
    "\n",
    "            ### Load the data if the pickles exist\n",
    "            print()\n",
    "            print()\n",
    "            gc_collect()\n",
    "\n",
    "            pickle_file = dir_save+model+pickleName\n",
    "            \n",
    "            obj[month+model] = pd.read_pickle(pickle_file)\n",
    "\n",
    "#             filehandler = open(pickle_file, 'rb') \n",
    "#             obj[month+model] = pickle.load(filehandler)\n",
    "#             filehandler.close()\n",
    "            print('Loaded data from pickle... ',  month+model)\n",
    "\n",
    "\n",
    "    ### Save space if doing density retrieval\n",
    "    for model in run_dict.keys():\n",
    "        del obj[model]['OrbitResids']\n",
    "        del obj[model]['Trajectory_orbfil']\n",
    "\n",
    "    gc_collect()\n",
    "    \n",
    "    \n",
    "    ## MAKE SCALING FACTORS\n",
    "    \n",
    "    if int(satnum) == 106:\n",
    "        satid=1903826\n",
    "    elif int(satnum) == 104:\n",
    "        satid = 1903812\n",
    "    elif int(satnum) == 113:\n",
    "        satid = 1903824\n",
    "\n",
    "    else:\n",
    "        print('Add satellite Numbers')\n",
    "\n",
    "        import sys\n",
    "        sys.exit(0)\n",
    "\n",
    "    \n",
    "    wgts = {}\n",
    "\n",
    "    for model in run_dict.keys():\n",
    "        wgts[model] = {}\n",
    "        ScalingFactors  = []\n",
    "        ScalingFactor_times = []\n",
    "\n",
    "        for ii,arc in enumerate(obj[model]['global_params']['arc_input']):\n",
    "            epochstart = obj[model]['global_params']['prms']['epoch_start'][ii]\n",
    "            hrs = pd.to_datetime(epochstart, format='%Y-%m-%d %H:%M:%S').hour\n",
    "            frachours =(hrs/24)\n",
    "            #\n",
    "            if len(arc) == 9:\n",
    "                maneuv_indicator = arc[8]\n",
    "            else:\n",
    "                maneuv_indicator = ''\n",
    "            arc_type = obj[model]['global_params']['prms']['arc_type']\n",
    "            if arc_type == \"Nominal30hr_and_AB\":\n",
    "                arc_name =arc[:8]+ maneuv_indicator\n",
    "            else:\n",
    "                arc_name =arc[:8]+('%.3f'%frachours).lstrip('0')+ maneuv_indicator\n",
    "            ### Collect the weights for the ensemble average\n",
    "            inv_rms          = 1/obj[model]['Statistics'][arc_name]['T_RMS'].values[0]\n",
    "            wgts[model][arc_name] = inv_rms#/sum_wgts\n",
    "\n",
    "            iters = int(obj[model]['run_parameters'+arc_name]['total_iterations']) \n",
    "            for iit, itime in enumerate(obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'].keys()):\n",
    "                if iit == 0 or iit==9:\n",
    "#                     print(itime)\n",
    "                    pass\n",
    "                else:\n",
    "                    CURRENT_VALUE = obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'][itime]['CURRENT_VALUE']\n",
    "                    APRIORI_VALUE = obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'][itime]['APRIORI_VALUE']\n",
    "                    ScalingFactors.append(CURRENT_VALUE/APRIORI_VALUE)\n",
    "                    ScalingFactor_times.append(itime)\n",
    "        if int(satnum) == 113:\n",
    "            data = np.sort(ScalingFactors)\n",
    "\n",
    "            ### Calculate the first quartile (Q1) and third quartile (Q3)\n",
    "            Q1 = np.percentile(data, 25)\n",
    "            Q3 = np.percentile(data, 75)\n",
    "            median = np.percentile(data, 50)\n",
    "            ### Calculate the IQR\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            ### Define a threshold for identifying outliers\n",
    "            threshold = 1                                ## You can adjust this threshold as needed\n",
    "\n",
    "            ### Calculate the lower and upper bounds for outliers\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "            df = pd.DataFrame(data={'x':ScalingFactor_times,\n",
    "                                    'y':ScalingFactors})\n",
    "            filtered_df = df[(df['y'] >= lower_bound) & (df['y'] <= upper_bound)]\n",
    "\n",
    "            ScalingFactor_times = pd.to_datetime(filtered_df['x'].values)\n",
    "            ScalingFactors      = filtered_df['y'].values\n",
    "\n",
    "        run_dict[model]['ScalingFactor_times'] = ScalingFactor_times\n",
    "        run_dict[model]['ScalingFactors']      = ScalingFactors\n",
    "        run_dict[model]['Weight'] = wgts[model]\n",
    "        \n",
    "\n",
    "    ###### Scale the densities\n",
    "    models_dens = {}\n",
    "    for monthmodel in run_dict.keys():\n",
    "        print(f\"---Making continuous scaled rho for {monthmodel}\")\n",
    "        models_dens[monthmodel] = get_continuous_scaled_densities(obj, run_dict, monthmodel, scale_cadence)\n",
    "\n",
    "#         del obj[monthmodel]\n",
    "        \n",
    "    gc_collect()               \n",
    "    ## Retrieve scaled ensemble weighted average\n",
    "    ##     'Rho_x' denotes the ensemble weighted avg\n",
    "    print(f\"---Making ensemble avg\")\n",
    "#     for monthmodel in run_dict.keys():\n",
    "#         month = monthmodel[:3]\n",
    "    models_dens =  calc_rho_ScaledEnsembleWgtAvg(models_dens, run_dict, month_list, run_list)\n",
    "\n",
    "\n",
    "    ### Clear up space \n",
    "#     del obj\n",
    "    for monthmodel in run_dict.keys():\n",
    "        print(monthmodel)\n",
    "        if 'Rho_x' not in monthmodel:\n",
    "            del models_dens[monthmodel]\n",
    "\n",
    "    gc_collect()      \n",
    "    \n",
    "    \n",
    "    \n",
    "if os.path.exists(sp113_pod_file) :\n",
    "    print(sp113_pod_file, ' exists' )\n",
    "    \n",
    "    sp113_pod_df = pd.read_csv(sp113_pod_file, \n",
    "                    sep = ',',\n",
    "                    )\n",
    "\n",
    "#     os.system('bzip2 -v '+ gfo_pod_df)\n",
    "\n",
    "else:\n",
    "    \n",
    "    sp113_pod_bigdf = {}\n",
    "    print(f\"---Calculating Spire {satnum} pod normalization\")\n",
    "    \n",
    "    for imonth,month in enumerate(month_list):\n",
    "        print(month)\n",
    "        \n",
    "        D500_sp113_pod = normalize_density_msis2( models_dens[month+'Rho_x'], 'Spire '+satnum, 500)\n",
    "        models_dens[month+'Rho_x']['D500_sp113_pod'] = D500_sp113_pod\n",
    "        \n",
    "        sp113_pod_bigdf[month] = pd.DataFrame.from_dict(models_dens[month+'Rho_x'])\n",
    "        \n",
    "        del models_dens[month+'Rho_x']\n",
    "\n",
    "    print('--concatenating and saving')\n",
    "\n",
    "    sp113_pod_df = pd.concat([ sp113_pod_bigdf[month] for month in month_list]  )\n",
    "    sp113_pod_df = sp113_pod_df.reset_index(drop=True)\n",
    "    sp113_pod_df.to_csv(sp113_pod_file, index=False)  \n",
    "\n",
    "    ### Need to set this up to only write to file once. and have that write include the full dataset. Maybe concat the dataframes by month first\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba463ed",
   "metadata": {},
   "source": [
    "### Spire 106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc8183",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.542392Z",
     "start_time": "2024-12-02T23:39:32.542377Z"
    }
   },
   "outputs": [],
   "source": [
    "satnum = '106'\n",
    "\n",
    "if os.path.exists(sp106_pod_file):\n",
    "    print(sp106_pod_file, ' exists' )\n",
    "\n",
    "    \n",
    "elif os.path.exists(sp106_pod_file+'.bz2') :\n",
    "    \n",
    "    print('must bunzip')\n",
    "    os.system('bunzip2 -v '+ sp106_pod_file+'.bz2')\n",
    "    \n",
    "else:\n",
    "\n",
    "\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from gc import collect as gc_collect\n",
    "    import pickle \n",
    "    from datetime import datetime,timedelta\n",
    "\n",
    "\n",
    "    dir_modeldat='/data/SatDragModelValidation/data/inputs/atmos_models'\n",
    "    run_dict={}\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        for i in run_list:\n",
    "            if i =='msis2':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 5\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='dtm2020_o':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 3\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='jb2008':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 1\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='hasdm_oc':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 2 \n",
    "                run_dict[month+i]['model_path'] = dir_modeldat+'/hasdm/sethasdm_orbitclouds' \n",
    "\n",
    "    print(run_dict)\n",
    "\n",
    "\n",
    "\n",
    "    dir_save    =  '/data/SatDragModelValidation/data/outputs_clean/'\\\n",
    "                 + 'spire'+satnum+'/'\n",
    "    obj = {}\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        for i,model in enumerate(run_list):\n",
    "            pickleName = f'_DRIA_scale{scale_cadence}.pkl'\n",
    "\n",
    "            ### Load the data if the pickles exist\n",
    "            print()\n",
    "            print()\n",
    "            gc_collect()\n",
    "\n",
    "            pickle_file = dir_save+model+pickleName\n",
    "            \n",
    "            obj[month+model] = pd.read_pickle(pickle_file)\n",
    "\n",
    "#             filehandler = open(pickle_file, 'rb') \n",
    "#             obj[month+model] = pickle.load(filehandler)\n",
    "#             filehandler.close()\n",
    "            print('Loaded data from pickle... ',  month+model)\n",
    "\n",
    "\n",
    "    ### Save space if doing density retrieval\n",
    "    for model in run_dict.keys():\n",
    "        del obj[model]['OrbitResids']\n",
    "        del obj[model]['Trajectory_orbfil']\n",
    "\n",
    "    gc_collect()\n",
    "    \n",
    "    \n",
    "    ## MAKE SCALING FACTORS\n",
    "    \n",
    "    if int(satnum) == 106:\n",
    "        satid=1903826\n",
    "    elif int(satnum) == 104:\n",
    "        satid = 1903812\n",
    "    elif int(satnum) == 113:\n",
    "        satid = 1903824\n",
    "\n",
    "    else:\n",
    "        print('Add satellite Numbers')\n",
    "\n",
    "        import sys\n",
    "        sys.exit(0)\n",
    "\n",
    "    \n",
    "    wgts = {}\n",
    "\n",
    "    for model in run_dict.keys():\n",
    "        wgts[model] = {}\n",
    "        ScalingFactors  = []\n",
    "        ScalingFactor_times = []\n",
    "\n",
    "        for ii,arc in enumerate(obj[model]['global_params']['arc_input']):\n",
    "            epochstart = obj[model]['global_params']['prms']['epoch_start'][ii]\n",
    "            hrs = pd.to_datetime(epochstart, format='%Y-%m-%d %H:%M:%S').hour\n",
    "            frachours =(hrs/24)\n",
    "            #\n",
    "            if len(arc) == 9:\n",
    "                maneuv_indicator = arc[8]\n",
    "            else:\n",
    "                maneuv_indicator = ''\n",
    "            arc_type = obj[model]['global_params']['prms']['arc_type']\n",
    "            if arc_type == \"Nominal30hr_and_AB\":\n",
    "                arc_name =arc[:8]+ maneuv_indicator\n",
    "            else:\n",
    "                arc_name =arc[:8]+('%.3f'%frachours).lstrip('0')+ maneuv_indicator\n",
    "            ### Collect the weights for the ensemble average\n",
    "            inv_rms          = 1/obj[model]['Statistics'][arc_name]['T_RMS'].values[0]\n",
    "            wgts[model][arc_name] = inv_rms#/sum_wgts\n",
    "\n",
    "            iters = int(obj[model]['run_parameters'+arc_name]['total_iterations']) \n",
    "            for iit, itime in enumerate(obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'].keys()):\n",
    "                if iit == 0 or iit==9:\n",
    "#                     print(itime)\n",
    "                    pass\n",
    "                else:\n",
    "                    CURRENT_VALUE = obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'][itime]['CURRENT_VALUE']\n",
    "                    APRIORI_VALUE = obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'][itime]['APRIORI_VALUE']\n",
    "                    ScalingFactors.append(CURRENT_VALUE/APRIORI_VALUE)\n",
    "                    ScalingFactor_times.append(itime)\n",
    "        if int(satnum) == 106:\n",
    "            data = np.sort(ScalingFactors)\n",
    "\n",
    "            ### Calculate the first quartile (Q1) and third quartile (Q3)\n",
    "            Q1 = np.percentile(data, 25)\n",
    "            Q3 = np.percentile(data, 75)\n",
    "            median = np.percentile(data, 50)\n",
    "            ### Calculate the IQR\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            ### Define a threshold for identifying outliers\n",
    "            threshold = 1                                ## You can adjust this threshold as needed\n",
    "\n",
    "            ### Calculate the lower and upper bounds for outliers\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "            df = pd.DataFrame(data={'x':ScalingFactor_times,\n",
    "                                    'y':ScalingFactors})\n",
    "            filtered_df = df[(df['y'] >= lower_bound) & (df['y'] <= upper_bound)]\n",
    "\n",
    "            ScalingFactor_times = pd.to_datetime(filtered_df['x'].values)\n",
    "            ScalingFactors      = filtered_df['y'].values\n",
    "\n",
    "        run_dict[model]['ScalingFactor_times'] = ScalingFactor_times\n",
    "        run_dict[model]['ScalingFactors']      = ScalingFactors\n",
    "        run_dict[model]['Weight'] = wgts[model]\n",
    "        \n",
    "\n",
    "    ###### Scale the densities\n",
    "    models_dens = {}\n",
    "    for monthmodel in run_dict.keys():\n",
    "        print(f\"---Making continuous scaled rho for {monthmodel}\")\n",
    "        models_dens[monthmodel] = get_continuous_scaled_densities(obj, run_dict, monthmodel, scale_cadence)\n",
    "\n",
    "#         del obj[monthmodel]\n",
    "        \n",
    "    gc_collect()               \n",
    "    ## Retrieve scaled ensemble weighted average\n",
    "    ##     'Rho_x' denotes the ensemble weighted avg\n",
    "    print(f\"---Making ensemble avg\")\n",
    "#     for monthmodel in run_dict.keys():\n",
    "#         month = monthmodel[:3]\n",
    "    models_dens =  calc_rho_ScaledEnsembleWgtAvg(models_dens, run_dict, month_list, run_list)\n",
    "\n",
    "\n",
    "    ### Clear up space \n",
    "#     del obj\n",
    "    for monthmodel in run_dict.keys():\n",
    "        print(monthmodel)\n",
    "        if 'Rho_x' not in monthmodel:\n",
    "            del models_dens[monthmodel]\n",
    "\n",
    "    gc_collect()      \n",
    "    \n",
    "    \n",
    "    \n",
    "if os.path.exists(sp106_pod_file) :\n",
    "    print(sp106_pod_file, ' exists' )\n",
    "    \n",
    "    sp106_pod_df = pd.read_csv(sp106_pod_file, \n",
    "                    sep = ',',\n",
    "                    )\n",
    "\n",
    "#     os.system('bzip2 -v '+ gfo_pod_df)\n",
    "\n",
    "else:\n",
    "    \n",
    "    sp106_pod_bigdf = {}\n",
    "    print(f\"---Calculating Spire {satnum} pod normalization\")\n",
    "    \n",
    "    for imonth,month in enumerate(month_list):\n",
    "        print(month)\n",
    "        \n",
    "        D500_sp106_pod = normalize_density_msis2( models_dens[month+'Rho_x'], 'Spire '+satnum, 500)\n",
    "        models_dens[month+'Rho_x']['D500_sp106_pod'] = D500_sp106_pod\n",
    "        \n",
    "        sp106_pod_bigdf[month] = pd.DataFrame.from_dict(models_dens[month+'Rho_x'])\n",
    "        \n",
    "        del models_dens[month+'Rho_x']\n",
    "\n",
    "    print('--concatenating and saving')\n",
    "\n",
    "    sp106_pod_df = pd.concat([ sp106_pod_bigdf[month] for month in month_list]  )\n",
    "    sp106_pod_df = sp106_pod_df.reset_index(drop=True)\n",
    "    sp106_pod_df.to_csv(sp106_pod_file, index=False)  \n",
    "\n",
    "    ### Need to set this up to only write to file once. and have that write include the full dataset. Maybe concat the dataframes by month first\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9612b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T21:15:18.200713Z",
     "start_time": "2024-10-23T21:15:18.197947Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55a3d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T21:15:18.830646Z",
     "start_time": "2024-10-23T21:15:18.812168Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47b09f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T21:15:08.958708Z",
     "start_time": "2024-10-23T21:15:08.958690Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f4df1bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T21:15:08.962178Z",
     "start_time": "2024-10-23T21:15:08.962158Z"
    }
   },
   "source": [
    "### Spire 104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d0277",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.544044Z",
     "start_time": "2024-12-02T23:39:32.544011Z"
    }
   },
   "outputs": [],
   "source": [
    "satnum = '104'\n",
    "if os.path.exists(sp104_pod_file):\n",
    "    print(sp104_pod_file, ' exists' )\n",
    "\n",
    "    \n",
    "elif os.path.exists(sp104_pod_file+'.bz2') :\n",
    "    \n",
    "    print('must bunzip')\n",
    "    os.system('bunzip2 -v '+ sp104_pod_file+'.bz2')\n",
    "    \n",
    "else:\n",
    "\n",
    "\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from gc import collect as gc_collect\n",
    "    import pickle \n",
    "    from datetime import datetime,timedelta\n",
    "\n",
    "\n",
    "    dir_modeldat='/data/SatDragModelValidation/data/inputs/atmos_models'\n",
    "    run_dict={}\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        for i in run_list:\n",
    "            if i =='msis2':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 5\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='dtm2020_o':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 3\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='jb2008':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 1\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='hasdm_oc':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 2 \n",
    "                run_dict[month+i]['model_path'] = dir_modeldat+'/hasdm/sethasdm_orbitclouds' #HASDM_OrbitCloud_2018313.01.csv\n",
    "\n",
    "    print(run_dict)\n",
    "\n",
    "\n",
    "\n",
    "    dir_save    =  '/data/SatDragModelValidation/data/outputs_clean/'\\\n",
    "                 + 'spire104/'\n",
    "    obj = {}\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        for i,model in enumerate(run_list):\n",
    "            pickleName = f'_DRIA_scale{scale_cadence}.pkl'\n",
    "\n",
    "            ### Load the data if the pickles exist\n",
    "            print()\n",
    "            print()\n",
    "            gc_collect()\n",
    "\n",
    "            pickle_file = dir_save+model+pickleName\n",
    "            \n",
    "            obj[month+model] = pd.read_pickle(pickle_file)\n",
    "\n",
    "#             filehandler = open(pickle_file, 'rb') \n",
    "#             obj[month+model] = pickle.load(filehandler)\n",
    "#             filehandler.close()\n",
    "            print('Loaded data from pickle... ',  month+model)\n",
    "\n",
    "\n",
    "    ### Save space if doing density retrieval\n",
    "    for model in run_dict.keys():\n",
    "        del obj[model]['OrbitResids']\n",
    "        del obj[model]['Trajectory_orbfil']\n",
    "\n",
    "    gc_collect()\n",
    "    \n",
    "    \n",
    "    ## MAKE SCALING FACTORS\n",
    "    \n",
    "    satid = 1903812\n",
    "    wgts = {}\n",
    "\n",
    "    for model in run_dict.keys():\n",
    "        wgts[model] = {}\n",
    "        ScalingFactors  = []\n",
    "        ScalingFactor_times = []\n",
    "\n",
    "        for ii,arc in enumerate(obj[model]['global_params']['arc_input']):\n",
    "            epochstart = obj[model]['global_params']['prms']['epoch_start'][ii]\n",
    "            hrs = pd.to_datetime(epochstart, format='%Y-%m-%d %H:%M:%S').hour\n",
    "            frachours =(hrs/24)\n",
    "            #\n",
    "            if len(arc) == 9:\n",
    "                maneuv_indicator = arc[8]\n",
    "            else:\n",
    "                maneuv_indicator = ''\n",
    "            arc_type = obj[model]['global_params']['prms']['arc_type']\n",
    "            if arc_type == \"Nominal30hr_and_AB\":\n",
    "                arc_name =arc[:8]+ maneuv_indicator\n",
    "            else:\n",
    "                arc_name =arc[:8]+('%.3f'%frachours).lstrip('0')+ maneuv_indicator\n",
    "            ### Collect the weights for the ensemble average\n",
    "            inv_rms          = 1/obj[model]['Statistics'][arc_name]['T_RMS'].values[0]\n",
    "            wgts[model][arc_name] = inv_rms#/sum_wgts\n",
    "\n",
    "            iters = int(obj[model]['run_parameters'+arc_name]['total_iterations']) \n",
    "            for iit, itime in enumerate(obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'].keys()):\n",
    "                if iit == 0 or iit==9:\n",
    "                    pass\n",
    "                else:\n",
    "                    CURRENT_VALUE = obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'][itime]['CURRENT_VALUE']\n",
    "                    APRIORI_VALUE = obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'][itime]['APRIORI_VALUE']\n",
    "                    ScalingFactors.append(CURRENT_VALUE/APRIORI_VALUE)\n",
    "                    ScalingFactor_times.append(itime)\n",
    "        if int(satnum) == 104:\n",
    "            data = np.sort(ScalingFactors)\n",
    "\n",
    "            ### Calculate the first quartile (Q1) and third quartile (Q3)\n",
    "            Q1 = np.percentile(data, 25)\n",
    "            Q3 = np.percentile(data, 75)\n",
    "            median = np.percentile(data, 50)\n",
    "            ### Calculate the IQR\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            ### Define a threshold for identifying outliers\n",
    "            threshold = 1                                ## You can adjust this threshold as needed\n",
    "\n",
    "            ### Calculate the lower and upper bounds for outliers\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "            df = pd.DataFrame(data={'x':ScalingFactor_times,\n",
    "                                    'y':ScalingFactors})\n",
    "            filtered_df = df[(df['y'] >= lower_bound) & (df['y'] <= upper_bound)]\n",
    "\n",
    "            ScalingFactor_times = pd.to_datetime(filtered_df['x'].values)\n",
    "            ScalingFactors      = filtered_df['y'].values\n",
    "\n",
    "        run_dict[model]['ScalingFactor_times'] = ScalingFactor_times\n",
    "        run_dict[model]['ScalingFactors']      = ScalingFactors\n",
    "        run_dict[model]['Weight'] = wgts[model]\n",
    "\n",
    "        run_dict[model]['ScalingFactor_times'] = ScalingFactor_times\n",
    "        run_dict[model]['ScalingFactors']      = ScalingFactors\n",
    "        run_dict[model]['Weight'] = wgts[model]\n",
    "\n",
    "\n",
    "\n",
    "    ###### Scale the densities\n",
    "    models_dens = {}\n",
    "    for monthmodel in run_dict.keys():\n",
    "        print(f\"---Making continuous scaled rho for {monthmodel}\")\n",
    "        models_dens[monthmodel] = get_continuous_scaled_densities(obj, run_dict, monthmodel, scale_cadence)\n",
    "\n",
    "        del obj[monthmodel]\n",
    "        \n",
    "    gc_collect()               \n",
    "    ## Retrieve scaled ensemble weighted average\n",
    "    ##     'Rho_x' denotes the ensemble weighted avg\n",
    "    print(f\"---Making ensemble avg\")\n",
    "#     for monthmodel in run_dict.keys():\n",
    "#         month = monthmodel[:3]\n",
    "    models_dens =  calc_rho_ScaledEnsembleWgtAvg(models_dens, run_dict, month_list, run_list)\n",
    "\n",
    "\n",
    "    ### Clear up space \n",
    "    del obj\n",
    "    for monthmodel in run_dict.keys():\n",
    "        print(monthmodel)\n",
    "        if 'Rho_x' not in monthmodel:\n",
    "            del models_dens[monthmodel]\n",
    "\n",
    "    gc_collect()      \n",
    "    \n",
    "    \n",
    "    \n",
    "if os.path.exists(sp104_pod_file) :\n",
    "    print(sp104_pod_file, ' exists' )\n",
    "    \n",
    "    sp104_pod_df = pd.read_csv(sp104_pod_file, \n",
    "                    sep = ',',\n",
    "                    )\n",
    "\n",
    "#     os.system('bzip2 -v '+ gfo_pod_df)\n",
    "\n",
    "else:\n",
    "    \n",
    "    sp104_pod_bigdf = {}\n",
    "    print(f\"---Calculating Spire 104 pod normalization\")\n",
    "    \n",
    "    for imonth,month in enumerate(month_list):\n",
    "        print(month)\n",
    "        \n",
    "        D500_sp104_pod = normalize_density_msis2( models_dens[month+'Rho_x'], 'Spire 104', 500)\n",
    "        models_dens[month+'Rho_x']['D500_sp104_pod'] = D500_sp104_pod\n",
    "        \n",
    "        sp104_pod_bigdf[month] = pd.DataFrame.from_dict(models_dens[month+'Rho_x'])\n",
    "        \n",
    "        del models_dens[month+'Rho_x']\n",
    "\n",
    "    print('--concatenating and saving')\n",
    "\n",
    "    sp104_pod_df = pd.concat([ sp104_pod_bigdf[month] for month in month_list]  )\n",
    "    sp104_pod_df = sp104_pod_df.reset_index(drop=True)\n",
    "    sp104_pod_df.to_csv(sp104_pod_file, index=False)  \n",
    "\n",
    "    ### Need to set this up to only write to file once. and have that write include the full dataset. Maybe concat the dataframes by month first\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3a74a",
   "metadata": {},
   "source": [
    "## Filter for common times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88033102",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.545022Z",
     "start_time": "2024-12-02T23:39:32.545007Z"
    }
   },
   "outputs": [],
   "source": [
    "gfo_acc_df = gfo_acc_df.query(\"Date >= '2022-06-01' and Date < '2022-06-30'\")\n",
    "sp104_pod_df = sp104_pod_df.query(\"date >= '2022-06-03' and date < '2022-06-25'\")\n",
    "sp113_pod_df = sp113_pod_df.query(\"date >= '2022-06-05' and date < '2022-06-28'\")\n",
    "sp106_pod_df = sp106_pod_df.query(\"date >= '2022-06-04' and date < '2022-06-12'\")\n",
    "\n",
    "(timeavg_gfo_acc,  denavg_gfo_acc) = orbit_avg_generic(gfo_acc_df['Date'], gfo_acc_df['D500_gfo'], gfo_acc_df['lat'])    \n",
    "(timeavg_sp104_pod,  denavg_sp104_pod) = orbit_avg_generic(sp104_pod_df['date'], sp104_pod_df['D500_sp104_pod'], sp104_pod_df['lat'])    \n",
    "(timeavg_sp113_pod,  denavg_sp113_pod) = orbit_avg_generic(sp113_pod_df['date'], sp113_pod_df['D500_sp113_pod'], sp113_pod_df['lat'])    \n",
    "(timeavg_sp106_pod,  denavg_sp106_pod) = orbit_avg_generic(sp106_pod_df['date'], sp106_pod_df['D500_sp106_pod'], sp106_pod_df['lat'])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e688d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.545861Z",
     "start_time": "2024-12-02T23:39:32.545846Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def calc_percent_diff_logspace(model, obs):\n",
    "    from numpy import exp as np_exp\n",
    "    from numpy import log as np_log # this is the natural log\n",
    "    \n",
    "#     return( ( np_exp(np_log(model/obs))-1 )*100)\n",
    "    return(  np_exp(np_log(model/obs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319e05b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.546965Z",
     "start_time": "2024-12-02T23:39:32.546950Z"
    }
   },
   "outputs": [],
   "source": [
    "def nearest(items, pivot):\n",
    "    return min(items, key=lambda x: abs(x - pivot))\n",
    "\n",
    "\n",
    "perc_stat = {}\n",
    "perc_stat['time_midpoint_GFO'] = []\n",
    "perc_stat['time_midpoint_sp104'] = []\n",
    "perc_stat['gfo_acc_denorbavg'] = []\n",
    "perc_stat['sp104_pod_denorbavg'] = []\n",
    "perc_stat['gfo_acc_time']      = []\n",
    "perc_stat['sp104_pod_time']      = []\n",
    "# \n",
    "perc_stat['percdiff_GFO_log']    = []\n",
    "perc_stat['percdiff_sp104_log']    = []\n",
    "# \n",
    "perc_stat['ratio_GFO']    = []\n",
    "perc_stat['ratio_sp104']    = []\n",
    "\n",
    "\n",
    "from numpy import exp as np_exp\n",
    "from numpy import log as np_log # this is the natural log\n",
    "\n",
    "\n",
    "for i,val in enumerate( timeavg_gfo_acc):\n",
    "    ###-----------------------------------\n",
    "    date_near = nearest(timeavg_sp104_pod, val)\n",
    "    indx = np.where(pd.to_datetime(timeavg_sp104_pod) == pd.to_datetime(date_near))[0][0]\n",
    "    \n",
    "    if np.abs((val - timeavg_sp104_pod[indx]).total_seconds()) > 3000:  # only compute % change if values within 1 orbit (95mins)\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        if np.abs((timeavg_sp104_pod[indx] - timeavg_sp104_pod[indx-1]).total_seconds()) > 40000:\n",
    "            print('skipping ', indx, timeavg_sp104_pod[indx])\n",
    "        else:\n",
    "            perc_stat['gfo_acc_time'].append( val)\n",
    "            perc_stat['gfo_acc_denorbavg'].append( denavg_gfo_acc[i])\n",
    "\n",
    "            perc_stat['time_midpoint_sp104'].append( pd.Timestamp(val) + (pd.Timestamp(timeavg_sp104_pod[indx]) - pd.Timestamp(val)) / 2)\n",
    "            perc_stat['sp104_pod_denorbavg'].append( denavg_sp104_pod[indx])\n",
    "            perc_stat['sp104_pod_time'].append( timeavg_sp104_pod[indx])\n",
    "            perc_stat['percdiff_sp104_log'].append(calc_percent_diff_logspace(denavg_sp104_pod[indx], denavg_gfo_acc[i]) )\n",
    "            perc_stat['ratio_sp104'].append(denavg_sp104_pod[indx]/denavg_gfo_acc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462aa232",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.548710Z",
     "start_time": "2024-12-02T23:39:32.548693Z"
    }
   },
   "outputs": [],
   "source": [
    "def nearest(items, pivot):\n",
    "    return min(items, key=lambda x: abs(x - pivot))\n",
    "\n",
    "# perc_stat = {}\n",
    "perc_stat['time_midpoint_GFO'] = []\n",
    "perc_stat['time_midpoint_sp113'] = []\n",
    "perc_stat['gfo_acc_denorbavg'] = []\n",
    "perc_stat['sp113_pod_denorbavg'] = []\n",
    "perc_stat['gfo_acc_time']      = []\n",
    "perc_stat['sp113_pod_time']      = []\n",
    "# \n",
    "perc_stat['percdiff_GFO_log']    = []\n",
    "perc_stat['percdiff_sp113_log']    = []\n",
    "# \n",
    "perc_stat['ratio_GFO']    = []\n",
    "perc_stat['ratio_sp113']    = []\n",
    "\n",
    "\n",
    "from numpy import exp as np_exp\n",
    "from numpy import log as np_log # this is the natural log\n",
    "\n",
    "\n",
    "for i,val in enumerate( timeavg_gfo_acc):\n",
    "    ###-----------------------------------\n",
    "    date_near = nearest(timeavg_sp113_pod, val)\n",
    "    indx = np.where(pd.to_datetime(timeavg_sp113_pod) == pd.to_datetime(date_near))[0][0]\n",
    "    \n",
    "    if np.abs((val - timeavg_sp113_pod[indx]).total_seconds()) > 3000:  # only compute % change if values within 1 orbit (95mins)\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        if np.abs((timeavg_sp113_pod[indx] - timeavg_sp113_pod[indx-1]).total_seconds()) > 40000:\n",
    "            print('skipping ', indx, timeavg_sp113_pod[indx])\n",
    "        else:\n",
    "            perc_stat['gfo_acc_time'].append( val)\n",
    "            perc_stat['gfo_acc_denorbavg'].append( denavg_gfo_acc[i])\n",
    "\n",
    "            perc_stat['time_midpoint_sp113'].append( pd.Timestamp(val) + (pd.Timestamp(timeavg_sp113_pod[indx]) - pd.Timestamp(val)) / 2)\n",
    "            perc_stat['sp113_pod_denorbavg'].append( denavg_sp113_pod[indx])\n",
    "            perc_stat['sp113_pod_time'].append( timeavg_sp113_pod[indx])\n",
    "            perc_stat['percdiff_sp113_log'].append(calc_percent_diff_logspace(denavg_sp113_pod[indx], denavg_gfo_acc[i]) )\n",
    "            perc_stat['ratio_sp113'].append(denavg_sp113_pod[indx]/denavg_gfo_acc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5be4ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.549965Z",
     "start_time": "2024-12-02T23:39:32.549945Z"
    }
   },
   "outputs": [],
   "source": [
    "def nearest(items, pivot):\n",
    "    return min(items, key=lambda x: abs(x - pivot))\n",
    "\n",
    "# perc_stat = {}\n",
    "perc_stat['time_midpoint_GFO'] = []\n",
    "perc_stat['time_midpoint_sp106'] = []\n",
    "perc_stat['gfo_acc_denorbavg'] = []\n",
    "perc_stat['sp106_pod_denorbavg'] = []\n",
    "perc_stat['gfo_acc_time']      = []\n",
    "perc_stat['sp106_pod_time']      = []\n",
    "# \n",
    "perc_stat['percdiff_GFO_log']    = []\n",
    "perc_stat['percdiff_sp106_log']    = []\n",
    "# \n",
    "perc_stat['ratio_GFO']    = []\n",
    "perc_stat['ratio_sp106']    = []\n",
    "\n",
    "\n",
    "from numpy import exp as np_exp\n",
    "from numpy import log as np_log # this is the natural log\n",
    "\n",
    "\n",
    "for i,val in enumerate( timeavg_gfo_acc):\n",
    "    ###-----------------------------------\n",
    "    date_near = nearest(timeavg_sp106_pod, val)\n",
    "    indx = np.where(pd.to_datetime(timeavg_sp106_pod) == pd.to_datetime(date_near))[0][0]\n",
    "    \n",
    "    if np.abs((val - timeavg_sp106_pod[indx]).total_seconds()) > 3000:  # only compute % change if values within 1 orbit (95mins)\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        if np.abs((timeavg_sp106_pod[indx] - timeavg_sp106_pod[indx-1]).total_seconds()) > 40000:\n",
    "            print('skipping ', indx, timeavg_sp106_pod[indx])\n",
    "        else:\n",
    "            perc_stat['gfo_acc_time'].append( val)\n",
    "            perc_stat['gfo_acc_denorbavg'].append( denavg_gfo_acc[i])\n",
    "\n",
    "            perc_stat['time_midpoint_sp106'].append( pd.Timestamp(val) + (pd.Timestamp(timeavg_sp106_pod[indx]) - pd.Timestamp(val)) / 2)\n",
    "            perc_stat['sp106_pod_denorbavg'].append( denavg_sp106_pod[indx])\n",
    "            perc_stat['sp106_pod_time'].append( timeavg_sp106_pod[indx])\n",
    "            perc_stat['percdiff_sp106_log'].append(calc_percent_diff_logspace(denavg_sp106_pod[indx], denavg_gfo_acc[i]) )\n",
    "            perc_stat['ratio_sp106'].append(denavg_sp106_pod[indx]/denavg_gfo_acc[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b069ccd",
   "metadata": {},
   "source": [
    "## Load Kp and Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e8056",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.551281Z",
     "start_time": "2024-12-02T23:39:32.551265Z"
    }
   },
   "outputs": [],
   "source": [
    "import linecache\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path_to_flux = \"/data/SatDragModelValidation/src/geodyn_code/support/geodyn_support/make_tables/\"\n",
    "\n",
    "\n",
    "dateS   = []\n",
    "dateAP  = []\n",
    "dateKP8 = []\n",
    "\n",
    "fluxS   = []\n",
    "fluxAP  = []\n",
    "fluxKP8 = []\n",
    "\n",
    "switch = 0\n",
    "with open(path_to_flux+\"new_master\", 'r') as f:\n",
    "    for line_no, string in enumerate(f):\n",
    "#         string = linecache.getline(\"new_master\",line+1) #\n",
    "\n",
    "        if 'MASTER' in string:        \n",
    "            switch = 'skip'\n",
    "            continue\n",
    "            \n",
    "        ### Skip the title lines and any dates before Y2K\n",
    "        if 'FLUXS' in string:        \n",
    "            switch = 'readfluxS'\n",
    "            print(switch)\n",
    "            continue\n",
    "\n",
    "        if 'FLUXAP' in string:        \n",
    "            switch = 'readfluxAP'\n",
    "            print(switch)\n",
    "            continue\n",
    "        if 'FLUXKP8' in string:        \n",
    "            switch = 'readfluxKP8'\n",
    "            print(switch)\n",
    "            continue\n",
    "\n",
    "        if switch =='skip':\n",
    "            continue\n",
    "            \n",
    "        if int(string[:2]) >= 58:\n",
    "            continue\n",
    "        else:\n",
    "            \n",
    "            if int(string[:2]) < 22 or int(string[:2]) > 22:\n",
    "                continue\n",
    "                \n",
    "            if switch == 'readfluxS':\n",
    "                ### Load the Solar Flux values after 2000. The date leading each line represents\n",
    "                ### the first flux value in that line.  The following n values are that day + n days.\n",
    "                ###  All Solar Flux values are mulitplied by 10 for some reason...\n",
    "\n",
    "                for n in range(0,12):\n",
    "                    dateS.append(pd.to_datetime(string[:6],format='%y%m%d') + n*pd.to_timedelta(1,'d') )\n",
    "                fluxS.append(int(string[11:15]) / 10)\n",
    "                fluxS.append(int(string[16:20]) / 10)\n",
    "                fluxS.append(int(string[21:25]) / 10)\n",
    "                fluxS.append(int(string[26:30]) / 10)\n",
    "                fluxS.append(int(string[31:35]) / 10)\n",
    "                fluxS.append(int(string[36:40]) / 10)\n",
    "                fluxS.append(int(string[41:45]) / 10)\n",
    "                fluxS.append(int(string[46:50]) / 10)\n",
    "                fluxS.append(int(string[51:55]) / 10)\n",
    "                fluxS.append(int(string[56:60]) / 10)\n",
    "                fluxS.append(int(string[61:65]) / 10)\n",
    "                fluxS.append(int(string[66:70]) / 10)\n",
    "\n",
    "            if switch == 'readfluxAP':\n",
    "                ### Load the AP values after 2000. These are formatted the same as the FLUXS section\n",
    "                for n in range(0,12):\n",
    "                    dateAP.append(pd.to_datetime(string[:6],format='%y%m%d') + n*pd.to_timedelta(1,'d') )\n",
    "                fluxAP.append(int(string[11:15]) / 10)\n",
    "                fluxAP.append(int(string[16:20]) / 10)\n",
    "                fluxAP.append(int(string[21:25]) / 10)\n",
    "                fluxAP.append(int(string[26:30]) / 10)\n",
    "                fluxAP.append(int(string[31:35]) / 10)\n",
    "                fluxAP.append(int(string[36:40]) / 10)\n",
    "                fluxAP.append(int(string[41:45]) / 10)\n",
    "                fluxAP.append(int(string[46:50]) / 10)\n",
    "                fluxAP.append(int(string[51:55]) / 10)\n",
    "                fluxAP.append(int(string[56:60]) / 10)\n",
    "                fluxAP.append(int(string[61:65]) / 10)\n",
    "                fluxAP.append(int(string[66:70]) / 10)\n",
    "\n",
    "            if switch == 'readfluxKP8':\n",
    "                ### Load the KP values after 2000. These are formatted such that each line is a day\n",
    "                ### and each nth value in the line is n*3hrs for the day\n",
    "                for n in range(0,8):\n",
    "                    dateKP8.append(pd.to_datetime(string[:6],format='%y%m%d') + n*pd.to_timedelta(3,'h') )\n",
    "                fluxKP8.append(int(string[7:11])  / 100)\n",
    "                fluxKP8.append(int(string[12:16]) / 100)\n",
    "                fluxKP8.append(int(string[17:21]) / 100)\n",
    "                fluxKP8.append(int(string[22:26]) / 100)\n",
    "                fluxKP8.append(int(string[27:31]) / 100)\n",
    "                fluxKP8.append(int(string[32:36]) / 100)\n",
    "                fluxKP8.append(int(string[37:41]) / 100)\n",
    "                fluxKP8.append(int(string[42:46]) / 100)\n",
    "\n",
    "fluxS = np.array(fluxS)\n",
    "fluxAP = np.array(fluxAP)\n",
    "fluxKP8 = np.array(fluxKP8)\n",
    "                \n",
    "fluxS   = np.where(fluxS  == 0., np.nan, fluxS  )\n",
    "fluxAP  = np.where(fluxAP ==0., np.nan, fluxAP )\n",
    "fluxKP8 = np.where(fluxKP8==0., np.nan, fluxKP8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7efdb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.552690Z",
     "start_time": "2024-12-02T23:39:32.552676Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, row_heights=[0.2, 0.8],#],\n",
    "                    specs=[[{\"secondary_y\": True}],\n",
    "#                            [{\"secondary_y\": False}],\n",
    "#                            [{\"secondary_y\": False}],\n",
    "#                            [{\"secondary_y\": False}],\n",
    "                           [{\"secondary_y\": False}]],\n",
    "                           shared_xaxes=True,\n",
    "                           vertical_spacing=0.02)\n",
    "    \n",
    "SHOW_alldata = True\n",
    "\n",
    "#     print('model_m1', model_m1)\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=dateS,\n",
    "                           y=fluxS,\n",
    "                           name= 'F107d_1AU',\n",
    "                           mode='lines',\n",
    "                           opacity=1,\n",
    "                           line = dict(shape = 'hvh',dash='dash', color = 'blue', width=2),\n",
    "                           showlegend=False),\n",
    "                           secondary_y=True,row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=dateKP8,\n",
    "                           y=fluxKP8,\n",
    "                           name= 'Kp',\n",
    "                           mode='lines',\n",
    "                           opacity=1,\n",
    "                           line = dict(shape = 'hvh', color = 'black', width=2),\n",
    "                           showlegend=False),\n",
    "                           secondary_y=False,row=1, col=1) \n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=timeavg_gfo_acc,\n",
    "                           y=denavg_gfo_acc,\n",
    "                           ### name= model_m1,\n",
    "                           mode='markers+lines',\n",
    "                           opacity=1,\n",
    "                               marker=dict(color='black',size=2),\n",
    "                               line = dict(dash ='solid', color = 'black', width=2),\n",
    "                           showlegend=False), row=2, col=1)\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=timeavg_sp104_pod,\n",
    "                           y=denavg_sp104_pod,\n",
    "                           ### name= model_m1,\n",
    "                           mode='markers+lines',\n",
    "                           opacity=1,\n",
    "                               marker=dict(size=2),\n",
    "                               line = dict(dash ='solid',  width=2),\n",
    "                           showlegend=False), row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=timeavg_sp113_pod,\n",
    "                           y=denavg_sp113_pod,\n",
    "                           ### name= model_m1,\n",
    "                           mode='markers+lines',\n",
    "                           opacity=1,\n",
    "                               marker=dict(size=2),\n",
    "                               line = dict(dash ='solid', width=2),\n",
    "                           showlegend=False), row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=timeavg_sp106_pod,\n",
    "                           y=denavg_sp106_pod,\n",
    "                           ### name= model_m1,\n",
    "                           mode='markers+lines',\n",
    "                           opacity=1,\n",
    "                               marker=dict(size=2),\n",
    "                               line = dict(dash ='solid', width=2),\n",
    "                           showlegend=False), row=2, col=1)\n",
    "\n",
    "    \n",
    "### UPDATE AXES \n",
    "fig.update_yaxes(title_text=\"Kp\", \n",
    "                 exponentformat= 'power',\n",
    "#                  range=[0,7],\n",
    "                 secondary_y=False,\n",
    "                 row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"F10.7\", \n",
    "                 exponentformat= 'power',\n",
    "                 range=[35,170],\n",
    "                 secondary_y=True,\n",
    "                 tickfont=dict(color=\"blue\"),\n",
    "                 titlefont=dict(color=\"blue\"),\n",
    "                 row=1, col=1)\n",
    "fig.update_xaxes(range=[pd.to_datetime('2022-06-01 02:00:00'), pd.to_datetime('2022-06-30 00:00:00')])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "    \n",
    "#######################################################\n",
    "font_dict=dict(family='Arial',size=11,color='black')\n",
    "\n",
    "\n",
    "fig = plot_cleanformat_axes(fig, font_dict)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Density\", \n",
    "                 type=\"log\", \n",
    "                 exponentformat= 'power',row=2, col=1)\n",
    "\n",
    "    \n",
    "# a='input'\n",
    "# s=settings_spire\n",
    "fig.update_layout(title=f\"Density Validation- Spire and GFO\", \n",
    "#                   title=f\"{s['cd_model'][a]}, {s['hours_between_cd_adj'][a]}-hr Scale from CD={s['cd_value'][a]}\", \n",
    "                  autosize=False,    width=1000,    height=800,\n",
    "                  legend= {'itemsizing': 'trace'},\n",
    "                  font=font_dict,\n",
    "                  plot_bgcolor='white', \n",
    "                 )\n",
    "fig.update_annotations(font_size=16)  # Increase size of subplot title\n",
    "# fig.show(config=config)\n",
    "fig.show(config= dict({\n",
    "                'displayModeBar': False,\n",
    "                'responsive': True,\n",
    "                'staticPlot': False,\n",
    "                'displaylogo': False,\n",
    "                'showTips': False,\n",
    "                }),\n",
    "#          renderer='jpg',\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c9845b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.554111Z",
     "start_time": "2024-12-02T23:39:32.554096Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "gfopodcolor = \"#d62728\"\n",
    "sp104podcolor = \"#AB63FA\"\n",
    "sp106podcolor = \"cyan\"\n",
    "gfoacccolor = 'black'\n",
    "sp113podcolor = '#FF6692'\n",
    "\n",
    "# fig = make_subplots(rows=1, cols=1)\n",
    "fig = make_subplots(rows=3, cols=1, row_heights=[0.2, 0.5, 0.3 ],#],\n",
    "                    specs=[[{\"secondary_y\": True}],\n",
    "                           [{\"secondary_y\": False}],\n",
    "#                            [{\"secondary_y\": False}],\n",
    "#                            [{\"secondary_y\": False}],\n",
    "                           [{\"secondary_y\": False}]],\n",
    "                           shared_xaxes=True,\n",
    "                           vertical_spacing=0.02)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=dateS,\n",
    "                           y=fluxS,\n",
    "                           name= 'F107d_1AU',\n",
    "                           mode='lines',\n",
    "                           opacity=1,\n",
    "                           line = dict(shape = 'hvh',dash='dash', color = 'blue', width=2),\n",
    "                           showlegend=False),\n",
    "                           secondary_y=True,row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=dateKP8,\n",
    "                           y=fluxKP8,\n",
    "                           name= 'Kp',\n",
    "                           mode='lines',\n",
    "                           opacity=1,\n",
    "                           line = dict(shape = 'hvh', color = 'grey', width=2),\n",
    "                           showlegend=False),\n",
    "                           secondary_y=False,row=1, col=1) \n",
    "\n",
    "\n",
    "\n",
    "# fig.add_trace(go.Scattergl(x= perc_stat['gfo_acc_time'],\n",
    "#                             y=perc_stat['gfo_acc_denorbavg'],\n",
    "#                             name=f'GFO_acc_500km',\n",
    "#                             mode='markers',\n",
    "#                             opacity=1,\n",
    "#                             marker=dict( size=4, color=gfoacccolor),\n",
    "#                             showlegend=False),\n",
    "#                             row=2, col=1)\n",
    "fig.add_trace(go.Scattergl(x= timeavg_gfo_acc,\n",
    "                            y=denavg_gfo_acc,\n",
    "                            name=f'GFO_acc_500km',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4, color=gfoacccolor),\n",
    "                            showlegend=False),\n",
    "                            row=2, col=1)\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=perc_stat['sp104_pod_time'],\n",
    "                            y=perc_stat['sp104_pod_denorbavg'],\n",
    "                            name=f'D500_sp104_pod',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4, color=sp104podcolor),\n",
    "                            showlegend=False),\n",
    "                            row=2, col=1)\n",
    "fig.add_trace(go.Scattergl(x=perc_stat['time_midpoint_sp104'],\n",
    "                            y=perc_stat['percdiff_sp104_log'],\n",
    "#                             name=f'D500_icesat2',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=3, color=sp104podcolor),\n",
    "                            showlegend=False),\n",
    "                            row=3, col=1)\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=perc_stat['sp113_pod_time'],\n",
    "                            y=perc_stat['sp113_pod_denorbavg'],\n",
    "                            name=f'D500_sp113_pod',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4, color=sp113podcolor),\n",
    "                            showlegend=False),\n",
    "                            row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=perc_stat['time_midpoint_sp113'],\n",
    "                            y=perc_stat['percdiff_sp113_log'],\n",
    "#                             name=f'D500_icesat2',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=3, color=sp113podcolor),\n",
    "                            showlegend=False),\n",
    "                            row=3, col=1)\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=perc_stat['sp106_pod_time'],\n",
    "                            y=perc_stat['sp106_pod_denorbavg'],\n",
    "                            name=f'D500_sp106_pod',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4, color=sp106podcolor),\n",
    "                            showlegend=False),\n",
    "                            row=2, col=1)\n",
    "fig.add_trace(go.Scattergl(x=perc_stat['time_midpoint_sp106'],\n",
    "                            y=perc_stat['percdiff_sp106_log'],\n",
    "#                             name=f'D500_icesat2',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=3, color=sp106podcolor),\n",
    "                            showlegend=False),\n",
    "                            row=3, col=1)\n",
    "\n",
    "\n",
    "fig.add_hline(y=0, line = dict(dash='solid', color = 'grey', width=1), row=3, col=1)\n",
    "\n",
    "df = pd.DataFrame(data={'time':perc_stat['time_midpoint_sp104'],'percdiff_sp104_log': perc_stat['percdiff_sp104_log'] })\n",
    "fig.add_trace(go.Scattergl(x=perc_stat['time_midpoint_sp104'],\n",
    "                            y=df.rolling(window=16, center=True)['percdiff_sp104_log'].mean(),\n",
    "                            opacity=1,\n",
    "#                             marker=dict( size=2, color='#1f77b4'),\n",
    "                           line = dict( color =sp104podcolor, width=4),\n",
    "                            showlegend=False),\n",
    "                            row=3, col=1)\n",
    "\n",
    "df = pd.DataFrame(data={'time':perc_stat['time_midpoint_sp113'],'percdiff_sp113_log': perc_stat['percdiff_sp113_log'] })\n",
    "fig.add_trace(go.Scattergl(x=perc_stat['time_midpoint_sp113'],\n",
    "                            y=df.rolling(window=16, center=True)['percdiff_sp113_log'].mean(),\n",
    "                            opacity=1,\n",
    "#                             marker=dict( size=2, color='#1f77b4'),\n",
    "                           line = dict( color =sp113podcolor, width=4),\n",
    "                            showlegend=False),\n",
    "                            row=3, col=1)\n",
    "\n",
    "df = pd.DataFrame(data={'time':perc_stat['time_midpoint_sp106'],'percdiff_sp106_log': perc_stat['percdiff_sp106_log'] })\n",
    "fig.add_trace(go.Scattergl(x=perc_stat['time_midpoint_sp106'],\n",
    "                            y=df.rolling(window=16, center=True)['percdiff_sp106_log'].mean(),\n",
    "                            opacity=1,\n",
    "#                             marker=dict( size=2, color='#1f77b4'),\n",
    "                           line = dict( color =sp106podcolor, width=4),\n",
    "                            showlegend=False),\n",
    "                            row=3, col=1)\n",
    "\n",
    "#### FANCY LEGEND ################################################################\n",
    "modelnames=[]\n",
    "modelcolors = []\n",
    "modelnames.append(\"GraceFO Acc\")\n",
    "modelcolors.append(\"black\")\n",
    "# modelnames.append(\"GraceFO-POD\")\n",
    "# modelcolors.append('#d62728')\n",
    "modelnames.append(\"Spire 104 POD\")\n",
    "modelcolors.append(sp104podcolor)\n",
    "modelnames.append(\"Spire 113 POD\")\n",
    "modelcolors.append(sp113podcolor)\n",
    "modelnames.append(\"Spire 106 POD\")\n",
    "modelcolors.append(sp106podcolor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_leg = pd.DataFrame({\"starts_colors\": modelcolors})\n",
    "fig.update_traces(showlegend=False).add_traces(\n",
    "    [   go.Scattergl(name=modelnames[i],\n",
    "               x=[pd.to_datetime( \"181107-000000\", format='%y%m%d-%H%M%S')],\n",
    "               mode='lines',\n",
    "               line = dict(shape = 'hv',  width=10),\n",
    "               marker_color=c,\n",
    "               showlegend=True)\n",
    "        for i,c in enumerate((df_leg.loc[:,[\"starts_colors\"]].values.ravel()))])\n",
    "## Legend Control\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=.78,\n",
    "    xanchor=\"center\",\n",
    "    x=.45,\n",
    "    orientation=\"h\",\n",
    "        font=dict(family='Arial',size=12,color='black'),\n",
    "        bgcolor=\"white\",\n",
    "        bordercolor=\"darkgrey\",\n",
    "        borderwidth=0.5,)  )\n",
    "################################################################################\n",
    "\n",
    "fig.add_hline(y=0, line = dict(dash='solid', color = 'grey', width=1), row=3, col=1)\n",
    "\n",
    "### UPDATE AXES \n",
    "fig.update_yaxes(title_text=\"Kp\", \n",
    "                 exponentformat= 'power',\n",
    "                 range=[0,7],\n",
    "                 secondary_y=False,\n",
    "                 row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"F10.7\", \n",
    "                 exponentformat= 'power',\n",
    "                 range=[35,170],\n",
    "                 secondary_y=True,\n",
    "                 tickfont=dict(color=\"blue\"),\n",
    "                 titlefont=dict(color=\"blue\"),\n",
    "                 row=1, col=1)\n",
    "################################################################################\n",
    "\n",
    "### SYLIZE LEGEND \n",
    "font_dict=dict(family='Arial',size=11,color='black')\n",
    "## automate the specification of the axes for subplots\n",
    "rownum, colnum = fig._get_subplot_rows_columns()\n",
    "for i in rownum:\n",
    "    if len(rownum)==1:\n",
    "        L_ticklabel = False\n",
    "    else:\n",
    "        if i < len(rownum):\n",
    "            L_ticklabel = False\n",
    "        else:\n",
    "            L_ticklabel = True\n",
    "    fig.update_xaxes(### LINE at axis border\n",
    "                      showline=True,\n",
    "                      showticklabels=L_ticklabel,\n",
    "#                       tickformat= '%m/%d',\n",
    "                      linecolor='black',\n",
    "                      linewidth=1,\n",
    "                     ### Major ticks\n",
    "                      ticks='inside',\n",
    "                      tickfont=font_dict,\n",
    "                      mirror=True,\n",
    "#                       tickwidth=2,\n",
    "#                       ticklen=9,\n",
    "                      tickcolor='grey',\n",
    "#                       tick0=\"2018-11-9\" ,\n",
    "#                       dtick=86400000.0*1,    # milliseconds in a day, every 7 days\n",
    "                      #### Minor Ticks\n",
    "                       minor=dict(\n",
    "                         dtick=86400000.0, # milliseconds in a day\n",
    "                         tickwidth=1,\n",
    "                         ticklen=4,\n",
    "                         tickcolor='grey',\n",
    "                         ticks='inside'),\n",
    "                      ### GRID\n",
    "                       gridcolor='gainsboro',\n",
    "                       gridwidth=1,\n",
    "                       layer='above traces',\n",
    "                       tickangle=0,\n",
    "                       row=i, col=1)\n",
    "    fig.update_yaxes(showline=True,      # add line at x=0\n",
    "                         showticklabels=True,\n",
    "                         linecolor='black',  # line color\n",
    "                         linewidth=1,        # line size\n",
    "                     ticks='inside',     # ticks outside axis\n",
    "                     tickfont=font_dict, # tick label font\n",
    "                     mirror='allticks',  # add ticks to top/right axes\n",
    "                     tickwidth=1,      # tick width\n",
    "                     tickcolor='black',  # tick color\n",
    "                     gridcolor='gainsboro',\n",
    "                     gridwidth=1,\n",
    "                     layer='above traces',\n",
    "                     row=i, col=1)\n",
    "\n",
    "\n",
    "# fig.update_xaxes(range=[pd.to_datetime( \"181101-000000\", format='%y%m%d-%H%M%S'),\n",
    "#                         pd.to_datetime( \"181130-000000\", format='%y%m%d-%H%M%S')],row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(range=[pd.to_datetime('2022-06-01 20:54:00'), pd.to_datetime('2022-06-30 00:00:00')])\n",
    "\n",
    "fig.update_yaxes(title_text=\"Orbit Averaged Density\", \n",
    "                 type=\"log\", \n",
    "                 exponentformat= 'power',row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Mean Ratio of Orbit Avg.\", \n",
    "                minor=dict(dtick=10, tickwidth=1,ticklen=4,tickcolor='grey',ticks='inside'),\n",
    "                 exponentformat= 'power',row=3, col=1)\n",
    "# fig.update_yaxes(title_text=\"GFO_acc-ICE_pod %d\", \n",
    "#                 minor=dict(dtick=10, tickwidth=1,ticklen=4,tickcolor='grey',ticks='inside'),\n",
    "#                  exponentformat= 'power',row=4, col=1)\n",
    "# fig.update_yaxes(title_text=\"GFO_pod-ICE_pod %d\", \n",
    "#                 minor=dict(dtick=10, tickwidth=1,ticklen=4,tickcolor='grey',ticks='inside'),\n",
    "#                  exponentformat= 'power',row=5, col=1)\n",
    "\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=30, b=20),)\n",
    "\n",
    "fig.update_layout(title=f\"Spire-FM104 Rho_x compared to GRACE-FO Acc., Norm 500km\",\n",
    "                  autosize=False,    width=1000,    height=800,\n",
    "                  legend= {'itemsizing': 'trace'},\n",
    "                  font=font_dict, plot_bgcolor='white', \n",
    "                 )\n",
    "\n",
    "fig.show(config=config)\n",
    "\n",
    "# pio.write_image(fig, 'ICEvGFO.jpg', scale=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4704f3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T17:37:13.958304Z",
     "start_time": "2024-09-27T17:37:13.912252Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484bbf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.554963Z",
     "start_time": "2024-12-02T23:39:32.554949Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# perc_stat['percdiff_sp104_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d952452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.556187Z",
     "start_time": "2024-12-02T23:39:32.556172Z"
    }
   },
   "outputs": [],
   "source": [
    "signal.savgol_filter(perc_stat['percdiff_sp104_log'], 53, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81250b32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.557264Z",
     "start_time": "2024-12-02T23:39:32.557249Z"
    }
   },
   "outputs": [],
   "source": [
    "perc_stat['percdiff_GFO_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c34db3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.558258Z",
     "start_time": "2024-12-02T23:39:32.558244Z"
    }
   },
   "outputs": [],
   "source": [
    "perc_stat['time_midpoint_sp104']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39fb9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T23:39:32.559383Z",
     "start_time": "2024-12-02T23:39:32.559369Z"
    }
   },
   "outputs": [],
   "source": [
    "perc_stat['time_midpoint_sp104']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba3bfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1070px",
    "left": "85px",
    "top": "111.125px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
