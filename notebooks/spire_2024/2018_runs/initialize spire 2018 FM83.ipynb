{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2c972e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T21:16:53.652041Z",
     "start_time": "2024-03-20T21:16:53.321151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'novdtm2020_o': {'num': 3, 'model_path': None}}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pygeodyn.PYGEODYN import Pygeodyn\n",
    "from gc import collect as gc_collect\n",
    "\n",
    "# ## Initialization timeperiod\n",
    "### Satellites:\n",
    "Sat_List = ['spire083',\n",
    "            #,'spire084',\n",
    "            #'spire085',\n",
    "           ]\n",
    "import datetime\n",
    "\n",
    "\n",
    "scaling_cadence = 3\n",
    "scale_cadence = scaling_cadence\n",
    "index_buffarc_start = 0\n",
    "run_list = [\n",
    "#                 'msis2',\n",
    "#                 'jb2008',\n",
    "                'dtm2020_o',\n",
    "           ]\n",
    "\n",
    "month_list = ['nov']\n",
    "# month_list = ['oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dir_modeldat='/data/SatDragModelValidation/data/inputs/atmos_models'\n",
    "run_dict={}\n",
    "for i in run_list:\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        if i =='msis2':\n",
    "            run_dict[month+i]={}\n",
    "            run_dict[month+i]['num'] = 5\n",
    "            run_dict[month+i]['model_path'] = None\n",
    "        if i =='dtm2020_o':\n",
    "            run_dict[month+i]={}\n",
    "            run_dict[month+i]['num'] = 3\n",
    "            run_dict[month+i]['model_path'] = None\n",
    "        if i =='jb2008':\n",
    "            run_dict[month+i]={}\n",
    "            run_dict[month+i]['num'] = 1\n",
    "            run_dict[month+i]['model_path'] = None\n",
    "\n",
    "print(run_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e86b9cfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T21:17:32.873128Z",
     "start_time": "2024-03-20T21:16:53.655375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " arc           2018.311\n",
      " epoch_startDT 2018-11-06 20:57:52        2.1333333333333333 mins from 21:00\n",
      " start_0ut     2018-11-07 00:00:00 \n",
      " end___0ut     2018-11-08 00:00:00 \n",
      " epoch_endDT   2018-11-08 03:00:00 \n",
      " scaleparameter_times     [[Timestamp('2018-11-07 00:00:00'), Timestamp('2018-11-07 03:00:00'), Timestamp('2018-11-07 06:00:00'), Timestamp('2018-11-07 09:00:00'), Timestamp('2018-11-07 12:00:00'), Timestamp('2018-11-07 15:00:00'), Timestamp('2018-11-07 18:00:00'), Timestamp('2018-11-07 21:00:00'), Timestamp('2018-11-08 00:00:00'), Timestamp('2018-11-08 03:00:00')]] \n",
      " \n",
      " arc           2018.312\n",
      " epoch_startDT 2018-11-07 20:47:51        12.15 mins from 21:00\n",
      " start_0ut     2018-11-08 00:00:00 \n",
      " end___0ut     2018-11-09 00:00:00 \n",
      " epoch_endDT   2018-11-09 03:00:00 \n",
      " scaleparameter_times     [[Timestamp('2018-11-07 00:00:00'), Timestamp('2018-11-07 03:00:00'), Timestamp('2018-11-07 06:00:00'), Timestamp('2018-11-07 09:00:00'), Timestamp('2018-11-07 12:00:00'), Timestamp('2018-11-07 15:00:00'), Timestamp('2018-11-07 18:00:00'), Timestamp('2018-11-07 21:00:00'), Timestamp('2018-11-08 00:00:00'), Timestamp('2018-11-08 03:00:00')], [Timestamp('2018-11-08 00:00:00'), Timestamp('2018-11-08 03:00:00'), Timestamp('2018-11-08 06:00:00'), Timestamp('2018-11-08 09:00:00'), Timestamp('2018-11-08 12:00:00'), Timestamp('2018-11-08 15:00:00'), Timestamp('2018-11-08 18:00:00'), Timestamp('2018-11-08 21:00:00'), Timestamp('2018-11-09 00:00:00'), Timestamp('2018-11-09 03:00:00')]] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from  datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import linecache\n",
    "\n",
    "def nearest(items, pivot):\n",
    "    return min(items, key=lambda x: abs(x - pivot))\n",
    "\n",
    "# for i,val in enumerate( timeavg_gfo):\n",
    "#     date_near = nearest(timeavg_ice, val)\n",
    "#     indx = np.where(pd.to_datetime(timeavg_ice) == pd.to_datetime(date_near))[0][0]\n",
    "    \n",
    "#     if np.abs((val - timeavg_ice[indx]).total_seconds()) > 5700:  # only compute % change if values within 1 orbit (95mins)\n",
    "#         pass\n",
    "\n",
    "\n",
    "\n",
    "def get_dates(test_timeperiod, file_raw_ICs):\n",
    "    \n",
    "    xyzline = pd.read_csv(\n",
    "                    file_raw_ICs,\n",
    "                    skiprows=23,\n",
    "                #     nrows=1,\n",
    "                    sep='\\s+',\n",
    "                    dtype=str,\n",
    "                    names=[\n",
    "                        'DateYMD',\n",
    "                        'DateHMS',\n",
    "                        'X',\n",
    "                        'Y',\n",
    "                        'Z',\n",
    "                        'X_dot',\n",
    "                        'Y_dot',\n",
    "                        'Z_dot',\n",
    "                    ],\n",
    "                )\n",
    "    date = pd.to_datetime(\\\n",
    "                        + xyzline['DateYMD']  \\\n",
    "                        + xyzline['DateHMS'], \\\n",
    "                                format='%Y-%m-%d%H:%M:%S')\n",
    "    xyzline.insert(0, 'Date', date)\n",
    "    del xyzline['DateYMD'], xyzline['DateHMS'], date\n",
    "    del xyzline['X'], xyzline['Y'], xyzline['Z']\n",
    "    del xyzline['X_dot'], xyzline['Y_dot'], xyzline['Z_dot']\n",
    "    \n",
    "#     list_arcs = []\n",
    "#     start_0ut = []\n",
    "#     end___0ut = []\n",
    "#     start_update = []\n",
    "    \n",
    "    list_arc_startDT   = []\n",
    "    list_epoch_startDT = []\n",
    "    list_start_0ut     = []\n",
    "    list_end___0ut     = []\n",
    "    list_epoch_endDT   = []\n",
    "    scaleparameter_times = []\n",
    "\n",
    "\n",
    "    startdate    = pd.to_datetime(test_timeperiod[0])\n",
    "    enddate      = pd.to_datetime(test_timeperiod[-1])\n",
    "    startdate_dt = pd.to_datetime(startdate, format='%Y-%m-%d')\n",
    "    enddate_dt   = pd.to_datetime(enddate,   format='%Y-%m-%d')\n",
    "    starts_linspace_dt = pd.date_range(start=startdate_dt ,\n",
    "                                         end=enddate_dt   ,\n",
    "                                        freq=str(1)+\"D\")\n",
    "\n",
    "    ### Make a list of the initial conditions for daily epoch start times that match\n",
    "    ### the first instance of pce data for that day\n",
    "    for iday, day in enumerate(starts_linspace_dt):\n",
    "\n",
    "        epoch_start =  f\"{day.strftime('%Y-%m-%d %H:%M:%S')}\" #f\"2018-11-{dayval:02d} 00:00:00\"\n",
    "        arc_startDT= pd.to_datetime(epoch_start,format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        start_0ut = arc_startDT\n",
    "        end___0ut = arc_startDT + pd.to_timedelta(24, 'h')\n",
    "        epoch_endDT = end___0ut + pd.to_timedelta(3, 'h')\n",
    "        \n",
    "\n",
    "        epoch_startDTtest   = pd.to_datetime(epoch_start,format='%Y-%m-%d %H:%M:%S')-pd.to_timedelta(3, 'h')\n",
    "\n",
    "        epoch_startDT = nearest( pd.to_datetime(xyzline['Date'].values), epoch_startDTtest)\n",
    "        indx = np.where(pd.to_datetime(xyzline['Date'].values) == pd.to_datetime(epoch_startDT))[0][0]\n",
    "\n",
    "        time_sep = np.abs((epoch_startDTtest - xyzline['Date'][indx]).total_seconds())/60\n",
    "        \n",
    "        print(' ')\n",
    "        print(f\" arc           {arc_startDT.strftime('%Y.%j')}\")\n",
    "        print(f\" epoch_startDT {epoch_startDT}        {time_sep} mins from 21:00\")\n",
    "        print(f\" start_0ut     {start_0ut} \")\n",
    "        print(f\" end___0ut     {end___0ut} \")\n",
    "        print(f\" epoch_endDT   {epoch_endDT} \")\n",
    "\n",
    "        list_arc_startDT.append(arc_startDT.strftime('%Y.%j'))\n",
    "        list_epoch_startDT.append(epoch_startDT)\n",
    "        list_start_0ut.append(start_0ut)\n",
    "        list_end___0ut.append(end___0ut)\n",
    "        list_epoch_endDT.append(epoch_endDT)\n",
    "    \n",
    "    \n",
    "        scalestart = str(pd.to_datetime(arc_startDT,format=\"%Y.%j\"))\n",
    "        scalestop  = str(pd.to_datetime(arc_startDT,format=\"%Y.%j\") + pd.to_timedelta(24,'h'))\n",
    "        scaletimes = pd.date_range(start = pd.to_datetime(scalestart),\n",
    "                                   end   = pd.to_datetime(scalestop ),\n",
    "                                   freq=str(scale_cadence)+\"H\")\n",
    "\n",
    "        ###append the epoch end time to the end\n",
    "        add_epochend = np.append(scaletimes.values.astype(np.int64) // 10 ** 9,\n",
    "                      pd.to_datetime(pd.Series(epoch_endDT)).values.astype(np.int64) // 10 ** 9)\n",
    "        scaleparameter_times.append([pd.to_datetime(datetime.strftime(datetime.fromtimestamp(ts), '%y%m%d%H%M%S.%f'),\n",
    "            format ='%y%m%d%H%M%S.%f' )\n",
    "                         for ts in add_epochend ])\n",
    "        print(f\" scaleparameter_times     {scaleparameter_times} \")\n",
    "\n",
    "    return(list_arc_startDT,\n",
    "           list_epoch_startDT,\n",
    "           list_start_0ut,\n",
    "           list_end___0ut,\n",
    "           list_epoch_endDT,\n",
    "           scaleparameter_times) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sat_List = ['spire083']  \n",
    "# test_timeperiod = [\"2018-11-02 00:00:00\", \"2018-11-29 00:00:00\"]  # 4 days\n",
    "test_timeperiod = [\"2018-11-07 00:00:00\", \"2018-11-08 00:00:00\"]  # 4 days\n",
    "\n",
    "\n",
    "for isat,satval in enumerate(Sat_List):\n",
    "    satnum = int(satval[5:])\n",
    "    file_raw_ICs = f\"/data/SatDragModelValidation/data/inputs/\"\\\n",
    "              +f\"sat_spire{satnum:03d}/g2b/Spire{satnum:03d}_RawEphem_20181101_20181130.txt\"\n",
    "\n",
    "    (arc_start, epoch_startDT, start_0ut,\n",
    "    end___0ut, epoch_endDT, scaleparameter_times) = get_dates(test_timeperiod, file_raw_ICs)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9838b968",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T21:24:01.470995Z",
     "start_time": "2024-03-20T21:17:32.874991Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "spire083\n",
      "check sat Spire\n",
      "----------------------------------------------------------------------------\n",
      "Initializing the time period from 2018-11-01 00:00:00 to 2018-12-01 00:00:00\n",
      "     overwriting the epoch start and stop to match\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "self.prms['arc'] ['2018.305', '2018.306', '2018.307', '2018.308', '2018.309', '2018.310', '2018.311', '2018.312', '2018.313', '2018.314', '2018.315', '2018.316', '2018.317', '2018.318', '2018.319', '2018.320', '2018.321', '2018.322', '2018.323', '2018.324', '2018.325', '2018.326', '2018.327', '2018.328', '2018.329', '2018.330', '2018.331', '2018.332', '2018.333', '2018.334']\n",
      "spire083\n",
      "check sat Spire\n",
      "Step 0: Make directory structure for satellite input data\n",
      "Directory Exists:  /data/SatDragModelValidation/data/inputs/sat_spire083\n",
      "Directory Exists:  /data/SatDragModelValidation/data/inputs/sat_spire083/setups\n",
      "Directory Exists:  /data/SatDragModelValidation/data/inputs/sat_spire083/external_attitude\n",
      "Directory Exists:  /data/SatDragModelValidation/data/inputs/sat_spire083/g2b\n",
      "Step 1: Construct daily external Attitude files\n",
      "   Making an external attitude file: EXAT01.2018.305\n",
      "        - converting dates from GPS to TDT.\n",
      "        - converting quaternions from SBF-->RSW to SBF-->J2000\n",
      "        - Interpolate Quaternions to linearly spaced time series from 2018-10-31 12:00:00 to 2018-11-03 00:00:00\n",
      "        - Write to a binary EXAT file.\n",
      "        - Reached end of attitude data.  Closing the File\n",
      "        - Saving external attitude as a text file\n",
      "        - filetxt /data/SatDragModelValidation/data/inputs/sat_spire083/external_attitude/EXAT01.2018.305_check.txt\n",
      "   Making an external attitude file: EXAT01.2018.306\n",
      "   Making an external attitude file: EXAT01.2018.307\n",
      "   Making an external attitude file: EXAT01.2018.308\n",
      "   Making an external attitude file: EXAT01.2018.309\n",
      "   Making an external attitude file: EXAT01.2018.310\n",
      "   Making an external attitude file: EXAT01.2018.311\n",
      "   Making an external attitude file: EXAT01.2018.312\n",
      "   Making an external attitude file: EXAT01.2018.313\n",
      "   Making an external attitude file: EXAT01.2018.314\n",
      "   Making an external attitude file: EXAT01.2018.315\n",
      "   Making an external attitude file: EXAT01.2018.316\n",
      "   Making an external attitude file: EXAT01.2018.317\n",
      "   Making an external attitude file: EXAT01.2018.318\n",
      "   Making an external attitude file: EXAT01.2018.319\n",
      "   Making an external attitude file: EXAT01.2018.320\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-92d20f07c4de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0msat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPygeodyn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings_spire\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m sat.initialize_timeperiod_stage1(startdate, enddate,\n\u001b[0m\u001b[1;32m    111\u001b[0m                              \u001b[0moverwrite_exat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                              overwrite_ICtext=False)\n",
      "\u001b[0;32m/data/SatDragModelValidation/pygeodyn/pygeodyn/PYGEODYN.py\u001b[0m in \u001b[0;36minitialize_timeperiod_stage1\u001b[0;34m(self, startdate, enddate, overwrite_exat, overwrite_ICtext)\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 self.prep_exat_check(self.raw_satinput,             \\\n\u001b[0m\u001b[1;32m    355\u001b[0m                                  \u001b[0mbool_overwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite_exat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                                  verbose=self.verbose)\n",
      "\u001b[0;32m/data/SatDragModelValidation/pygeodyn/pygeodyn/prep_inputs.py\u001b[0m in \u001b[0;36mprep_exat_check\u001b[0;34m(self, raw_satinput, bool_overwrite, verbose)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.tab} Making an external attitude file: {self.filename_exat}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_write_exat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_satinput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/SatDragModelValidation/pygeodyn/pygeodyn/prep_inputs.py\u001b[0m in \u001b[0;36mmake_write_exat\u001b[0;34m(self, raw_satinput, verbose)\u001b[0m\n\u001b[1;32m    109\u001b[0m                                     \u001b[0;31m# note: the start and end epoch are expanded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                                     \u001b[0;31m#       by an hour on either side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         SpireDF = load_attitude_spire(file__AttitudeRaw,\n\u001b[0m\u001b[1;32m    112\u001b[0m                                         \u001b[0mstartEpoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                                         stopEpoch)\n",
      "\u001b[0;32m/data/SatDragModelValidation/pygeodyn/pygeodyn/satellite_spire.py\u001b[0m in \u001b[0;36mload_attitude_spire\u001b[0;34m(filename, start_date, stop_date)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0mmsec\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminute\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsec\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                 \u001b[0;32mif\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_gps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                     \u001b[0;31m#print('Found a copy, SKIP', date)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                     \u001b[0mnoSkip_flag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from gc import collect as gc_collect\n",
    "import pickle \n",
    "import os\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "\n",
    "# g2b_path = \"/data/SatDragModelValidation/data/inputs/sat_icesat2/g2b/\"\n",
    "g2b_path = \"/data/SatDragModelValidation/data/inputs/sat_spire083/g2b/\"\n",
    "            \n",
    "dir_save    =  '/data/SatDragModelValidation/data/outputs_clean/'\\\n",
    "             + 'spire083/O2R_comparisons/1_DRIAruns_3hr/'\n",
    "\n",
    "\n",
    "\n",
    "obj = {}\n",
    "\n",
    "for imonth,month in enumerate(month_list):\n",
    "    if month=='oct':\n",
    "        m_num = 10\n",
    "    if month=='nov':\n",
    "        m_num = 11\n",
    "    if month=='dec':\n",
    "        m_num = 12\n",
    "    if month=='jan':\n",
    "        m_num = 1\n",
    "    if month=='feb':\n",
    "        m_num = 2\n",
    "    if month=='mar':\n",
    "        m_num = 3\n",
    "    if month=='apr':\n",
    "        m_num = 4\n",
    "\n",
    "#     file_raw_ICs = f\"{g2b_path}ICESat2_RawEphem_2018_{m_num:02d}.txt\"\n",
    "#     file_g2b     = f\"pce_icesat2_pso_2018_{m_num:02d}\"\n",
    "    file_raw_ICs = f\"{g2b_path}Spire083_RawEphem_20181101_20181130.txt\"\n",
    "    file_g2b     = f\"pce_spire083_leoOrb_20181101_20181130\"\n",
    "\n",
    "    \n",
    "    \n",
    "    for i,den in enumerate(run_list):\n",
    "        settings_spire= {# Basic input settings\n",
    "                     'satellite'      : {'input': 'spire083'},\n",
    "                     'den_model'      : {'input': den},\n",
    "                     'run_type'       : {'input': 'DataReduction_PCE'},\n",
    "                     'run_specifier'  : {'input': '_'+month},\n",
    "                     'cd_model'       : {'input': 'DRIA'},\n",
    "                     'file_string'    : {'input': 'DRIAscaled_'},\n",
    "                     'model_data_path' : {'input': run_dict[month+den]['model_path']},\n",
    "                     'verbose' : {'input': True},\n",
    "                     # Force Model settings\n",
    "                      'cd_type'               : {'input':\"Fixed_CD\"},\n",
    "                      'cd_value'              : {'input':1.0 },\n",
    "                      'scaling_factor'        : {'input':True},\n",
    "                      'hours_between_cd_adj'  : {'input':scaling_cadence},\n",
    "                      #### Comment for unadjusted run:\n",
    "                      'cd_adjustment_boolean' : {'input':True },\n",
    "                    #### DRIA CD Model Parameters\n",
    "                    'cd_model_params' : {'input':{ \n",
    "                            'MS'     : '26.980D0'   ,  #!  molar mass for each panel (g/mol)\n",
    "                            'TW'     : '300.0D0'    ,  #!  temperature of panels  (K)\n",
    "                            'ALPHA'  : '0.890D0'    ,  #!  accomodation coefficient, Alpha is b/w 0 and 1\n",
    "                            'KL'     : '0.0D0'    ,    #!  langmuir parameter\n",
    "                            'FRACOX' : '1.0D0'   ,     #!  fraction of surface covered by atomic oxygen\n",
    "                       }},\n",
    "                      #### ---------------------------------------\n",
    "                     # Run\n",
    "            \n",
    "            \n",
    "#             arc_startDT,\n",
    "#             epoch_startDT,\n",
    "# #             start_0ut,\n",
    "# #             end___0ut,\n",
    "#             epoch_endDT,\n",
    "#             scaleparameter_times\n",
    "                      'arc_type'       : {'input':'Nominal30hr_and_AB'},      \n",
    "                      'step'           : {'input': 60.},\n",
    "                      'orbfil_step'    : {'input': 120.},\n",
    "                      'which_ICfile'   : {'input':file_raw_ICs},\n",
    "                      'which_g2bfile'  : {'input':file_g2b},\n",
    "                        #\n",
    "                      'arc'            : {'input': arc_start},\n",
    "                      'epoch_start'    : {'input': epoch_startDT},\n",
    "                      'epoch_stop'     : {'input': epoch_endDT},  \n",
    "#                 'scaleparameter_times' : {'input': scaleparameter_times},  \n",
    "                'scaleparameter_times' : {'input': None},  \n",
    "                       #                                \n",
    "                      'global_options' : {'input':'pso_2018'},\n",
    "                     # Request read on raw outputs\n",
    "                      'request_data'   : {'input': ['Trajectory_orbfil', \n",
    "                                                   'Density', \n",
    "                                                   'Residuals_summary',\n",
    "                                                   'DragFile',\n",
    "                                                   'AdjustedParams'\n",
    "                                                   ]},\n",
    "                  #end dict\n",
    "                  }\n",
    "\n",
    "#         sat = Pygeodyn(settings_spire, use_file=False)\n",
    "#         sat.run_arcs()\n",
    "#         obj[month+den] =  sat.getData_BigData_lowmemory(orbit_propagation=False)\n",
    "#         obj[month+den] = vars(obj[month+den])\n",
    "#         gc_collect()\n",
    "\n",
    "startdate = \"2018-11-01\"\n",
    "enddate   = \"2018-11-30\"  #24\n",
    "\n",
    "sat = Pygeodyn(settings_spire, use_file=False)\n",
    "sat.initialize_timeperiod_stage1(startdate, enddate,\n",
    "                             overwrite_exat=True, \n",
    "                             overwrite_ICtext=False)\n",
    "sat = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0342b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T21:24:01.474284Z",
     "start_time": "2024-03-20T21:16:53.987Z"
    }
   },
   "outputs": [],
   "source": [
    "arc_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa75d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T21:24:01.475265Z",
     "start_time": "2024-03-20T21:16:53.989Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#         pickleName = f'_{month}_DRIA_scale{scale_cadence}.pkl'\n",
    "\n",
    "#         pickle_file = dir_save+den+pickleName\n",
    "#         if not os.path.exists(pickle_file):\n",
    "#             print('Must create pickle file...')\n",
    "#             print('   ',  pickle_file)\n",
    "#             print('   ', 'Reading Geodyn Data')\n",
    "\n",
    "#             ### Load the data into an object\n",
    "#             sat = Pygeodyn(settings_icesat2, use_file=False)\n",
    "#             obj = sat.getData_BigData_lowmemory(orbit_propagation=False)\n",
    "#             gc_collect()\n",
    "\n",
    "#             #### Pickle the object to save it\n",
    "#             print('   ', 'Saving pickle')\n",
    "#             filehandler = open(pickle_file, 'wb') \n",
    "#             pickle.dump(vars(obj), filehandler)\n",
    "#             filehandler.close()\n",
    "#             obj = 0\n",
    "#             print('   ', 'Saved pickle')\n",
    "\n",
    "# obj = {}\n",
    "# for i,model in enumerate(run_list):\n",
    "#     for imonth,month in enumerate(month_list):\n",
    "#         pickleName = f'_{month}_DRIA_scale{scale_cadence}.pkl'\n",
    "\n",
    "#         ### Load the data if the pickles exist\n",
    "#         print()\n",
    "#         print()\n",
    "#         gc_collect()\n",
    "\n",
    "#         pickle_file = dir_save+model+pickleName\n",
    "\n",
    "#         filehandler = open(pickle_file, 'rb') \n",
    "#         obj[month+model] = pickle.load(filehandler)\n",
    "#         filehandler.close()\n",
    "#         print('Loaded data from pickle... ',  model)\n",
    "    \n",
    "    \n",
    "# ### Save space if doing density retrieval\n",
    "# for model in run_dict.keys():\n",
    "#     del obj[model]['OrbitResids']\n",
    "#     del obj[model]['Trajectory_orbfil']\n",
    "    \n",
    "# gc_collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
