{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe7a07b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T19:34:05.173755Z",
     "start_time": "2025-01-14T19:34:04.596156Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "import sys\n",
    "from gc import collect as gc_collect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c651410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T17:12:19.233457Z",
     "start_time": "2024-07-16T17:12:19.211294Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12a7e430",
   "metadata": {},
   "source": [
    "# initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b58f39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T19:34:05.223042Z",
     "start_time": "2025-01-14T19:34:05.176244Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pygeodyn.PYGEODYN import Pygeodyn\n",
    "from gc import collect as gc_collect\n",
    "\n",
    "scaling_cadence = 3\n",
    "scale_cadence = scaling_cadence\n",
    "run_list = [\n",
    "#                 'msis2',\n",
    "#                 'jb2008'\n",
    "                'dtm2020_o',\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3c656",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "Exat naming conventions\n",
    "Check the external att files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e325f397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:13:24.111915Z",
     "start_time": "2025-01-14T19:34:05.227681Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using the ICESat-2 Class\n",
      "['2019-04-28', '2019-06-03']\n",
      "  * Global Option not needed during Initializing Stage\n",
      "check sat ICESat2\n",
      "----------------------------------------------------------------------------\n",
      "Initializing the time period from 2019-03-28 00:00:00 to 2019-05-04 00:00:00\n",
      "     overwriting the epoch start and stop to match\n",
      "     self.prms['arc'] ['2019.090', '2019.091A', '2019.092A', '2019.093', '2019.094', '2019.095', '2019.096', '2019.097', '2019.098', '2019.099', '2019.100', '2019.101', '2019.102', '2019.103', '2019.104', '2019.105', '2019.106A', '2019.107A', '2019.108', '2019.109', '2019.110', '2019.111', '2019.112', '2019.113', '2019.114', '2019.115', '2019.116', '2019.117', '2019.118', '2019.119', '2019.120', '2019.121', '2019.122A', '2019.123A']\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "Using the ICESat-2 Class\n",
      "[Timestamp('2019-03-31 00:00:00'), Timestamp('2019-04-01 00:00:00'), Timestamp('2019-04-02 00:00:00'), Timestamp('2019-04-03 00:00:00'), Timestamp('2019-04-04 00:00:00'), Timestamp('2019-04-05 00:00:00'), Timestamp('2019-04-06 00:00:00'), Timestamp('2019-04-07 00:00:00'), Timestamp('2019-04-08 00:00:00'), Timestamp('2019-04-09 00:00:00'), Timestamp('2019-04-10 00:00:00'), Timestamp('2019-04-11 00:00:00'), Timestamp('2019-04-12 00:00:00'), Timestamp('2019-04-13 00:00:00'), Timestamp('2019-04-14 00:00:00'), Timestamp('2019-04-15 00:00:00'), Timestamp('2019-04-16 00:00:00'), Timestamp('2019-04-17 00:00:00'), Timestamp('2019-04-18 00:00:00'), Timestamp('2019-04-19 00:00:00'), Timestamp('2019-04-20 00:00:00'), Timestamp('2019-04-21 00:00:00'), Timestamp('2019-04-22 00:00:00'), Timestamp('2019-04-23 00:00:00'), Timestamp('2019-04-24 00:00:00'), Timestamp('2019-04-25 00:00:00'), Timestamp('2019-04-26 00:00:00'), Timestamp('2019-04-27 00:00:00'), Timestamp('2019-04-28 00:00:00'), Timestamp('2019-04-29 00:00:00'), Timestamp('2019-04-30 00:00:00'), Timestamp('2019-05-01 00:00:00'), Timestamp('2019-05-02 00:00:00'), Timestamp('2019-05-03 00:00:00')]\n",
      "  * Global Option not needed during Initializing Stage\n",
      "check sat ICESat2\n",
      "Step 0: Make directory structure for satellite input data\n",
      "  * Directory Exists:  /data/SatDragModelValidation/data/inputs/sat_icesat2\n",
      "  * Directory Exists:  /data/SatDragModelValidation/data/inputs/sat_icesat2/setups\n",
      "  * Directory Exists:  /data/SatDragModelValidation/data/inputs/sat_icesat2/external_attitude\n",
      "  * Directory Exists:  /data/SatDragModelValidation/data/inputs/sat_icesat2/g2b\n",
      "Do not update EXAT files for  icesat2\n",
      "\n",
      "Step 2: Construct/Find a PCE file\n",
      "   Making PCE g2b file: /data/SatDragModelValidation/data/inputs/sat_icesat2/g2b/pce_icesat2_pso_20190331_20190503\n",
      "        - Initialize Raw Satellite Ephemerides as UTC, Cartesian-ECI-J2000\n",
      "satellite icesat2\n",
      "        - ICESAT-2\n",
      "        - processing raw satellite ephemerides from files.\n",
      "        - for dates: DatetimeIndex(['2019-03-31', '2019-04-01', '2019-04-02', '2019-04-03',\n",
      "               '2019-04-04', '2019-04-05', '2019-04-06', '2019-04-07',\n",
      "               '2019-04-08', '2019-04-09', '2019-04-10', '2019-04-11',\n",
      "               '2019-04-12', '2019-04-13', '2019-04-14', '2019-04-15',\n",
      "               '2019-04-16', '2019-04-17', '2019-04-18', '2019-04-19',\n",
      "               '2019-04-20', '2019-04-21', '2019-04-22', '2019-04-23',\n",
      "               '2019-04-24', '2019-04-25', '2019-04-26', '2019-04-27',\n",
      "               '2019-04-28', '2019-04-29', '2019-04-30', '2019-05-01',\n",
      "               '2019-05-02', '2019-05-03'],\n",
      "              dtype='datetime64[ns]', freq='D')\n",
      "      Running through the pre-processing procedure...\n",
      "      =======================================================\n",
      "      STEP 1: Convert RVG binary files to pandas DataFrame...\n",
      "      =======================================================\n",
      "      Loading and processing 34 files will take approx. 30.88 minutes.\n",
      "            Not including unzipping/zipping times\n",
      "\n",
      "      --- File 1 / 34\n",
      "      ----- Loading  2019.090\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 2 / 34\n",
      "      ----- Loading  2019.091A\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 3 / 34\n",
      "      ----- Loading  2019.092A\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 4 / 34\n",
      "      ----- Loading  2019.093\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 5 / 34\n",
      "      ----- Loading  2019.094\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 6 / 34\n",
      "      ----- Loading  2019.095\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 7 / 34\n",
      "      ----- Loading  2019.096\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 8 / 34\n",
      "      ----- Loading  2019.097\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 9 / 34\n",
      "      ----- Loading  2019.098\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 10 / 34\n",
      "      ----- Loading  2019.099\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 11 / 34\n",
      "      ----- Loading  2019.100\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 12 / 34\n",
      "      ----- Loading  2019.101\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 13 / 34\n",
      "      ----- Loading  2019.102\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 14 / 34\n",
      "      ----- Loading  2019.103\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 15 / 34\n",
      "      ----- Loading  2019.104\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 16 / 34\n",
      "      ----- Loading  2019.105\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 17 / 34\n",
      "      ----- Loading  2019.106A\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 18 / 34\n",
      "      ----- Loading  2019.107A\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 19 / 34\n",
      "      ----- Loading  2019.108\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 20 / 34\n",
      "      ----- Loading  2019.109\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 21 / 34\n",
      "      ----- Loading  2019.110\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 22 / 34\n",
      "      ----- Loading  2019.111\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 23 / 34\n",
      "      ----- Loading  2019.112\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 24 / 34\n",
      "      ----- Loading  2019.113\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 25 / 34\n",
      "      ----- Loading  2019.114\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 26 / 34\n",
      "      ----- Loading  2019.115\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 27 / 34\n",
      "      ----- Loading  2019.116\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 28 / 34\n",
      "      ----- Loading  2019.117\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 29 / 34\n",
      "      ----- Loading  2019.118\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 30 / 34\n",
      "      ----- Loading  2019.119\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 31 / 34\n",
      "      ----- Loading  2019.120\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 32 / 34\n",
      "      ----- Loading  2019.121\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 33 / 34\n",
      "      ----- Loading  2019.122A\n",
      "      ----- End of file\n",
      "\n",
      "      --- File 34 / 34\n",
      "      ----- Loading  2019.123A\n",
      "      ----- End of file\n",
      "\n",
      "          saving satellite ephemeris to single file.\n",
      "(2936524,)\n",
      "\n",
      "       Processed file in :  238.4065 secs (3.9734416666666665 minutes)\n",
      "        - Convert UTC to GPS and convert to MJDS\n",
      "                - done with conversion\n",
      "                - done with the above\n",
      "        - Put data in a DataFrame\n",
      "        - Save prepped PCE as TRAJ.txt\n",
      "        - Free up some memory\n",
      "        - pce_fortran.f compiled\n",
      "        - Run fortran PCE converter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘/data/SatDragModelValidation/data/inputs/sat_icesat2/g2b/pce_icesat2_pso_20190331_20190503’: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        - pce_fortran.f executed\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from gc import collect as gc_collect\n",
    "import pickle \n",
    "import os\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "obj = {}\n",
    "\n",
    "# file_raw_ICs = f\"{g2b_path}Spire083_RawEphem_20181101_20181130.txt\"\n",
    "# file_g2b     = f\"pce_spire083_leoOrb_20181101_20181130\"\n",
    "\n",
    "\n",
    "\n",
    "for i,den in enumerate(run_list):\n",
    "    settings_ice2= {# Basic input settings\n",
    "                 'satellite'            : {'input': 'icesat2'},\n",
    "                 'den_model'            : {'input': den},\n",
    "                 'initialize_satellite' : {'input': True},\n",
    "                 'run_type'       : {'input': 'DataReduction_PCE'},\n",
    "                 'epoch_start'    : {'input': [\"2019-04-28\",\n",
    "                                               \"2019-06-03\" ]},\n",
    "\n",
    "\n",
    "#                  'run_specifier'  : {'input': '_'},\n",
    "#                  'cd_model'       : {'input': 'DRIA'},\n",
    "#                  'file_string'    : {'input': 'DRIAscaled_'},\n",
    "#                  'model_data_path' : {'input': run_dict[den]['model_path']},\n",
    "#                  'verbose' : {'input': True},\n",
    "#                  # Force Model settings\n",
    "#                   'cd_value'              : {'input':1.0 },\n",
    "#                   'scaling_factor'        : {'input':True},\n",
    "#                   'hours_between_cd_adj'  : {'input':scaling_cadence},\n",
    "#                   #### Comment for unadjusted run:\n",
    "#                   'cd_adjustment_boolean' : {'input':True },\n",
    "#                 #### DRIA CD Model Parameters\n",
    "#                 'cd_model_params' : {'input':{ \n",
    "#                         'MS'     : '26.980D0'   ,  #!  molar mass for each panel (g/mol)\n",
    "#                         'TW'     : '300.0D0'    ,  #!  temperature of panels  (K)\n",
    "#                         'ALPHA'  : '0.890D0'    ,  #!  accomodation coefficient, Alpha is b/w 0 and 1\n",
    "#                         'KL'     : '0.0D0'    ,    #!  langmuir parameter\n",
    "#                         'FRACOX' : '1.0D0'   ,     #!  fraction of surface covered by atomic oxygen\n",
    "#                    }},\n",
    "#                   #### ---------------------------------------\n",
    "#                  # Run\n",
    "\n",
    "\n",
    "# #             scaleparameter_times\n",
    "#                   'arc_type'       : {'input':'Nominal30hr'},      \n",
    "#                   'step'           : {'input': 60.},\n",
    "#                   'orbfil_step'    : {'input': 120.},\n",
    "# #                   'which_ICfile'   : {'input':file_raw_ICs},\n",
    "# #                   'which_g2bfile'  : {'input':file_g2b},\n",
    "#                     #\n",
    "# #                   'arc'            : {'input': arc_start},\n",
    "# #                   'epoch_start'    : {'input': epoch_startDT},\n",
    "# #                   'epoch_stop'     : {'input': epoch_endDT},  \n",
    "# #                 'scaleparameter_times' : {'input': scaleparameter_times},  \n",
    "#                    #                                \n",
    "# #                   'global_options' : {'input':'pso_2018'},\n",
    "#                  # Request read on raw outputs\n",
    "#                   'request_data'   : {'input': ['Trajectory_orbfil', \n",
    "#                                                'Density', \n",
    "#                                                'Residuals_summary',\n",
    "#                                                'DragFile',\n",
    "#                                                'AdjustedParams'\n",
    "#                                                ]},\n",
    "              #end dict\n",
    "     }\n",
    "\n",
    "startdate = \"2019-03-28\"  \n",
    "enddate   = \"2019-05-03\"  \n",
    "\n",
    "sat = Pygeodyn(settings_ice2, use_file=False)\n",
    "sat.initialize_timeperiod_stage1(startdate, enddate,\n",
    "                             overwrite_exat=False, \n",
    "                             overwrite_ICtext=True)\n",
    "sat = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03749169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12dfcf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03873fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79eb35b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:13:24.133164Z",
     "start_time": "2025-01-14T20:13:24.114653Z"
    }
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda3/envs/pygeodyn2/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a766920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T23:15:00.754914Z",
     "start_time": "2024-06-24T23:15:00.749057Z"
    }
   },
   "source": [
    "# Process Raw Ephemerides-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e36895a",
   "metadata": {},
   "source": [
    "## read binary orbfil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a231f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:13:24.137537Z",
     "start_time": "2025-01-14T20:13:24.137518Z"
    }
   },
   "outputs": [],
   "source": [
    "from pygeodyn.util_dir.time_systems import time_mjdsecs_tdt_to_utc\n",
    "from pygeodyn.util_dir.time_systems import mjds_to_ymdhms\n",
    "\n",
    "def read_binary_ORBFIL(orb_fil):\n",
    "\n",
    "    from scipy.io import FortranFile\n",
    "    # orb_fil = file\n",
    "    f = FortranFile(orb_fil, 'r')\n",
    "\n",
    "    #### -----------------------------------------------------\n",
    "    #### ------------------- HEADER RECORD -------------------\n",
    "    #### -----------------------------------------------------\n",
    "    ### Read the first record, this is the header buffer\n",
    "    a = f.read_record(float)  # read the record with the required datatype\n",
    "\n",
    "\n",
    "    #### Glean important variables\n",
    "    NA     = int(a[2-1]) # Number of alphanumeric buffers to follow the header\n",
    "    NC     = int(a[3-1]) # Number of card images in the GEODYN II input control deck\n",
    "    NSATS  = int(a[7-1])  # Number of satellites on this file:  \n",
    "    NWDSAT = int(a[8-1])  # Actual number of words per satellite per time point (NWDSAT <= 39).\n",
    "    NWDATA = int(a[9-1])   #NSATS*NWDSAT\n",
    "    NTIMBF = int(a[10-1]) # Number of time points per Data Buffer\n",
    "\n",
    "    header= {}\n",
    "    header['Number of alphanumeric data buffers to follow (NA)']             = a[2-1]\n",
    "    header['Number of card images in the GEODYN II input control deck (NC)'] = a[3-1]\n",
    "    header['Arc Number.']                                                    = a[4-1]\n",
    "    header['Global Iteration Number']                                        = a[5-1]\n",
    "    header['Inner Iteration Number']                                         = a[6-1]\n",
    "    header['Number of satellites on this file']                              = a[7-1]  # upper limit of 50\n",
    "    header['Actual number of words per satellite per time point']            = a[8-1]\n",
    "    header['Number of words of data per time point (NWDATA=NSATS*NWDSAT)']   = a[9-1]\n",
    "    header['Number of time points per Data Buffer (NTIMBF)']                 = a[10-1]\n",
    "    header['Trajectory Start Date & Time in form YYMMDDHHMMSS .0D0 UTC']     = a[11-1]\n",
    "    header['Fractional seconds of Start Time. UTC']                          = a[12-1]\n",
    "    header['Trajectory Stop Date & Time in form YYMMDDHHMMSS .0D0 UTC']      = a[13-1]\n",
    "    header['Fractional seconds of Stop Time. UTC']                           = a[14-1]\n",
    "    header['Trajectory Start Date & Time in MJDS']                           = a[15-1] # (MJDS=(JD -2430000.5 D0 )*86400+ ISEC) ET\n",
    "    header['Fractional seconds of Start Time']                               = a[16-1]\n",
    "    header['Trajectory Stop Date & Time in MJDS']                            = a[17-1] # (MJDS=(JD -2430000.5 D0 )*86400+ ISEC) ET\n",
    "    header['Fractional seconds of Stop Time. ET']                            = a[18-1]\n",
    "    header['Nominal interval between trajectory times in seconds. ET ']      = a[19-1]\n",
    "    header['Nominal number of trajectory times.']                            = a[20-1]\n",
    "    header['Output S/C ephem ref sys(0 = TOD, 1 = TOR, 2 =  J2000)']         = a[22-1]\n",
    "    # \n",
    "    header['Speed of Light.']                                     = a[101-1]\n",
    "    header['GM for Earth.']                                       = a[102-1]\n",
    "    header['Semi -major axis of Earth reference ellipsoid.']      = a[103-1]\n",
    "    header['Equatorial Flattening of Earth reference ellipsoid.'] = a[104-1]\n",
    "    header['Gravitational Potential Checksum.']                   = a[105-1]\n",
    "    header['Maximum Degree of Gravitational Expansion.']          = a[106-1]\n",
    "    header['Maximum Order of Gravitational Expansion.']           = a[107-1]   ### SKIP from 108 -200\n",
    "    #\n",
    "    #### PRESENCE ON FILE INDICATORS\n",
    "    ## right ascension of Greenwich \n",
    "    header['Presence of right ascension of Greenwich for each time point in each Buffer'] = a[201-1]   \n",
    "    ## Inertial State Vector\n",
    "    header['Presence per Sat. of inertial X coordinate for each time point']    = a[202-1]   \n",
    "    header['Presence per Sat. of inertial Y coordinate for each time point']    = a[203-1]   \n",
    "    header['Presence per Sat. of inertial Z coordinate for each time point']    = a[204-1]   \n",
    "    header['Presence per Sat. of inertial Xdot coordinate for each time point'] = a[205-1]   \n",
    "    header['Presence per Sat. of inertial Ydot coordinate for each time point'] = a[206-1]   \n",
    "    header['Presence per Sat. of inertial Zdot coordinate for each time point'] = a[207-1] \n",
    "    # \n",
    "    header['Presence per Sat. of geodetic latitude for each time point'] = a[208-1]   \n",
    "    header['Presence per Sat. of east longitude for each time point']    = a[209-1]   \n",
    "    # \n",
    "    header['Presence per Sat. of ECF X coordinate for each time point']  = a[210-1]   \n",
    "    header['Presence per Sat. of ECF Y coordinate for each time point']  = a[211-1]   \n",
    "    header['Presence per Sat. of ECF Z coordinate for each time point']  = a[212-1]   \n",
    "    header['Presence per Sat. of ECF Xdot for each time point']         = a[213-1]   \n",
    "    header['Presence per Sat. of ECF Ydot for each time point']         = a[214-1]   \n",
    "    header['Presence per Sat. of ECF Zdot for each time point']         = a[215-1]   \n",
    "    header['Presence per Sat. of polar motion X for each time point']   = a[216-1]   \n",
    "    header['Presence per Sat. of polar motion Y for each time point']   = a[217-1]   \n",
    "    header['Presence per Sat. of beta prime angle for each time point'] = a[218-1]   \n",
    "    header['Presence per Sat. of yaw angle for each time point']        = a[219-1]   \n",
    "    header['Presence per Sat. of orbit angle for each time point']      = a[220-1]   \n",
    "\n",
    "\n",
    "\n",
    "    # break\n",
    "    ##### Satellite ID ’s for all Satellites on File.\n",
    "    ###       Trajectory data is ordered based upon order of these Satellite ID ’s.'\n",
    "    for i in range(int(NSATS)):\n",
    "        ii = i + 1\n",
    "        index_sats = 300 + (ii)\n",
    "        header['Satellite '+str(ii)+' ID'] = a[index_sats-1]  \n",
    "\n",
    "\n",
    "    #### ----------------------------------------------------\n",
    "    #### --------------- ALPHANUMERIC RECORDS ---------------\n",
    "    #### ----------------------------------------------------\n",
    "    #### We don't care about the Alphanumeric buffers so skip over them.\n",
    "    for i in range(int(NA)):\n",
    "        a = f.read_record(float)\n",
    "\n",
    "\n",
    "    #### -----------------------------------------------------\n",
    "    #### -------------- DATA + SENTINEL RECORDS --------------\n",
    "    #### -----------------------------------------------------\n",
    "    ### Read the Data records in a while loop.  \n",
    "    ### When we hit the end_data_val, we have reached the\n",
    "    ###    sentinel record and we can exit the while loop \n",
    "    ###    to read in the sentinel buffer.\n",
    "\n",
    "\n",
    "    end_data_val           = 9000000000\n",
    "    end_datarecord         = False\n",
    "    data_dict_times        = {}\n",
    "    data_dict_RA_greenwich = {}\n",
    "    data_dict_sat_packets  = {}\n",
    "\n",
    "    count_while = 0\n",
    "\n",
    "    data_dict_sat_packets['MJDSEC ET']                       =[]\n",
    "    data_dict_sat_packets['X j2000'] =[]\n",
    "    data_dict_sat_packets['Y j2000'] =[]\n",
    "    data_dict_sat_packets['Z j2000'] =[]\n",
    "    data_dict_sat_packets['X_dot j2000']   =[]\n",
    "    data_dict_sat_packets['Y_dot j2000']   =[]\n",
    "    data_dict_sat_packets['Z_dot j2000']   =[]\n",
    "    data_dict_sat_packets['Geodetic Latitude']     =[]\n",
    "    data_dict_sat_packets['East Longitude']        =[]\n",
    "    data_dict_sat_packets['Height']                =[]\n",
    "    #         data_dict_sat_packets['Satellite ECF X coordinate']      =[]\n",
    "    #         data_dict_sat_packets['Satellite ECF Y coordinate']      =[]\n",
    "    #         data_dict_sat_packets['Satellite ECF Z coordinate']      =[]\n",
    "    #         data_dict_sat_packets['Satellite ECF X velocity']        =[]\n",
    "    #         data_dict_sat_packets['Satellite ECF Y velocity']        =[]\n",
    "    #         data_dict_sat_packets['Satellite ECF Z velocity']        =[]\n",
    "    #         data_dict_sat_packets['Polar Motion X']                  =[]\n",
    "    #         data_dict_sat_packets['Polar Motion Y']                  =[]\n",
    "    #         data_dict_sat_packets['Beta prime angle']                =[]\n",
    "    #         data_dict_sat_packets['Yaw angle']                       =[]\n",
    "    #         data_dict_sat_packets['Orbit Angle']                     =[]\n",
    "    #         data_dict_sat_packets['Q(1)']                            =[]\n",
    "    #         data_dict_sat_packets['Q(2)']                            =[]\n",
    "    #         data_dict_sat_packets['Q(3)']                            =[]\n",
    "    #         data_dict_sat_packets['Q(4)']                            =[]\n",
    "\n",
    "    while end_datarecord == False:\n",
    "        ### Read in each data buffer\n",
    "        a = f.read_record(float)\n",
    "\n",
    "        if not end_data_val in a:\n",
    "            count_while+=1\n",
    "            NTB    = int(a[5-1])  # Number of trajectory times in this Data Buffer (NTB <= NTIMBF ).\n",
    "            MJDSBF = a[4-1]\n",
    "\n",
    "            #### Trajectory Times in elapsed ET seconds from MJDSBF\n",
    "            counter = 0\n",
    "            for itime in np.arange( (6)   ,   ((NTB+5)  +1)  ):\n",
    "                index_times = int(itime)\n",
    "                data_dict_times[counter] = str(MJDSBF + a[index_times-1] )\n",
    "                counter+=1\n",
    "\n",
    "    #             if counter <= 100:\n",
    "    #                 print(MJDSBF + a[index_times-1])\n",
    "\n",
    "\n",
    "            #### Right Ascension of Greenwich Values (radians) for each time in Buffer.\n",
    "            counter = 0\n",
    "            for i in np.arange((NTIMBF+6) ,((NTIMBF+5 + NTB)+1)):\n",
    "                counter+=1\n",
    "                index = int(i)\n",
    "                data_dict_RA_greenwich['Right Ascension of Greenwich Values '+ str(counter)] = a[index-1] \n",
    "\n",
    "\n",
    "            ##### Satellite Data Packets\n",
    "            #####    first satellite \n",
    "            #####    first time point \n",
    "            counter = 0        \n",
    "            first_sat_first_time = ((NSATS +1)* NTIMBF +6) + (NSATS -1)* NWDSAT #2* NTIMBF +6\n",
    "            last_sat_last_time   = ((NSATS +1)* NTIMBF +5) + NSATS*NWDSAT*NTB #(((NSATS+1)* NTIMBF+5)+(NSATS*NWDSAT))\n",
    "\n",
    "            for i in np.arange(first_sat_first_time, last_sat_last_time  , 24):\n",
    "                index = int(i)\n",
    "\n",
    "                data_dict_sat_packets['MJDSEC ET'].append(data_dict_times[counter])\n",
    "                data_dict_sat_packets['X j2000'].append(a[(index +1) - 2])\n",
    "                data_dict_sat_packets['Y j2000'].append(a[(index +2) - 2])\n",
    "                data_dict_sat_packets['Z j2000'].append(a[(index +3) - 2])\n",
    "                data_dict_sat_packets['X_dot j2000'].append(a[(index +4) - 2])\n",
    "                data_dict_sat_packets['Y_dot j2000'].append(a[(index +5) - 2])\n",
    "                data_dict_sat_packets['Z_dot j2000'].append(a[(index +6) - 2])\n",
    "                data_dict_sat_packets['Geodetic Latitude'].append(a[(index +7) - 2])\n",
    "                data_dict_sat_packets['East Longitude'].append(a[(index +8) - 2])\n",
    "                data_dict_sat_packets['Height'].append(a[(index +9) - 2])\n",
    "    #                     data_dict_sat_packets['Satellite ECF X coordinate'].append(a[(index +10) - 2])\n",
    "    #                     data_dict_sat_packets['Satellite ECF Y coordinate'].append(a[(index +11) - 2])\n",
    "    #                     data_dict_sat_packets['Satellite ECF Z coordinate'].append(a[(index +12) - 2])\n",
    "    #                     data_dict_sat_packets['Satellite ECF X velocity'].append(a[(index +13) - 2])\n",
    "    #                     data_dict_sat_packets['Satellite ECF Y velocity'].append(a[(index +14) - 2])\n",
    "    #                     data_dict_sat_packets['Satellite ECF Z velocity'].append(a[(index +15) - 2])\n",
    "    #                     data_dict_sat_packets['Polar Motion X'].append(a[(index +16) - 2])\n",
    "    #                     data_dict_sat_packets['Polar Motion Y'].append(a[(index +17) - 2])\n",
    "    #                     data_dict_sat_packets['Beta prime angle'].append(a[(index +18) - 2])\n",
    "    #                     data_dict_sat_packets['Yaw angle'].append(a[(index +19) - 2])\n",
    "    #                     data_dict_sat_packets['Orbit Angle'].append(a[(index +20) - 2])\n",
    "    #                     data_dict_sat_packets['Q(1)'].append(a[(index +21) - 2])\n",
    "    #                     data_dict_sat_packets['Q(2)'].append(a[(index +22) - 2])\n",
    "    #                     data_dict_sat_packets['Q(3)'].append(a[(index +23) - 2])\n",
    "    #                     data_dict_sat_packets['Q(4)'].append(a[(index +24) - 2])\n",
    "                counter+=1\n",
    "\n",
    "\n",
    "    #         print('counter',counter)    \n",
    "\n",
    "        else:\n",
    "            ####  If the the first index has +9000000000 we are at the sentinel record \n",
    "            #     which denotes the end of the data section.\n",
    "    #                 print('----- End of file')\n",
    "    #                 print('sentinel buffer indicator                       ',a[1-1])\n",
    "    #                 print('Count of the number of Data Buffers. GEODYN     ',a[2-1])\n",
    "    #                 print('GEODYN II Interface File creation date and time.',a[3-1])\n",
    "    #                 print('GEODYN II -S version used.                      ',a[4-1])\n",
    "    #                 print('GEODYN II -E version used.                      ',a[5-1])\n",
    "    #                 print('spare                                           ',a[6-1])\n",
    "    #                 print('spare                                           ',a[7-1])\n",
    "            end_datarecord = True\n",
    "            f.close()  #### be sure to close the file\n",
    "\n",
    "\n",
    "    data_record_df = pd.DataFrame.from_dict(data_dict_sat_packets, orient='columns')\n",
    "\n",
    "    #### Save as a dictionary\n",
    "    orbfil_dict = {}\n",
    "    orbfil_dict['header'] = header\n",
    "    orbfil_dict['data_record'] = data_record_df\n",
    "\n",
    "    ##### Convert from Terrestrial time to UTC:\n",
    "    MJDS_UTC = [time_mjdsecs_tdt_to_utc(float(x), 37) \\\n",
    "                for x in orbfil_dict['data_record']['MJDSEC ET'] ]\n",
    "\n",
    "    ##### Calculate the Gregorian Calendar date:\n",
    "    yymmdd_str = [mjds_to_ymdhms(x) for x in MJDS_UTC]\n",
    "\n",
    "    ##### Compute date as Datetime object:\n",
    "    dates_dt_UTC = [pd.to_datetime( x, format='%y%m%d-%H%M%S') for x in yymmdd_str]\n",
    "\n",
    "\n",
    "    orbfil_dict['data_record'][\"Date_UTC\"] = dates_dt_UTC\n",
    "    orbfil_dict['data_record'][\"MJDS_UTC\"] = MJDS_UTC\n",
    "\n",
    "    return(orbfil_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8d9bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:13:24.139588Z",
     "start_time": "2025-01-14T20:13:24.139571Z"
    }
   },
   "outputs": [],
   "source": [
    "gfo_traj_filename =  \"/data/SatDragModelValidation/data/inputs/raw_inputdata/data_GRACEFO/traj_raw/geodyn.traj.20181101\"\n",
    "\n",
    "gfo_traj_filename =  \"/data/SatDragModelValidation/data/inputs/raw_inputdata/data_GRACEFO/traj_raw/geodyn.traj.20181101\"\n",
    "\n",
    "\n",
    "# Read in the binary data into a dict w/ pd.dataframes\n",
    "traj_data = read_binary_ORBFIL(gfo_traj_filename)\n",
    "\n",
    "\n",
    "\n",
    "# sat_process_raw_ephemeris(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6f62f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:13:24.140596Z",
     "start_time": "2025-01-14T20:13:24.140581Z"
    }
   },
   "outputs": [],
   "source": [
    "traj_data['header']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe309dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:13:24.142114Z",
     "start_time": "2025-01-14T20:13:24.142099Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f6ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:13:24.143406Z",
     "start_time": "2025-01-14T20:13:24.143382Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_timechopped_trajdata_1(path_binary, arc_files):\n",
    "  \n",
    "    \n",
    "    total_files=np.size(arc_files)\n",
    "    tabtab = '     '\n",
    "\n",
    "    print(tabtab,'Running through the pre-processing procedure...')\n",
    "    print(tabtab,'=======================================================')\n",
    "    print(tabtab,'STEP 1: Convert TRAJ binary files to pandas DataFrame...')\n",
    "    print(tabtab,'=======================================================')\n",
    "    \n",
    "    time_estimate_onAWS = 0.0005 # ~ num of secs per record ( 56.11/ 109434)\n",
    "    rate = (total_files*time_estimate_onAWS*109000)/60\n",
    "    print(tabtab, f\"Loading and processing {total_files} files will take approx. {rate:.2f} minutes.\")\n",
    "    print(tabtab,tabtab, 'Not including unzipping/zipping times')\n",
    "\n",
    "    print()\n",
    "#         print(tabtab,'First we unzip the files... check the console for progess.')\n",
    "    os.chdir(path_binary)\n",
    "\n",
    "\n",
    "    # Initialize a Dataframe \n",
    "    df1 = pd.DataFrame()\n",
    "\n",
    "    count=0\n",
    "    for i, file in enumerate(arc_files):\n",
    "\n",
    "        file = pd.to_datetime(file, format = '%Y.%j').strftime('%Y%m%d')\n",
    "        \n",
    "        filenum  =i+1 \n",
    "        print(tabtab, '--- File %i / %i'% (filenum, total_files))\n",
    "\n",
    "        print(tabtab, '----- Loading ', file  )\n",
    "\n",
    "\n",
    "        __traj_filename =  path_binary + f\"geodyn.traj.{file}\"\n",
    "\n",
    "\n",
    "        # Read in the binary data into a dict w/ pd.dataframes\n",
    "        traj_data = read_binary_ORBFIL(__traj_filename)\n",
    "        \n",
    "        traj_data = pd.DataFrame.from_dict(traj_data['data_record']) \n",
    "            \n",
    "        if count == 0:\n",
    "            df1 = traj_data\n",
    "            count += 1\n",
    "        else:\n",
    "            # to append df2 at the end of df1 dataframe\n",
    "            df1 = pd.concat([df1, traj_data])\n",
    "\n",
    "\n",
    "        df1 = df1.drop_duplicates(subset=[\"Date_UTC\"], keep='first'\\\n",
    "        ).sort_values(by='Date_UTC'\\\n",
    "                        ).reset_index(drop=True)\n",
    "\n",
    "        del traj_data\n",
    "        gc_collect()\n",
    "\n",
    "#         print(tabtab,'Zipping file...', file)\n",
    "#             os.system('gzip -v '+file)\n",
    "\n",
    "        print()\n",
    "\n",
    "    TRAJ_FINAL = df1\n",
    "    return(TRAJ_FINAL)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e119cc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:13:24.144860Z",
     "start_time": "2025-01-14T20:13:24.144845Z"
    }
   },
   "outputs": [],
   "source": [
    "def sat_process_raw_ephemeris( verbose=False):\n",
    "\n",
    "\n",
    "    from datetime import datetime, timedelta\n",
    "    import time\n",
    "    \n",
    "    raw_satinput = {}\n",
    "    prms = {}\n",
    "    path_data_inputs = '/data/SatDragModelValidation/data/inputs'\n",
    "    raw_satinput['ephem_path']  = path_data_inputs +'/'\\\n",
    "                            +f'sat_gracefoc/g2b/'  \\\n",
    "                            +f'GRACEFOC_RawEphem_test_v1.txt'\n",
    "\n",
    "    raw_satinput['ephem_path_dir'] = '/data/SatDragModelValidation/data/inputs/raw_inputdata/data_GRACEFO/traj_raw/'\n",
    "    prms['epoch_start'] = ['2018-11-01 00:00:00']\n",
    "    prms['arc'] = ['2018.305']\n",
    "\n",
    "    tabtab= '           '\n",
    "\n",
    "    prms['sat_ID'] = 2012003\n",
    "    \n",
    "    \n",
    "\n",
    "    raw_ephem = raw_satinput['ephem_path']\n",
    "    ephem_path_dir = raw_satinput['ephem_path_dir']\n",
    "    #\n",
    "    dt_2days = pd.Series(pd.to_timedelta(48,'h'))\n",
    "    startdate = pd.to_datetime(prms['epoch_start'][0])\n",
    "    enddate   = pd.to_datetime(prms['epoch_start'][-1])\n",
    "    startdate_dt = pd.to_datetime(startdate, format='%Y-%m-%d')\n",
    "    enddate_dt   = pd.to_datetime(enddate,   format='%Y-%m-%d')\n",
    "    starts_linspace_dt = pd.date_range(start=startdate_dt ,\n",
    "                                        end=enddate_dt   ,\n",
    "                                        freq=str(1)+\"D\")\n",
    "\n",
    "    if verbose: print(f\"{tabtab} - GRACEFOC\")\n",
    "    if verbose: print(f\"{tabtab} - processing raw satellite ephemerides from files.\")\n",
    "    if verbose: print(f\"{tabtab} - for dates: {starts_linspace_dt}\")\n",
    "\n",
    "\n",
    "    ### PLACE THE RAW FILE PROCESSING CODE HERE\n",
    "    ### ---------------------------------------\n",
    "\n",
    "    ORBFIL_FINAL = get_timechopped_trajdata_1(ephem_path_dir, \\\n",
    "                                        prms['arc'],\\\n",
    "                                        )\n",
    "    ### ---------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    ### Date must be in UTC\n",
    "    ### Ephemerides must be in ECI-J2000\n",
    "\n",
    "    if verbose: print(f\"{tabtab}   saving satellite ephemeris to single file.\")\n",
    "    f = open(raw_ephem, \"w\")\n",
    "    f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    ### Write to file\n",
    "    with open(raw_ephem, 'r+') as file:\n",
    "\n",
    "        #### Manually write the header units\n",
    "        header_units =\\\n",
    "                f\"{'UTC'.rjust(19-1,' ') }\"\\\n",
    "                +f\"  {'(m)'.rjust(15,' ')}\"\\\n",
    "                +f\"  {'(m)'.rjust(15,' ')}\"\\\n",
    "                +f\"  {'(m)'.rjust(15,' ')}\"\\\n",
    "                +f\"  {'(m/s)'.rjust(15,' ')}\"\\\n",
    "                +f\"  {'(m/s)'.rjust(15,' ')}\"\\\n",
    "                +f\"  {'(m/s)'.rjust(15,' ')}\"\\\n",
    "\n",
    "        #### Manually write the header field names\n",
    "        header_names =\\\n",
    "                f\"{'Date'.rjust(19-1,' ') }\"\\\n",
    "                +f\"  {'X'.rjust(15,' ')}\"\\\n",
    "                +f\"  {'Y'.rjust(15,' ')}\"\\\n",
    "                +f\"  {'Z'.rjust(15,' ')}\"\\\n",
    "                +f\"  {'X_dot'.rjust(15,' ')}\"\\\n",
    "                +f\"  {'Y_dot'.rjust(15,' ')}\"\\\n",
    "                +f\"  {'Z_dot'.rjust(15,' ')}\"\\\n",
    "\n",
    "        #### Manually write the detailed header description\n",
    "        header_meta = \\\n",
    "        f'''### \"Raw\" Satellite Ephemeris\n",
    "### -----------------------\n",
    "###     Satellite: GRACE-FO-C_({prms['sat_ID']})\n",
    "###     Epoch: +start____ {ORBFIL_FINAL['Date_UTC'].values[0]} \n",
    "###            +stop_____ {ORBFIL_FINAL['Date_UTC'].values[-1]}\n",
    "###     Last modified: {datetime.now()-timedelta(hours=7)}\n",
    "###\n",
    "### Source\n",
    "### -------\n",
    "###     {raw_satinput['ephem_path_dir']}\n",
    "###     GRACE-FO-C PSO, Orbit Trajectory binary files\n",
    "###\n",
    "### Contents\n",
    "### --------\n",
    "###     Date:  (YYYY-MM-DD hh:mm:ss) (UTC, converted from mjdsec-gps time)\n",
    "###     Ephem:  Position and velocity (X, Y, Z, X_dot, Y_dot, Z_dot)\n",
    "###             coordinate: ECI-J2000\n",
    "###             unit: m\n",
    "###\n",
    "###\n",
    "#{header_units}\n",
    "#{header_names}\n",
    "### %eoh\n",
    "'''\n",
    "        print(np.shape(ORBFIL_FINAL['X j2000'].values))\n",
    "\n",
    "        \n",
    "        \n",
    "        file.write(header_meta)  \n",
    "        for indx,valdate in enumerate(ORBFIL_FINAL['Date_UTC'].values):\n",
    "        #### Manually write each row of the data.\n",
    "            row =   f\"{pd.to_datetime(ORBFIL_FINAL['Date_UTC'].values[indx]).strftime(format='%Y-%m-%d %H:%M:%S')}\"\\\n",
    "                +f\"  {ORBFIL_FINAL['X j2000'].values[indx]:15.5f}\"\\\n",
    "                +f\"  {ORBFIL_FINAL['Y j2000'].values[indx]:15.5f}\"\\\n",
    "                +f\"  {ORBFIL_FINAL['Z j2000'].values[indx]:15.5f}\"\\\n",
    "                +f\"  {ORBFIL_FINAL['X_dot j2000'].values[indx]:15.5f}\"\\\n",
    "                +f\"  {ORBFIL_FINAL['Y_dot j2000'].values[indx]:15.5f}\"\\\n",
    "                +f\"  {ORBFIL_FINAL['Z_dot j2000'].values[indx]:15.5f}\"\\\n",
    "                +f\"\\n\"\n",
    "            file.write(row)\n",
    "    #\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    #\n",
    "    print()\n",
    "    # print(f'       indxes: 0 -',indx,'') \n",
    "    print(f'       Processed file in : ',np.round(elapsed,5),'secs', f\"({np.round(elapsed,5)/60} minutes)\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f1879",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T20:15:13.944379Z",
     "start_time": "2024-06-26T20:15:13.944363Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc812c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T20:00:20.166938Z",
     "start_time": "2024-06-20T20:00:20.161666Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07bd2eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T19:27:08.969860Z",
     "start_time": "2024-06-24T19:27:08.969846Z"
    }
   },
   "source": [
    "# Read ExAtt Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3de6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c7d18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:13:24.146557Z",
     "start_time": "2025-01-14T20:13:24.146541Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_EXTAT_binary():\n",
    "    \"\"\"\n",
    "        OVERVIEW  \n",
    "            External Attitude File is a binary file where all records contain 9, 64-bit real words\n",
    "            +-----------------------------------------------------------\n",
    "            |Nomenclature\n",
    "            |    SBF   --> Spacecraft  Body  Fixed  Frame\n",
    "            |    J2000 --> J2000  Earth  Equator  and  Equinox  Frame\n",
    "            |    MVP   -->  Movable  Panel  Frame\n",
    "            |    MVA   -->  Movable  Antenna  Frame\n",
    "            |        (\"Movable\" is with  respect  to the SBF  frame)\n",
    "            |        (All  times in this  file  are TDT or TDB)\n",
    "            |              TDT (Terrestrial  Dynamic  Time)\n",
    "            |              TDB (Barycenter   Dynamic  Time)\n",
    "            |              GPS (Global  Positioning  System Time)\n",
    "            +------------------------------------------------------------\n",
    "\n",
    "        The file is segmented as follows:\n",
    "            1) GENERAL HEADER RECORD\n",
    "            2) SATELLITE INFORMATION HEADER RECORDS\n",
    "            3) QUATERNION  SET  HEADER  RECORD\n",
    "            4) DATA RECORDS\n",
    "\n",
    "\n",
    "\n",
    "                1)  GENERAL HEADER RECORD\n",
    "                    --------------------- \n",
    "                    There is only one of these records and it must be first in the file.\n",
    "                -----------------------------------------------------------------------\n",
    "                WORD         DESCRIPTION\n",
    "                ----------------------------------------------------------------------- \n",
    "                1          Record Indicator                              -6666666.00\n",
    "                2          Version Number\n",
    "                3          Number of Satellites represented in this file.\n",
    "                4          Not used at present time\n",
    "                5          Not used at present time\n",
    "                6          Not used at present time\n",
    "                7          Not used at present time\n",
    "                8          Not used at present time\n",
    "                9          Not used at present time\n",
    "\n",
    "\n",
    "                2)  SATELLITE INFORMATION HEADER RECORDS\n",
    "                    ------------------------------------ \n",
    "                    The number of these records equals the number of satellites from the \n",
    "                    General Header Record. All of these records must directly follow the\n",
    "                    above General Header Record.\n",
    "                -----------------------------------------------------------------------\n",
    "                WORD         DESCRIPTION\n",
    "                ----------------------------------------------------------------------- \n",
    "                1           Record Indicator                      -7777777.00\n",
    "                2           Not used at present time              \n",
    "                3           Satellite ID*                         \n",
    "                4           Interval                              SSSSS.SSSSS\n",
    "                5           Start time                            YYMMDDHHMMSS .00\n",
    "                6           Start (fractional seconds)            00.SS\n",
    "                7           Stop time                             YYMMDDHHMMSS .00\n",
    "                8           Stop (fractional seconds)             00.SS\n",
    "                9           No. of: panels+antenna separate       QQQPPPAAA **\n",
    "                                quaternion sets/movable panels\n",
    "                                represented/movable antenna \n",
    "                                represented        \n",
    "\n",
    "                3)  QUATERNION SET HEADER RECORD\n",
    "                    ----------------------------\n",
    "                    This header record must precede the quaternion data \n",
    "                    records for a particular set of quaternions.\n",
    "                -----------------------------------------------------------------------\n",
    "                WORD           DESCRIPTION\n",
    "                -----------------------------------------------------------------------\n",
    "                1           Record  Indicator                    -8888888.00\n",
    "                2           Satellite  ID\n",
    "                3           Panel  Number*                        MMMNNN\n",
    "                4           Antenna -Link  Number ***\n",
    "                5           Start  time**                         YYMMDDHHMMSS .00\n",
    "                6           Start (fractional  seconds )**        00.SS\n",
    "                7           Stop  time**                          YYMMDDHHMMSS .00\n",
    "                8           Stop (fractional  seconds )**         00.SS\n",
    "                9           Interval **                           SSSSS.SSSSS\n",
    "\n",
    "    \"\"\"\n",
    "    from scipy.io import FortranFile\n",
    "\n",
    "\n",
    "    \n",
    "    AttitudeFile = '/data/SatDragModelValidation/data/inputs/sat_gracefoc/external_attitude/EXAT01_AB_2018303115947_2018305115947'\n",
    "    f = FortranFile(AttitudeFile, 'r')\n",
    "\n",
    "    sp = '    '\n",
    "    header= {}\n",
    "\n",
    "    ### ----------------------------------------------------\n",
    "    ### ------------- 1) GENERAL HEADER RECORD -------------\n",
    "    ###\n",
    "    ###      Read the first record, this is the header buffer\n",
    "    ####     Use 64-bit float datatype. Each record contains 9, 64-bit words\n",
    "    a = f.read_record(float)  \n",
    "\n",
    "    if a[1 -1] == -6666666.00:\n",
    "        print('Reading GENERAL HEADER RECORD')\n",
    "\n",
    "        header['NSATS']           = a[3 -1]  # Number of Satellites represented in this file.\n",
    "        print(sp, 'NSATS =', header['NSATS'] )\n",
    "    for sats in np.arange(1, int(header['NSATS'])+1):\n",
    "        header['SATINFO_'+f\"{int(sats):02}\"] = {}\n",
    "    #\n",
    "    #\n",
    "    ### ----------------------------------------------------\n",
    "    ### ---------- 2) SATELLITE INFORMATION RECORD ---------\n",
    "    ###  \n",
    "    a = f.read_record(float)\n",
    "    if a[1 -1] == -7777777.00:\n",
    "        print('Reading SATELLITE INFORMATION HEADER RECORD')\n",
    "\n",
    "        for i in np.arange(1, int(header['NSATS'])+1):\n",
    "            iSat = 'SATINFO_'+f\"{int(i):02}\"\n",
    "\n",
    "            header[iSat]['SATID']                             = a[3 -1]  \n",
    "\n",
    "            header[iSat]['Interval (SSSSS.SSSSS)']            = a[4 -1]  \n",
    "            header[iSat]['Start time (YYMMDDHHMMSS.00)']      = a[5 -1]  \n",
    "            header[iSat]['Start (fractional seconds, 00.SS)'] = a[6 -1] \n",
    "            header[iSat]['Stop time (YYMMDDHHMMSS.00)']       = a[7 -1] \n",
    "            header[iSat]['Stop (fractional seconds, 00.SS)']  = a[8 -1]  \n",
    "            header[iSat]['QQQPPPAAA']                         = a[9 -1]  \n",
    "\n",
    "            datestart_string = str(header[iSat]['Start time (YYMMDDHHMMSS.00)']).split('.')[0]\n",
    "            datestop_string  = str(header[iSat]['Stop time (YYMMDDHHMMSS.00)']).split('.')[0]\n",
    "            date_start       = pd.to_datetime(datestart_string, format='%y%m%d%H%M%S')\n",
    "            date_stop        = pd.to_datetime(datestop_string, format='%y%m%d%H%M%S')\n",
    "\n",
    "            print(sp,'Start:',date_start)\n",
    "            print(sp,'Stop:' ,date_stop)\n",
    "            print(sp,'Interval:',header[iSat]['Interval (SSSSS.SSSSS)'])\n",
    "\n",
    "            print(sp,'QQQPPPAAA', a[9 -1])\n",
    "                # QQQ is the total # of separate movable panels + antenna quaternion  \n",
    "                #     sets for this satellite (excludes SBF to J2000 quaternion set  \n",
    "                #     which is mandatory for each satellite represented in the file).   \n",
    "                # PPP is the number of movable panels represented for this satellite. \n",
    "                # AAA is the number of moveable antenna represented for this  satellite.\n",
    "                #     One quaternion set may represent the attitude for up to two  \n",
    "                #     movable panels and one movable antenna.                \n",
    "            print('SatID',header[iSat]['SATID'])\n",
    "\n",
    "\n",
    "    #\n",
    "    #\n",
    "    ### -------------------------------------------------------\n",
    "    ### ---------- 3) QUATERNION SET HEADER RECORD  --------\n",
    "    ###  \n",
    "    ###       This header record must precede the \n",
    "    ###            quaternion data records for a \n",
    "    ###            particular set of quaternions.\n",
    "\n",
    "\n",
    "    def read_QUAT_SET_HEADER(set_count, header, a):\n",
    "        if a[1 -1] ==  -8888888.00:\n",
    "    #         print(a)\n",
    "            set_count+=1\n",
    "            iQuat = \"QuatSet\"+str(set_count)\n",
    "            print('Reading QUATERNION SET HEADER RECORD', str(set_count))\n",
    "\n",
    "            header[iQuat] = {}\n",
    "\n",
    "            header[iQuat]['SATID']                            = a[2 -1] \n",
    "            print('SatID',header[iQuat]['SATID'])\n",
    "            header[iQuat]['Panel # (MMMNNN)']                 = a[3 -1] \n",
    "                #    MMM is 1st panel  number  and \n",
    "                #    NNN is 2nd panel  number\n",
    "            header[iQuat]['AntennaLink #']                    = a[4 -1]  \n",
    "                #Antenna -Link  number  is used to  specify  both  the  antenna  and  linknumber(see  table  below ).\n",
    "            ####\n",
    "            ###  Qaternion  satellite  information  must  match  \n",
    "            ###  that on thes satellite  information  header  record.\n",
    "            header[iQuat]['Start time (YYMMDDHHMMSS.00)']      = a[5 -1]  \n",
    "            header[iQuat]['Start (fractional seconds, 00.SS)'] = a[6 -1] \n",
    "            header[iQuat]['Stop time (YYMMDDHHMMSS.00)']       = a[7 -1] \n",
    "            header[iQuat]['Stop (fractional seconds, 00.SS)']  = a[8 -1]  \n",
    "            header[iQuat]['Interval (SSSSS.SSSSS)']            = a[9 -1]  \n",
    "\n",
    "        else:\n",
    "            print(a)\n",
    "            f.close()  #### be sure to close the file\n",
    "            sys.exit(0)\n",
    "        return(set_count, header)\n",
    "\n",
    "\n",
    "    #### Read the first header record\n",
    "    a = f.read_record(float)\n",
    "    set_count = 0\n",
    "    (set_count, header) = read_QUAT_SET_HEADER(set_count, header, a)\n",
    "\n",
    "\n",
    "\n",
    "    def read_QUAT_DATA(a, data, set_count, header):\n",
    "        print(\"Reading Data for set:\", set_count)\n",
    "        data[set_count] = {}\n",
    "        data[set_count]['q1'] = []\n",
    "        data[set_count]['q2'] = []\n",
    "        data[set_count]['q3'] = []\n",
    "        data[set_count]['q4'] = []\n",
    "\n",
    "        while a[1 -1] ==  0:\n",
    "            data[set_count]['q1'].append(a[3 -1])  ###  sin (/2)n1\n",
    "            data[set_count]['q2'].append(a[4 -1])  ###  sin (/2)n2\n",
    "            data[set_count]['q3'].append(a[5 -1])  ###  sin (/2)n3\n",
    "            data[set_count]['q4'].append(a[6 -1])  ###  cos (/2)\n",
    "\n",
    "            ### Move to next data record while in the loop\n",
    "            a = f.read_record(float)\n",
    "        else:\n",
    "            if a[1 -1] !=  0:\n",
    "    #             print('Back to a header?')\n",
    "                (set_count, header) = read_QUAT_SET_HEADER(set_count, header, a)\n",
    "                try:\n",
    "                    a = f.read_record(float)\n",
    "                    (a, data, set_count) = read_QUAT_DATA(a, data, set_count, header)\n",
    "\n",
    "                except:\n",
    "                    print('End of File')\n",
    "                    f.close()  #### be sure to close the file\n",
    "\n",
    "        return(a, data, set_count)\n",
    "\n",
    "\n",
    "\n",
    "    ### Read the first Data record and then open the loop until the end of file\n",
    "    a = f.read_record(float)\n",
    "    data = {}\n",
    "    (a, data, set_count) = read_QUAT_DATA(a, data, set_count, header)\n",
    "    f.close()  #### be sure to close the file\n",
    "\n",
    "    ## attempt to clear out some memory\n",
    "    f         = 0\n",
    "    a         = 0\n",
    "    set_count = 0\n",
    "\n",
    "    return()\n",
    "\n",
    "\n",
    "\n",
    "read_EXTAT_binary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f97430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "870.667px",
    "left": "29px",
    "top": "144.806px",
    "width": "167.222px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "212.847px",
    "left": "952.667px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
