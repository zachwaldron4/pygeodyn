{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aad1109",
   "metadata": {},
   "source": [
    "# Normalize to 500 KM using MSIS\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2be918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:11.984912Z",
     "start_time": "2024-07-23T17:43:10.711068Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "from pygeodyn.pygeodyn_plot_scalingfactors import *\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf1162",
   "metadata": {},
   "source": [
    "### Load GFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1971e45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:11.992296Z",
     "start_time": "2024-07-23T17:43:11.987535Z"
    }
   },
   "outputs": [],
   "source": [
    "# month_list = ['oct', 'nov', 'dec']#, 'jan', 'feb', 'mar', 'apr']\n",
    "month_list = ['nov']\n",
    "\n",
    "ice_file = 'icesat2_6month.csv'\n",
    "gfo_file = 'gfo_6month.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9025982",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.010690Z",
     "start_time": "2024-07-23T17:43:11.995246Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:46: SyntaxWarning:\n",
      "\n",
      "invalid escape sequence '\\s'\n",
      "\n",
      "<>:46: SyntaxWarning:\n",
      "\n",
      "invalid escape sequence '\\s'\n",
      "\n",
      "/tmp/ipykernel_29183/3868659362.py:46: SyntaxWarning:\n",
      "\n",
      "invalid escape sequence '\\s'\n",
      "\n",
      "/tmp/ipykernel_29183/3868659362.py:46: SyntaxWarning:\n",
      "\n",
      "invalid escape sequence '\\s'\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/SatDragModelValidation/data/inputs/raw_inputdata/data_GRACEFO/GC_DNS_ACC_2018_11_v02.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m\n\u001b[1;32m     26\u001b[0m datapath_gfo     \u001b[38;5;241m=\u001b[39m path_gfo \u001b[38;5;241m+\u001b[39m filename_gfo\n\u001b[1;32m     28\u001b[0m headers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m,        \u001b[38;5;66;03m#         Date (yyyy-mm-dd)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m,        \u001b[38;5;66;03m#         Time (hh:mm:ss.sss)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflag_orbitavg\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;66;03m#   f4.1  Flag for running orbit average density: 0 = nominal data, 1 = anomalous data (-)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m             ]\n\u001b[0;32m---> 44\u001b[0m gfo_bigdf[month] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(datapath_gfo, \n\u001b[1;32m     45\u001b[0m         skiprows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m38\u001b[39m, \n\u001b[1;32m     46\u001b[0m         sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     47\u001b[0m         names \u001b[38;5;241m=\u001b[39m headers,\n\u001b[1;32m     48\u001b[0m                    )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#Convert date from GPS to UTC\u001b[39;00m\n\u001b[1;32m     51\u001b[0m date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\\\n\u001b[1;32m     52\u001b[0m                     \u001b[38;5;241m+\u001b[39m gfo_bigdf[month][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]  \\\n\u001b[1;32m     53\u001b[0m                     \u001b[38;5;241m+\u001b[39m gfo_bigdf[month][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m], \\\n\u001b[1;32m     54\u001b[0m                             \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.000\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_timedelta(\u001b[38;5;241m18\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/data/miniconda3/envs/pygeodyn2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/data/miniconda3/envs/pygeodyn2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/data/miniconda3/envs/pygeodyn2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/data/miniconda3/envs/pygeodyn2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/data/miniconda3/envs/pygeodyn2/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/SatDragModelValidation/data/inputs/raw_inputdata/data_GRACEFO/GC_DNS_ACC_2018_11_v02.txt'"
     ]
    }
   ],
   "source": [
    "if os.path.exists(gfo_file) :\n",
    "    print(gfo_file, ' exists' )\n",
    "\n",
    "else:\n",
    "    gfo_bigdf = {}\n",
    "\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        if month=='oct':\n",
    "            m_num = 10\n",
    "        if month=='nov':\n",
    "            m_num = 11\n",
    "        if month=='dec':\n",
    "            m_num = 12\n",
    "        if month=='jan':\n",
    "            m_num = 1\n",
    "        if month=='feb':\n",
    "            m_num = 2\n",
    "        if month=='mar':\n",
    "            m_num = 3\n",
    "        if month=='apr':\n",
    "            m_num = 4\n",
    "\n",
    "        path_gfo     = \"/data/SatDragModelValidation/data/inputs/raw_inputdata/data_GRACEFO/\"\n",
    "        filename_gfo = f'GC_DNS_ACC_2018_{m_num}_v02.txt'\n",
    "\n",
    "        datapath_gfo     = path_gfo + filename_gfo\n",
    "\n",
    "        headers = [\n",
    "            'date',        #         Date (yyyy-mm-dd)\n",
    "            'time',        #         Time (hh:mm:ss.sss)\n",
    "            'time_system', #         Time system: UTC or GPS (differs per mission)\n",
    "            'alt',         #  f10.3  Altitude (m), GRS80\n",
    "            'lon',         #   f8.3  Geodetic longitude (deg), GRS80\n",
    "            'lat',         #   f7.3  Geodetic latitude (deg), GRS80\n",
    "            'lst',         #   f6.3  Local solar time (h)\n",
    "            'arglat',      #   f7.3  Argument of latitude (deg)\n",
    "            'dens_x',      #  e15.8  Density derived from accelerometer measurements (kg/m3)\n",
    "            'dens_mean',   #  e15.8  Running orbit average of density (kg/m3)\n",
    "            'flag_den',    #   f4.1  Flag for density: 0 = nominal data, 1 = anomalous data (-)\n",
    "            'flag_orbitavg',#   f4.1  Flag for running orbit average density: 0 = nominal data, 1 = anomalous data (-)\n",
    "                    ]\n",
    "\n",
    "\n",
    "        gfo_bigdf[month] = pd.read_csv(datapath_gfo, \n",
    "                skiprows = 38, \n",
    "                sep = '\\s+',\n",
    "                names = headers,\n",
    "                           )\n",
    "\n",
    "        #Convert date from GPS to UTC\n",
    "        date = pd.to_datetime(\\\n",
    "                            + gfo_bigdf[month]['date']  \\\n",
    "                            + gfo_bigdf[month]['time'], \\\n",
    "                                    format='%Y-%m-%d%H:%M:%S.000') - pd.to_timedelta(18,'s')\n",
    "\n",
    "        gfo_bigdf[month].insert(0, 'Date', date)\n",
    "\n",
    "        del gfo_bigdf[month]['date'], gfo_bigdf[month]['time'], date\n",
    "        del gfo_bigdf[month]['time_system']\n",
    "        del gfo_bigdf[month]['dens_mean']\n",
    "        del gfo_bigdf[month]['flag_den']\n",
    "        del gfo_bigdf[month]['flag_orbitavg']\n",
    "\n",
    "\n",
    "    #     resid_meas_summry = pd.concat([ gfo_bigdf, resid_meas_summry_iter])\n",
    "\n",
    "    gfo_df = pd.concat([ gfo_bigdf[month] for month in month_list]  )\n",
    "    gfo_df = gfo_df.reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "##### NORMALIZATION OF DATA\n",
    "\n",
    "if os.path.exists(gfo_file) :\n",
    "    print(gfo_file, ' exists' )\n",
    "    gfo_df = pd.read_csv(gfo_file, \n",
    "                    sep = ',',\n",
    "                    )\n",
    "else:\n",
    "    print(f\"---Calculating Grace-FO normalization\")\n",
    "    D500_gfo = normalize_density_msis2( gfo_df , 'GRACE-FO', 500)\n",
    "    gfo_df['D500_gfo'] = D500_gfo\n",
    "    #### save to a csv\n",
    "    gfo_df.to_csv(gfo_file, index=False)  \n",
    "\n",
    "    \n",
    "\n",
    "# gfo_df2 =     gfo_df.query(\"Date >= '2018-10-14' and Date < '2018-12-30'\")\n",
    "# del gfo_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad5a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f40d20b1",
   "metadata": {},
   "source": [
    "### Load Icesat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5fc12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.019168Z",
     "start_time": "2024-07-23T17:43:13.019146Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(ice_file):\n",
    "    print(ice_file, ' exists' )\n",
    "    \n",
    "else:\n",
    "\n",
    "\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    from gc import collect as gc_collect\n",
    "    import pickle \n",
    "    from datetime import datetime,timedelta\n",
    "\n",
    "    run_list = ['msis2',\n",
    "                'jb2008',\n",
    "                'dtm2020_o']\n",
    "\n",
    "#     month_list = ['nov']\n",
    "\n",
    "\n",
    "    scale_cadence = 3\n",
    "\n",
    "    dir_modeldat='/data/SatDragModelValidation/data/inputs/atmos_models'\n",
    "    run_dict={}\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        for i in run_list:\n",
    "            if i =='msis2':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 5\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='dtm2020_o':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 3\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "            if i =='jb2008':\n",
    "                run_dict[month+i]={}\n",
    "                run_dict[month+i]['num'] = 1\n",
    "                run_dict[month+i]['model_path'] = None\n",
    "\n",
    "    print(run_dict)\n",
    "\n",
    "\n",
    "\n",
    "    dir_save    =  '/data/SatDragModelValidation/data/outputs_clean/'\\\n",
    "                 + 'icesat2/O2R2023_longimeperiod/1_DRIAruns/'\n",
    "    obj = {}\n",
    "    for imonth,month in enumerate(month_list):\n",
    "        for i,model in enumerate(run_list):\n",
    "            pickleName = f'_{month}_DRIA_scale{scale_cadence}.pkl'\n",
    "\n",
    "            ### Load the data if the pickles exist\n",
    "            print()\n",
    "            print()\n",
    "            gc_collect()\n",
    "\n",
    "            pickle_file = dir_save+model+pickleName\n",
    "\n",
    "            filehandler = open(pickle_file, 'rb') \n",
    "            obj[month+model] = pickle.load(filehandler)\n",
    "            filehandler.close()\n",
    "            print('Loaded data from pickle... ',  month+model)\n",
    "\n",
    "\n",
    "    ### Save space if doing density retrieval\n",
    "    for model in run_dict.keys():\n",
    "        del obj[model]['OrbitResids']\n",
    "        del obj[model]['Trajectory_orbfil']\n",
    "\n",
    "    gc_collect()\n",
    "    \n",
    "    \n",
    "    ## MAKE SCALING FACTORS\n",
    "    \n",
    "    satid = 1807001\n",
    "    wgts = {}\n",
    "\n",
    "    for model in run_dict.keys():\n",
    "        wgts[model] = {}\n",
    "        ScalingFactors  = []\n",
    "        ScalingFactor_times = []\n",
    "\n",
    "        for ii,arc in enumerate(obj[model]['global_params']['arc_input']):\n",
    "            epochstart = obj[model]['global_params']['prms']['epoch_start'][ii]\n",
    "            hrs = pd.to_datetime(epochstart, format='%Y-%m-%d %H:%M:%S').hour\n",
    "            frachours =(hrs/24)\n",
    "            #\n",
    "            if len(arc) == 9:\n",
    "                maneuv_indicator = arc[8]\n",
    "            else:\n",
    "                maneuv_indicator = ''\n",
    "            arc_type = obj[model]['global_params']['prms']['arc_type']\n",
    "            if arc_type == \"Nominal30hr_and_AB\":\n",
    "                arc_name =arc[:8]+ maneuv_indicator\n",
    "            else:\n",
    "                arc_name =arc[:8]+('%.3f'%frachours).lstrip('0')+ maneuv_indicator\n",
    "            ### Collect the weights for the ensemble average\n",
    "            inv_rms          = 1/obj[model]['Statistics'][arc_name]['T_RMS'].values[0]\n",
    "            wgts[model][arc_name] = inv_rms#/sum_wgts\n",
    "\n",
    "            iters = int(obj[model]['run_parameters'+arc_name]['total_iterations']) \n",
    "            for iit, itime in enumerate(obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'].keys()):\n",
    "                if iit == 0 or iit==9:\n",
    "                    pass\n",
    "                else:\n",
    "                    CURRENT_VALUE = obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'][itime]['CURRENT_VALUE']\n",
    "                    APRIORI_VALUE = obj[model]['AdjustedParams'][arc_name][iters][satid]['0CD'][itime]['APRIORI_VALUE']\n",
    "                    ScalingFactors.append(CURRENT_VALUE/APRIORI_VALUE)\n",
    "                    ScalingFactor_times.append(itime)\n",
    "        run_dict[model]['ScalingFactor_times'] = ScalingFactor_times\n",
    "        run_dict[model]['ScalingFactors']      = ScalingFactors\n",
    "        run_dict[model]['Weight'] = wgts[model]\n",
    "\n",
    "\n",
    "\n",
    "    ###### Scale the densities\n",
    "    models_dens = {}\n",
    "    for monthmodel in run_dict.keys():\n",
    "        print(f\"---Making continuous scaled rho for {monthmodel}\")\n",
    "        models_dens[monthmodel] = get_continuous_scaled_densities(obj, run_dict, monthmodel, scale_cadence)\n",
    "\n",
    "\n",
    "    ## Retrieve scaled ensemble weighted average\n",
    "    ##     'Rho_x' denotes the ensemble weighted avg\n",
    "    print(f\"---Making ensemble avg\")\n",
    "#     for monthmodel in run_dict.keys():\n",
    "#         month = monthmodel[:3]\n",
    "    models_dens =  calc_rho_ScaledEnsembleWgtAvg(models_dens, run_dict, month_list, run_list)\n",
    "\n",
    "\n",
    "    ### Clear up space \n",
    "    del obj\n",
    "    for monthmodel in run_dict.keys():\n",
    "        print(monthmodel)\n",
    "        if 'Rho_x' not in monthmodel:\n",
    "            del models_dens[monthmodel]\n",
    "\n",
    "    gc_collect()       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c14a50",
   "metadata": {},
   "source": [
    "## Process normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f9069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.020135Z",
     "start_time": "2024-07-23T17:43:13.020120Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pygeodyn.pygeodyn_plot_scalingfactors import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49023d75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.021088Z",
     "start_time": "2024-07-23T17:43:13.021073Z"
    }
   },
   "outputs": [],
   "source": [
    "# models_dens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a288f31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.022029Z",
     "start_time": "2024-07-23T17:43:13.022015Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(ice_file) :\n",
    "    print(ice_file, ' exists' )\n",
    "    \n",
    "    ice_df = pd.read_csv(ice_file, \n",
    "                    sep = ',',\n",
    "                    )\n",
    "    calc_lst = np.mod(ice_df['lon']/15 + pd.to_datetime(ice_df['date']).dt.hour, 24)\n",
    "    ice_df['lst'] = calc_lst\n",
    "    ice_df['Date'] = ice_df['date']\n",
    "else:\n",
    "    \n",
    "    ice_bigdf = {}\n",
    "    print(f\"---Calculating ICESat-2 normalization\")\n",
    "    \n",
    "    for imonth,month in enumerate(month_list):\n",
    "\n",
    "        D500_ice = normalize_density_msis2( models_dens[month+'Rho_x'], 'ICESat-2', 500)\n",
    "        models_dens[month+'Rho_x']['D500_ice'] = D500_ice\n",
    "        \n",
    "        ice_bigdf[month] = pd.DataFrame.from_dict(models_dens[month+'Rho_x'])\n",
    "\n",
    "    \n",
    "    ice_df = pd.concat([ ice_bigdf[month] for month in month_list]  )\n",
    "    ice_df = ice_df.reset_index(drop=True)\n",
    "    ice_df.to_csv(ice_file, index=False)  \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71874a7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.022939Z",
     "start_time": "2024-07-23T17:43:13.022925Z"
    }
   },
   "outputs": [],
   "source": [
    "gfo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef6263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.024007Z",
     "start_time": "2024-07-23T17:43:13.023993Z"
    }
   },
   "outputs": [],
   "source": [
    "ice_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88c44f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.025383Z",
     "start_time": "2024-07-23T17:43:13.025369Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# days = np.arange(0,30)\n",
    "# dates = pd.date_range(start='11/1/2018', end='12/30/2018')\n",
    "dates = pd.date_range(start='11/1/2018', end='4/30/2019')\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "for SAT in ['gfo', 'ice']:\n",
    "    \n",
    "    if SAT== 'gfo':\n",
    "        df = gfo_df\n",
    "        sat_symbol = 'cross'\n",
    "        \n",
    "    if SAT== 'ice':\n",
    "        df = ice_df\n",
    "        sat_symbol = 'circle'\n",
    "        \n",
    "    \n",
    "\n",
    "    fns = dates\n",
    "\n",
    "    # Allocate variables\n",
    "    ascdescLT = np.zeros((2, len(fns)))\n",
    "    year = np.zeros(len(fns))\n",
    "    doy = np.zeros(len(fns))\n",
    "    avgheight = np.zeros(len(fns))\n",
    "\n",
    "\n",
    "    # Loop through daily data files\n",
    "    for ifn, idate in enumerate(dates):\n",
    "\n",
    "        nday = idate+pd.Timedelta(1,unit='D')\n",
    "        # Load data for current day\n",
    "        d = df.query(f\"Date >= '{idate.year}-{idate.month:02d}-{idate.day:02d}' and Date < '{nday.year}-{nday.month:02d}-{nday.day:02d}'\")\n",
    "\n",
    "        if d.empty:\n",
    "            pass\n",
    "        else:\n",
    "\n",
    "            d_dict = {}\n",
    "            d_dict['lat'] = d['lat'].values\n",
    "            d_dict['lst'] = d['lst'].values\n",
    "            d_dict['alt'] = d['alt'].values\n",
    "            #\n",
    "            d_dict['year'] = pd.to_datetime(d['Date'].values).year.values\n",
    "            d_dict['doy']  = pd.to_datetime(d['Date'].values).day_of_year.values\n",
    "\n",
    "            # Find ascending equatorial crossings\n",
    "            i = np.where(np.diff(d_dict['lat']) > 0)[0]\n",
    "            j = np.where(np.abs(d_dict['lat'][i]) < 20)[0]\n",
    "            meanLT1 = np.mean(d_dict['lst'][i[j]])\n",
    "            stdLT1 = np.std(d_dict['lst'][i[j]])\n",
    "\n",
    "            # Find descending equatorial crossings\n",
    "            i = np.where(np.diff(d_dict['lat']) < 0)[0]\n",
    "            j = np.where(np.abs(d_dict['lat'][i]) < 20)[0]\n",
    "            meanLT2 = np.mean(d_dict['lst'][i[j]])\n",
    "            stdLT2 = np.std(d_dict['lst'][i[j]])\n",
    "\n",
    "            # Find lowest of the two LT standard deviations, ensuring that the\n",
    "            #  orbit portion away from 0/24LT crossover is chosen\n",
    "            b = np.argmin([stdLT1, stdLT2])\n",
    "            ascdescLT[b, ifn] = meanLT1 if b == 0 else meanLT2\n",
    "            c = 1 if b == 0 else 0                   # index of the highest of the two standard deviations\n",
    "            ascdescLT[c, ifn] = (meanLT1 + 12) % 24  # offset the more reliable LT by 12 hours\n",
    "\n",
    "            #   % Find lowest of the two LT standard deviations, ensuring that the orbit portion away from 0/24LT crossover is chosen\n",
    "            # \t[a,b] = min(stdLT);\n",
    "            # \tascdescLT(b,ifn) = meanLT(b);\n",
    "            # \tc = setxor(b,[1,2]); % index of the highest of the two standard deviations\n",
    "            # \tascdescLT(c,ifn) = mod(meanLT(b)+12,24); % offset the more reliable LT by 12 hours\n",
    "\n",
    "            # Find average height\n",
    "            i = np.where((d_dict['alt'] < 1e6) & (d_dict['alt'] > 1e5))[0]  # between 100-1000 km\n",
    "            avgheight[ifn] = 1e-3 * np.mean(d_dict['alt'][i])  # km\n",
    "\n",
    "            # Store current time\n",
    "            year[ifn] = d_dict['year'][0]\n",
    "            doy[ifn] = d_dict['doy'][0]\n",
    "\n",
    "\n",
    "\n",
    "    ascdescLT[0][ascdescLT[0] == 0.] = np.nan # or use np.nan\n",
    "    ascdescLT[1][ascdescLT[1] == 0.] = np.nan # or use np.nan\n",
    "    year[year == 0.] = np.nan # or use np.nan\n",
    "    doy[doy == 0.] = np.nan # or use np.nan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fig.add_trace(go.Scattergl(x=year + doy / (365 + (year % 4 == 0)),\n",
    "                     y=ascdescLT[0],\n",
    "                     name=SAT+' ascending',\n",
    "                     mode='markers',\n",
    "                     opacity=1,\n",
    "                     marker=dict(size=8,color=\"#1f77b4\",symbol=sat_symbol),\n",
    "                     showlegend=True),\n",
    "        secondary_y=False,row=1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scattergl(x=year + doy / (365 + (year % 4 == 0)),\n",
    "                     y=ascdescLT[1],\n",
    "                     name=SAT+' descending',\n",
    "                     mode='markers',\n",
    "                     opacity=1,\n",
    "                     marker=dict(size=8, color=\"#ff7f0e\",symbol=sat_symbol),\n",
    "                     showlegend=True),\n",
    "        secondary_y=False,row=1, col=1)\n",
    "\n",
    "\n",
    "fig.update_xaxes(title=\"Year\", row=1, col=1)\n",
    "fig.update_yaxes(title=\"Local Time (hours)\",range=[0,24], row=1, col=1,\n",
    "        tick0 = 0,\n",
    "        dtick = 4,\n",
    "                )\n",
    "\n",
    "fig.update_layout(autosize=True,legend= {'itemsizing': 'constant'})\n",
    "fig.show(config=config)\n",
    "\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(year + doy / (365 + (year % 4 == 0)), ascdescLT[0], '.')\n",
    "# plt.plot(year + doy / (365 + (year % 4 == 0)), ascdescLT[1], '.')\n",
    "# # plt.plot(days                                  , ascdescLT, '.')\n",
    "# # plt.hold(True)\n",
    "# plt.axis('tight')\n",
    "# for i in np.arange((2018 + 80 / 365.25 + 0.5), 2023, 0.5):\n",
    "#     plt.plot([i, i], [0, 24], color=0.7 * np.ones(3))\n",
    "# plt.plot(plt.xlim(), [6, 6], color=0.7 * np.ones(3))\n",
    "# plt.plot(plt.xlim(), [18, 18], color=0.7 * np.ones(3))\n",
    "# co = plt.cm.get_cmap('tab10')\n",
    "# h = []\n",
    "# h.append(plt.Polygon(np.array([[np.nan, np.nan], [np.nan, np.nan]]), facecolor=co(0)))\n",
    "# h.append(plt.Polygon(np.array([[np.nan, np.nan], [np.nan, np.nan]]), facecolor=co(1)))\n",
    "# plt.legend(h, ['Ascending', 'Descending'])\n",
    "# # plt.gca().set_layer('top')\n",
    "# plt.ylim([0, 24])\n",
    "# plt.yticks(np.arange(0, 25, 6))\n",
    "# plt.ylabel('Local Time (hours)')\n",
    "# plt.xlabel('Year')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4340ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.026267Z",
     "start_time": "2024-07-23T17:43:13.026253Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot average height\n",
    "plt.figure()\n",
    "plt.plot(year + doy / (365 + (year % 4 == 0)), avgheight, '.')\n",
    "plt.axis('tight')\n",
    "# plt.gca().set_layer('top')\n",
    "plt.ylabel('Average Height (km)')\n",
    "plt.xlabel('Year')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177373fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.027143Z",
     "start_time": "2024-07-23T17:43:13.027130Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(year + doy / (365 + (year % 4 == 0)), ascdescLT[0], '.')\n",
    "# plt.plot(year + doy / (365 + (year % 4 == 0)), ascdescLT[1], '.')\n",
    "# plt.axis('tight')\n",
    "# for i in np.arange((2018 + 80 / 365.25 + 0.5), 2023, 0.5):\n",
    "#     plt.plot([i, i], [0, 24], color=0.7 * np.ones(3))\n",
    "# plt.plot(plt.xlim(), [6, 6], color=0.7 * np.ones(3))\n",
    "# plt.plot(plt.xlim(), [18, 18], color=0.7 * np.ones(3))\n",
    "# co = plt.cm.get_cmap('tab10')\n",
    "# h = []\n",
    "# h.append(plt.Polygon(np.array([[np.nan, np.nan], [np.nan, np.nan]]), facecolor=co(0)))\n",
    "# h.append(plt.Polygon(np.array([[np.nan, np.nan], [np.nan, np.nan]]), facecolor=co(1)))\n",
    "# plt.legend(h, ['Ascending', 'Descending'])\n",
    "# plt.ylim([0, 24])\n",
    "# plt.yticks(np.arange(0, 25, 6))\n",
    "# plt.ylabel('Local Time (hours)')\n",
    "# plt.xlabel('Year')\n",
    "\n",
    "\n",
    "\n",
    "# fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "\n",
    "# fig.add_trace(go.Scattergl(x=year + doy / (365 + (year % 4 == 0)),\n",
    "#                  y=ascdescLT[0],\n",
    "#                  name='ascending',\n",
    "#                  mode='markers',\n",
    "#                  opacity=1,\n",
    "#                  marker=dict(size=4),\n",
    "#                  showlegend=True),\n",
    "#     secondary_y=False,row=1, col=1)\n",
    "\n",
    "# fig.add_trace(go.Scattergl(x=year + doy / (365 + (year % 4 == 0)),\n",
    "#                  y=ascdescLT[1],\n",
    "#                  name='descending',\n",
    "#                  mode='markers',\n",
    "#                  opacity=1,\n",
    "#                  marker=dict(size=4),\n",
    "#                  showlegend=True),\n",
    "#     secondary_y=False,row=1, col=1)\n",
    "\n",
    "\n",
    "# fig.update_xaxes(title=\"Year\", row=1, col=1)\n",
    "# fig.update_yaxes(title=\"Local Time (hours)\",range=[0,24], row=1, col=1,\n",
    "#         tick0 = 0,\n",
    "#         dtick = 4,\n",
    "#                 )\n",
    "\n",
    "# fig.update_layout(autosize=True,legend= {'itemsizing': 'constant'})\n",
    "# fig.show(config=config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990edb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.028044Z",
     "start_time": "2024-07-23T17:43:13.028030Z"
    }
   },
   "outputs": [],
   "source": [
    "# year + doy / (365 + (year % 4 == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f772d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.028950Z",
     "start_time": "2024-07-23T17:43:13.028936Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(idate)\n",
    "# print(nday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe0881e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.030027Z",
     "start_time": "2024-07-23T17:43:13.030012Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213102b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.030952Z",
     "start_time": "2024-07-23T17:43:13.030938Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "\n",
    "# # fig.add_trace(\n",
    "# #     go.Scattergl(x=ice_df2['lst'][::10],\n",
    "# #                  y=ice_df2['lat'][::10],\n",
    "# #                  name='ICESat2',\n",
    "# #                  mode='markers',\n",
    "# #                  opacity=1,\n",
    "# #                  marker=dict(\n",
    "# #                     size=3,\n",
    "# #                     cmax=gfo_df2['D500_gfo'].max(),\n",
    "# #                     cmin=ice_df2['D500_ice'].min(),\n",
    "# #                     color=ice_df2['D500_ice'],\n",
    "# #                     colorbar=dict(title=\"Density\"),colorscale=\"Viridis\"),\n",
    "# #                  showlegend=False),\n",
    "# #     secondary_y=False,\n",
    "# #     row=1, col=1)\n",
    "\n",
    "\n",
    "# fig.add_trace(\n",
    "#     go.Scattergl(x=gfo_df2['lst'],\n",
    "#                  y=gfo_df2['lat'],\n",
    "#                  name='GRACE-FO',\n",
    "#                  mode='markers',\n",
    "#                  opacity=0.5,\n",
    "# #                  marker=dict( size=3 ),\n",
    "#                  marker=dict(\n",
    "#                     size=3,\n",
    "#                     cmax=gfo_df2['D500_gfo'].max(),\n",
    "#                     cmin=ice_df2['D500_ice'].min(),\n",
    "#                     color=gfo_df2['D500_gfo'],\n",
    "#                     colorbar=dict(title=\"Density\"),colorscale=\"Viridis\"),\n",
    "#                  showlegend=False),\n",
    "#     secondary_y=False,\n",
    "#     row=1, col=1)\n",
    "\n",
    "\n",
    "\n",
    "# fig.update_xaxes(title=\"Local solar time (h)\", row=1, col=1)\n",
    "# fig.update_yaxes(title=\"Geodetic Latitude (deg), GRS80\", row=1, col=1)\n",
    "# fig.update_layout(autosize=True,\n",
    "#                   legend= {'itemsizing': 'constant'},\n",
    "#                  )\n",
    "\n",
    "# fig.show(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe75bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T20:04:46.938036Z",
     "start_time": "2024-02-19T20:02:30.071Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbe667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.031898Z",
     "start_time": "2024-07-23T17:43:13.031885Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "\n",
    "(time_avg, den_gfo ) = orbit_avg_generic(gfo_df['Date'], gfo_df['D500_gfo'], gfo_df['lat'])    \n",
    "fig.add_trace(go.Scattergl(x=time_avg,\n",
    "                            y=den_gfo,\n",
    "                            name=f'GFO_500km',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4, color='black' ),\n",
    "                            showlegend=False),\n",
    "                            row=1, col=1)\n",
    "\n",
    "(time_avg, den_avg ) = orbit_avg_generic(ice_df['date'],ice_df['D500_ice'],ice_df['lat'])    \n",
    "fig.add_trace(go.Scattergl(x=time_avg,\n",
    "                            y=den_avg,\n",
    "                            name=f'D500_icesat2',\n",
    "                            mode='markers+lines',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4, color='blue'),\n",
    "                            showlegend=False),\n",
    "                            row=1, col=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### SYLIZE LEGEND \n",
    "font_dict=dict(family='Arial',size=11,color='black')\n",
    "## automate the specification of the axes for subplots\n",
    "rownum, colnum = fig._get_subplot_rows_columns()\n",
    "for i in rownum:\n",
    "    if len(rownum)==1:\n",
    "        L_ticklabel = True\n",
    "    else:\n",
    "        if i < len(rownum):\n",
    "            L_ticklabel = False\n",
    "        else:\n",
    "            L_ticklabel = True\n",
    "    fig.update_xaxes(### LINE at axis border\n",
    "                      showline=True,\n",
    "                      showticklabels=L_ticklabel,\n",
    "#                       tickformat= '%m/%d',\n",
    "                      linecolor='black',\n",
    "                      linewidth=1,\n",
    "                     ### Major ticks\n",
    "                      ticks='inside',\n",
    "                      tickfont=font_dict,\n",
    "                      mirror=True,\n",
    "#                       tickwidth=2,\n",
    "#                       ticklen=9,\n",
    "                      tickcolor='grey',\n",
    "#                       tick0=\"2018-11-9\" ,\n",
    "#                       dtick=86400000.0*1,    # milliseconds in a day, every 7 days\n",
    "                      #### Minor Ticks\n",
    "                       minor=dict(\n",
    "                         dtick=86400000.0, # milliseconds in a day\n",
    "                         tickwidth=1,\n",
    "                         ticklen=4,\n",
    "                         tickcolor='grey',\n",
    "                         ticks='inside'),\n",
    "                      ### GRID\n",
    "                       gridcolor='gainsboro',\n",
    "                       gridwidth=1,\n",
    "                       layer='above traces',\n",
    "                       tickangle=0,\n",
    "                       row=i, col=1)\n",
    "    fig.update_yaxes(showline=True,      # add line at x=0\n",
    "                         showticklabels=True,\n",
    "                         linecolor='black',  # line color\n",
    "                         linewidth=1,        # line size\n",
    "                     ticks='inside',     # ticks outside axis\n",
    "                     tickfont=font_dict, # tick label font\n",
    "                     mirror='allticks',  # add ticks to top/right axes\n",
    "                     tickwidth=1,      # tick width\n",
    "                     tickcolor='black',  # tick color\n",
    "                     gridcolor='gainsboro',\n",
    "                     gridwidth=1,\n",
    "                     layer='above traces',\n",
    "                     row=i, col=1)\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text=\"Density\", \n",
    "                 type=\"log\", \n",
    "                 exponentformat= 'power',row=1, col=1)\n",
    "# fig.update_xaxes(title_text=\"date\", \n",
    "#                  exponentformat= 'power',row=1, col=1)\n",
    "fig.update_layout(title=\"ICESat2 3-hr Scaled Model Rho vs GRACE-FO Rho, All Norm to 500km\",\n",
    "                  autosize=False,    width=1000,    height=700,\n",
    "                  legend= {'itemsizing': 'trace'},\n",
    "                  font=font_dict, plot_bgcolor='white', \n",
    "                 )\n",
    "\n",
    "fig.show(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7fbc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.032893Z",
     "start_time": "2024-07-23T17:43:13.032880Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9dc63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.034346Z",
     "start_time": "2024-07-23T17:43:13.034332Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pygeodyn.pygeodyn_plot_scalingfactors import *\n",
    "import os\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "def read_nc_file( filename, variables):\n",
    "    ''' This function reads the TIEGCM .nc files and saves the given input variables to a dictionary.\n",
    "        The breakloop feature is here so that if the file doesn't exist the code can still continue.  '''\n",
    "    status = os.path.exists(filename)\n",
    "    \n",
    "    if status == True:\n",
    "        data = {}\n",
    "        for i, var_names in enumerate(variables):\n",
    "            ncid =  Dataset(filename,\"r+\", format=\"NETCDF4\")# filename must be a string\n",
    "            varData = ncid.variables\n",
    "            data[var_names] = np.array(varData[var_names])  \n",
    "    elif status == False:\n",
    "        print('No File Found', filename )\n",
    "        breakloop = True\n",
    "        data = 0\n",
    "        return( data , breakloop)\n",
    "    breakloop = False\n",
    "    return(data,breakloop )\n",
    "\n",
    "\n",
    "arc_list = []\n",
    "\n",
    "arc_list_18 = np.arange(270,365)\n",
    "for i in arc_list_18:\n",
    "    val = '2018'+str(i)\n",
    "    arc_list.append(int(val))\n",
    "    \n",
    "    #     print(val)\n",
    "    \n",
    "# arc_list_19 = np.arange(1,112)\n",
    "# for i in arc_list_19:\n",
    "#     val = f\"2019{i:03d}\"\n",
    "#     arc_list.append(int(val))\n",
    "\n",
    "\n",
    "path_to_f107 = '/data/SatDragModelValidation/data/inputs/atmos_models/geo_phys_indicies/gpi_1960001-2021243_f107aDaily.nc'\n",
    "\n",
    "f107_data = read_nc_file(path_to_f107, ['year_day', 'f107d', 'f107a', 'kp'])\n",
    "\n",
    "\n",
    "date = []\n",
    "kp_list = []\n",
    "f107d_list = []\n",
    "f107a_list  = []\n",
    "date_3hr = []\n",
    "doy_list    = []\n",
    "\n",
    "\n",
    "\n",
    "for i,val in enumerate(arc_list):\n",
    "    \n",
    "    index = f107_data[0]['year_day']==val\n",
    "    kp_list.append(f107_data[0]['kp'][index][0])\n",
    "    f107d_list.append(f107_data[0]['f107d'][index][0])\n",
    "    f107a_list.append(f107_data[0]['f107a'][index][0])\n",
    "    doy_list.append(str(f107_data[0]['year_day'][index][0])[-3:])\n",
    "\n",
    "    date.append(pd.to_datetime( str(val), format='%Y%j'))\n",
    "\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=0))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=3))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=6))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=9))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=12))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=15))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=18))\n",
    "    date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=21))\n",
    "#     date_3hr.append(pd.to_datetime( str(val), format='%Y%j') +pd.Timedelta(hours=24))\n",
    "    \n",
    "kp_expand = []\n",
    "for i in kp_list:\n",
    "    for ii in i:\n",
    "        kp_expand.append(ii)\n",
    "        \n",
    "        \n",
    "        \n",
    "solar_fluxes = {}\n",
    "solar_fluxes['f107d_list'] = f107d_list\n",
    "solar_fluxes['f107a_list'] = f107a_list\n",
    "solar_fluxes['date']       = date\n",
    "solar_fluxes['date_3hr']   = date_3hr\n",
    "solar_fluxes['kp_expand']  = kp_expand\n",
    "\n",
    "f107d_earth = []\n",
    "f107a_earth = []\n",
    "######################################################################### \n",
    "##### Account for the F10.7 at earth (instead of referenced at 1AU) #####\n",
    "######################################################################### \n",
    "\n",
    "for i_doy,val_doy in enumerate(doy_list):\n",
    "    iday = int(val_doy)\n",
    "    theta0 = 2 * np.pi * (iday)/365.\n",
    "    sfeps = 1.000110 + 0.034221*np.cos(theta0)+0.001280* np.sin(theta0) +0.000719*np.cos(2.*theta0)+0.000077*np.sin(2.*theta0)\n",
    "\n",
    "    f107d_earth.append(sfeps * solar_fluxes['f107d_list'][i_doy])\n",
    "    f107a_earth.append(sfeps * solar_fluxes['f107a_list'][i_doy])\n",
    "\n",
    "solar_fluxes['f107d_earth'] = f107d_earth\n",
    "solar_fluxes['f107a_earth'] = f107a_earth\n",
    "\n",
    " \n",
    "del f107d_earth\n",
    "del f107a_earth\n",
    "del kp_expand\n",
    "del f107d_list\n",
    "del f107a_list\n",
    "del date\n",
    "del date_3hr\n",
    "del f107_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a8018",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.037203Z",
     "start_time": "2024-07-23T17:43:13.037189Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# fig = make_subplots(rows=1, cols=1)\n",
    "fig = make_subplots(rows=2, cols=1, row_heights=[0.3, 0.7],\n",
    "                    specs=[[{\"secondary_y\": True}],\n",
    "                           [{\"secondary_y\": True}]],\n",
    "                           shared_xaxes=True,\n",
    "                           vertical_spacing=0.02)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=solar_fluxes['date'],\n",
    "                           y=solar_fluxes['f107d_earth'],\n",
    "                           name= 'F107d_1AU',\n",
    "                           mode='lines',\n",
    "                           opacity=1,\n",
    "                           line = dict(shape = 'hvh',dash='dash', color = 'blue', width=2),\n",
    "                           showlegend=False),\n",
    "                           secondary_y=True,row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=solar_fluxes['date_3hr'],\n",
    "                           y=solar_fluxes['kp_expand'],\n",
    "                           name= 'Kp',\n",
    "                           mode='lines',\n",
    "                           opacity=1,\n",
    "                           line = dict(shape = 'hvh', color = 'black', width=2),\n",
    "                           showlegend=False),\n",
    "                           secondary_y=False,row=1, col=1) \n",
    "\n",
    "\n",
    "\n",
    "(time_avg, den_gfo ) = orbit_avg_generic(gfo_df['Date'], gfo_df['D500_gfo'], gfo_df['lat'])    \n",
    "fig.add_trace(go.Scattergl(x=time_avg,\n",
    "                            y=den_gfo,\n",
    "                            name=f'GFO_500km',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4, color='black' ),\n",
    "                            showlegend=False),\n",
    "                            row=2, col=1)\n",
    "\n",
    "(time_avg, den_avg ) = orbit_avg_generic(ice_df['datescaled'],ice_df['D500_ice'],ice_df['lat'])    \n",
    "# remove the datapoints that are definitely artifacts of the orbit average\n",
    "# 74, 334,  365  \n",
    "\n",
    "# time_avg[74]=np.nan\n",
    "# time_avg[334]=np.nan\n",
    "# time_avg[365]=np.nan\n",
    "# den_avg[74]=np.nan\n",
    "# den_avg[334]=np.nan\n",
    "# den_avg[365]=np.nan\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=time_avg,\n",
    "                            y=den_avg,\n",
    "                            name=f'D500_icesat2',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4, color='blue'),\n",
    "                            showlegend=False),\n",
    "                            row=2, col=1)\n",
    "\n",
    "\n",
    "\n",
    "#### FANCY LEGEND ################################################################\n",
    "modelnames=[]\n",
    "modelcolors = []\n",
    "# for model in run_list:\n",
    "#     if model == 'msis2':\n",
    "#         modelnames.append(\"MSISe2\")\n",
    "#         modelcolors.append(coldict[model])\n",
    "#     elif model == 'dtm2020_o':\n",
    "#         modelnames.append(\"DTM2020\")\n",
    "#         modelcolors.append(coldict[model])\n",
    "#     elif model == 'jb2008':\n",
    "#         modelnames.append(\"JB2008\")\n",
    "#         modelcolors.append(coldict[model])\n",
    "modelnames.append(\"GraceFO\")\n",
    "modelcolors.append('black')\n",
    "modelnames.append(\"ICESat2\")\n",
    "modelcolors.append('blue')\n",
    "df_leg = pd.DataFrame({\"starts_colors\": modelcolors})\n",
    "fig.update_traces(showlegend=False).add_traces(\n",
    "    [   go.Scattergl(name=modelnames[i],\n",
    "               x=[pd.to_datetime( \"181107-000000\", format='%y%m%d-%H%M%S')],\n",
    "               mode='lines',\n",
    "               line = dict(shape = 'hv',  width=10),\n",
    "               marker_color=c,\n",
    "               showlegend=True)\n",
    "        for i,c in enumerate((df_leg.loc[:,[\"starts_colors\"]].values.ravel()))])\n",
    "## Legend Control\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=.65,\n",
    "    xanchor=\"center\",\n",
    "    x=.65,\n",
    "    orientation=\"h\",\n",
    "        font=dict(family='Arial',size=12,color='black'),\n",
    "        bgcolor=\"white\",\n",
    "        bordercolor=\"darkgrey\",\n",
    "        borderwidth=0.5,)  )\n",
    "################################################################################\n",
    "\n",
    "\n",
    "### UPDATE AXES \n",
    "fig.update_yaxes(title_text=\"Kp\", \n",
    "                 exponentformat= 'power',\n",
    "                 range=[0,7],\n",
    "                 secondary_y=False,\n",
    "                 row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"F10.7\", \n",
    "                 exponentformat= 'power',\n",
    "                 range=[40,80],\n",
    "                 secondary_y=True,\n",
    "                 tickfont=dict(color=\"blue\"),\n",
    "                 titlefont=dict(color=\"blue\"),\n",
    "                 row=1, col=1)\n",
    "################################################################################\n",
    "\n",
    "### SYLIZE LEGEND \n",
    "font_dict=dict(family='Arial',size=11,color='black')\n",
    "## automate the specification of the axes for subplots\n",
    "rownum, colnum = fig._get_subplot_rows_columns()\n",
    "for i in rownum:\n",
    "    if len(rownum)==1:\n",
    "        L_ticklabel = False\n",
    "    else:\n",
    "        if i < len(rownum):\n",
    "            L_ticklabel = False\n",
    "        else:\n",
    "            L_ticklabel = True\n",
    "    fig.update_xaxes(### LINE at axis border\n",
    "                      showline=True,\n",
    "                      showticklabels=L_ticklabel,\n",
    "#                       tickformat= '%m/%d',\n",
    "                      linecolor='black',\n",
    "                      linewidth=1,\n",
    "                     ### Major ticks\n",
    "                      ticks='inside',\n",
    "                      tickfont=font_dict,\n",
    "                      mirror=True,\n",
    "#                       tickwidth=2,\n",
    "#                       ticklen=9,\n",
    "                      tickcolor='grey',\n",
    "#                       tick0=\"2018-11-9\" ,\n",
    "#                       dtick=86400000.0*1,    # milliseconds in a day, every 7 days\n",
    "                      #### Minor Ticks\n",
    "                       minor=dict(\n",
    "                         dtick=86400000.0, # milliseconds in a day\n",
    "                         tickwidth=1,\n",
    "                         ticklen=4,\n",
    "                         tickcolor='grey',\n",
    "                         ticks='inside'),\n",
    "                      ### GRID\n",
    "                       gridcolor='gainsboro',\n",
    "                       gridwidth=1,\n",
    "                       layer='above traces',\n",
    "                       tickangle=0,\n",
    "                       row=i, col=1)\n",
    "    fig.update_yaxes(showline=True,      # add line at x=0\n",
    "                         showticklabels=True,\n",
    "                         linecolor='black',  # line color\n",
    "                         linewidth=1,        # line size\n",
    "                     ticks='inside',     # ticks outside axis\n",
    "                     tickfont=font_dict, # tick label font\n",
    "                     mirror='allticks',  # add ticks to top/right axes\n",
    "                     tickwidth=1,      # tick width\n",
    "                     tickcolor='black',  # tick color\n",
    "                     gridcolor='gainsboro',\n",
    "                     gridwidth=1,\n",
    "                     layer='above traces',\n",
    "                     row=i, col=1)\n",
    "\n",
    "\n",
    "fig.update_xaxes(range=[pd.to_datetime( \"181101-000000\", format='%y%m%d-%H%M%S'),\n",
    "                        pd.to_datetime( \"181130-000000\", format='%y%m%d-%H%M%S')],row=1, col=1)\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text=\"Density\", \n",
    "#                  type=\"log\", \n",
    "                 exponentformat= 'power',row=2, col=1)\n",
    "fig.update_layout(title=\"ICESat2 3-hr Scaled Model Rho vs GRACE-FO Rho, All Norm to 500km\",\n",
    "                  autosize=False,    width=1000,    height=700,\n",
    "                  legend= {'itemsizing': 'trace'},\n",
    "                  font=font_dict, plot_bgcolor='white', \n",
    "                 )\n",
    "\n",
    "fig.show(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20568b2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.038057Z",
     "start_time": "2024-07-23T17:43:13.038044Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # fig.add_trace(go.Scattergl(x=gfo_df['Date'][:10000],\n",
    "# #                             y=gfo_df['D500_gfo'][:10000],\n",
    "# #                             name=f'D500_gfo',\n",
    "# #                             mode='markers',\n",
    "# #                             opacity=1,\n",
    "# #                             marker=dict( size=4 ),\n",
    "# #                             showlegend=True),\n",
    "# #                             row=1, col=1)\n",
    "\n",
    "# # fig.add_trace(go.Scattergl(x=ice_df['datescaled'],\n",
    "# #                             y=ice_df['D500_ice'],\n",
    "# #                             name=f'D500_icesat2',\n",
    "# #                             mode='markers',\n",
    "# #                             opacity=1,\n",
    "# #                             marker=dict( size=4 ),\n",
    "# #                             showlegend=True),\n",
    "# #                             row=1, col=1)\n",
    "# (time_avg, den_gfo ) = orbit_avg_generic(gfo_df['Date'], gfo_df['D500_gfo'], gfo_df['lat'])    \n",
    "# fig.add_trace(go.Scattergl(x=time_avg,\n",
    "#                             y=den_gfo,\n",
    "#                             name=f'D500_gfo',\n",
    "#                             mode='markers',\n",
    "#                             opacity=1,\n",
    "#                             marker=dict( size=4 ),\n",
    "#                             showlegend=True),\n",
    "#                             row=1, col=1)\n",
    "\n",
    "# (time_avg, den_avg ) = orbit_avg_generic(ice_df['datescaled'],ice_df['D500_ice'],ice_df['lat'])    \n",
    "# # remove the datapoints that are definitely artifacts of the orbit average\n",
    "# # 74, 334,  365  \n",
    "\n",
    "# time_avg[74]=np.nan\n",
    "# time_avg[334]=np.nan\n",
    "# time_avg[365]=np.nan\n",
    "# den_avg[74]=np.nan\n",
    "# den_avg[334]=np.nan\n",
    "# den_avg[365]=np.nan\n",
    "\n",
    "\n",
    "# fig.add_trace(go.Scattergl(x=time_avg,\n",
    "#                             y=den_avg,\n",
    "#                             name=f'D500_icesat2',\n",
    "#                             mode='markers',\n",
    "#                             opacity=1,\n",
    "#                             marker=dict( size=4 ),\n",
    "#                             showlegend=True),\n",
    "#                             row=1, col=1)\n",
    "\n",
    "\n",
    "\n",
    "# ### SYLIZE AXES \n",
    "# font_dict=dict(family='Arial',size=11,color='black')\n",
    "# ## automate the specification of the axes for subplots\n",
    "# rownum, colnum = fig._get_subplot_rows_columns()\n",
    "# for i in rownum:\n",
    "#     if len(rownum)==1:\n",
    "#         L_ticklabel = True\n",
    "#     else:\n",
    "#         if i < len(rownum):\n",
    "#             L_ticklabel = True\n",
    "#         else:\n",
    "#             L_ticklabel = True\n",
    "#     fig.update_xaxes(### LINE at axis border\n",
    "#                       showline=True,\n",
    "#                       showticklabels=L_ticklabel,\n",
    "# #                       tickformat= '%m/%d',\n",
    "#                       linecolor='black',\n",
    "#                       linewidth=1,\n",
    "#                      ### Major ticks\n",
    "#                       ticks='inside',\n",
    "#                       tickfont=font_dict,\n",
    "#                       mirror=True,\n",
    "# #                       tickwidth=2,\n",
    "# #                       ticklen=9,\n",
    "#                       tickcolor='grey',\n",
    "# #                       tick0=\"2018-11-9\" ,\n",
    "# #                       dtick=86400000.0*1,    # milliseconds in a day, every 7 days\n",
    "#                       #### Minor Ticks\n",
    "#                        minor=dict(\n",
    "#                          dtick=86400000.0, # milliseconds in a day\n",
    "#                          tickwidth=1,\n",
    "#                          ticklen=4,\n",
    "#                          tickcolor='grey',\n",
    "#                          ticks='inside'),\n",
    "#                       ### GRID\n",
    "#                        gridcolor='gainsboro',\n",
    "#                        gridwidth=1,\n",
    "#                        layer='above traces',\n",
    "#                        tickangle=0,\n",
    "#                        row=i, col=1)\n",
    "#     fig.update_yaxes(showline=True,      # add line at x=0\n",
    "#                          showticklabels=True,\n",
    "#                          linecolor='black',  # line color\n",
    "#                          linewidth=1,        # line size\n",
    "#                      ticks='inside',     # ticks outside axis\n",
    "#                      tickfont=font_dict, # tick label font\n",
    "#                      mirror='allticks',  # add ticks to top/right axes\n",
    "#                      tickwidth=1,      # tick width\n",
    "#                      tickcolor='black',  # tick color\n",
    "#                      gridcolor='gainsboro',\n",
    "#                      gridwidth=1,\n",
    "#                      layer='above traces',\n",
    "#                      row=i, col=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig.update_yaxes(title_text=\"Density\", \n",
    "#                  type=\"log\", \n",
    "#                  exponentformat= 'power',row=1, col=1)\n",
    "# fig.update_layout(#title=\"ICESat2 3-hr Scaled Model Rho vs GRACE-FO Rho\",\n",
    "#                   autosize=True,#    width=1000,    height=700,\n",
    "#                   legend= {'itemsizing': 'trace'},\n",
    "#                   font=font_dict, plot_bgcolor='white', \n",
    "#                  )\n",
    "\n",
    "# fig.show(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ee217",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.038888Z",
     "start_time": "2024-07-23T17:43:13.038874Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,val in enumerate(time_avg):\n",
    "    print(i,val)\n",
    "\n",
    "# 74, 334,  365  \n",
    "#25th\n",
    "#29th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026c87f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T22:46:35.497276Z",
     "start_time": "2024-02-13T22:46:35.475241Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4788ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.039693Z",
     "start_time": "2024-07-23T17:43:13.039680Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.to_datetime('2018-11-07 00:19:14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb8594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.040513Z",
     "start_time": "2024-07-23T17:43:13.040500Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c71eb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.041293Z",
     "start_time": "2024-07-23T17:43:13.041280Z"
    }
   },
   "outputs": [],
   "source": [
    "np.where(time_avg==pd.to_datetime('2018-11-07 00:19:14'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa018b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.042128Z",
     "start_time": "2024-07-23T17:43:13.042115Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig.add_trace(go.Scattergl(x=gfo_df['Date'],\n",
    "#                             y=gfo_df['D500_gfo'],\n",
    "#                             name=f'D500_gfo',\n",
    "#                             mode='markers',\n",
    "#                             opacity=1,\n",
    "#                             marker=dict( size=4 ),\n",
    "#                             showlegend=True),\n",
    "#                             row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=ice_df['dates'],\n",
    "                            y=ice_df['D500_ice'],\n",
    "                            name=f'D500_icesat2',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4 ),\n",
    "                            showlegend=True),\n",
    "                            row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scattergl(x=ice_df['dates'],\n",
    "                            y=ice_df['Rho_x'],\n",
    "                            name=f'Rho_x_icesat2',\n",
    "                            mode='markers',\n",
    "                            opacity=1,\n",
    "                            marker=dict( size=4 ),\n",
    "                            showlegend=True),\n",
    "                            row=1, col=1)\n",
    "\n",
    "\n",
    "\n",
    "### SYLIZE AXES \n",
    "font_dict=dict(family='Arial',size=11,color='black')\n",
    "## automate the specification of the axes for subplots\n",
    "rownum, colnum = fig._get_subplot_rows_columns()\n",
    "for i in rownum:\n",
    "    if len(rownum)==1:\n",
    "        L_ticklabel = True\n",
    "    else:\n",
    "        if i < len(rownum):\n",
    "            L_ticklabel = True\n",
    "        else:\n",
    "            L_ticklabel = True\n",
    "    fig.update_xaxes(### LINE at axis border\n",
    "                      showline=True,\n",
    "                      showticklabels=L_ticklabel,\n",
    "#                       tickformat= '%m/%d',\n",
    "                      linecolor='black',\n",
    "                      linewidth=1,\n",
    "                     ### Major ticks\n",
    "                      ticks='inside',\n",
    "                      tickfont=font_dict,\n",
    "                      mirror=True,\n",
    "#                       tickwidth=2,\n",
    "#                       ticklen=9,\n",
    "                      tickcolor='grey',\n",
    "#                       tick0=\"2018-11-9\" ,\n",
    "#                       dtick=86400000.0*1,    # milliseconds in a day, every 7 days\n",
    "                      #### Minor Ticks\n",
    "                       minor=dict(\n",
    "                         dtick=86400000.0, # milliseconds in a day\n",
    "                         tickwidth=1,\n",
    "                         ticklen=4,\n",
    "                         tickcolor='grey',\n",
    "                         ticks='inside'),\n",
    "                      ### GRID\n",
    "                       gridcolor='gainsboro',\n",
    "                       gridwidth=1,\n",
    "                       layer='above traces',\n",
    "                       tickangle=0,\n",
    "                       row=i, col=1)\n",
    "    fig.update_yaxes(showline=True,      # add line at x=0\n",
    "                         showticklabels=True,\n",
    "                         linecolor='black',  # line color\n",
    "                         linewidth=1,        # line size\n",
    "                     ticks='inside',     # ticks outside axis\n",
    "                     tickfont=font_dict, # tick label font\n",
    "                     mirror='allticks',  # add ticks to top/right axes\n",
    "                     tickwidth=1,      # tick width\n",
    "                     tickcolor='black',  # tick color\n",
    "                     gridcolor='gainsboro',\n",
    "                     gridwidth=1,\n",
    "                     layer='above traces',\n",
    "                     row=i, col=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.update_yaxes(title_text=\"Density\", \n",
    "                 type=\"log\", \n",
    "                 exponentformat= 'power',row=1, col=1)\n",
    "fig.update_layout(#title=\"ICESat2 3-hr Scaled Model Rho vs GRACE-FO Rho\",\n",
    "                  autosize=True,#    width=1000,    height=700,\n",
    "                  legend= {'itemsizing': 'trace'},\n",
    "                  font=font_dict, plot_bgcolor='white', \n",
    "                 )\n",
    "\n",
    "fig.show(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0cff3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T17:43:13.043103Z",
     "start_time": "2024-07-23T17:43:13.043090Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np \n",
    "# import sys  \n",
    "# from scipy.io import loadmat  #allows us to read in .mat files\n",
    "# from datetime import datetime, timedelta\n",
    "# import gc\n",
    "\n",
    "# #### MAKE MSIS Take the 3HR Ap values\n",
    "# from pymsis import msis\n",
    "# SWI_option = [1.0]*25\n",
    "# SWI_option[8] = -1.0\n",
    "#             #  C    AP - MAGNETIC INDEX(DAILY) OR WHEN SW(9)=-1. :\n",
    "#             #  C      - ARRAY CONTAINING:\n",
    "#             #  C       (1) DAILY AP\n",
    "#             #  C       (2) 3 HR AP INDEX FOR CURRENT TIME\n",
    "#             #  C       (3) 3 HR AP INDEX FOR 3 HRS BEFORE CURRENT TIME\n",
    "#             #  C       (4) 3 HR AP INDEX FOR 6 HRS BEFORE CURRENT TIME\n",
    "#             #  C       (5) 3 HR AP INDEX FOR 9 HRS BEFORE CURRENT TIME\n",
    "#             #  C       (6) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 12 TO 33 HRS PRIOR\n",
    "#             #  C          TO CURRENT TIME\n",
    "#             #  C       (7) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 36 TO 57 HRS PRIOR\n",
    "#             #  C          TO CURRENT TIME\n",
    "\n",
    "\n",
    "# #################################\n",
    "# years =  [2002]#, 2003, 2004, 2005, 2006, 2007, 2008, 2009]\n",
    "# days = np.arange(1,2)\n",
    "# path_champ = '/space/DNR_data/CHAMP_2002_2010/'\n",
    "# #################################\n",
    "\n",
    "\n",
    "# def make_champ_timeseries_v2_redorenorm(years, days):\n",
    "\n",
    "#     \"\"\"This function redoes the construction of the CHAMP timeseries for the orbit.\n",
    "\n",
    "#     Changes from the original include:\n",
    "#         - Use F10.7 indicies that are scaled with MgII and re-referenced to the Earth's locatiion (instead of at 1AU)\n",
    "#         - Use MSIS2.0 for the normalization to 400km\n",
    "#         - Use a 3Hour Ap input as required by MSIS for the more granular Stormtime forcing option.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     \"\"\"    \n",
    "\n",
    "\n",
    "#     path_champ = '/space/DNR_data/CHAMP_2002_2010/'\n",
    "\n",
    "\n",
    "#     #### Load MgII Scaled F10.7 Values:\n",
    "#     import pickle\n",
    "#     dir_save = '/space/DNR_data/'\n",
    "#     filehandler = open(dir_save+'MgII_F107_KpAp'+'.pkl', 'rb') \n",
    "#     mgII_data = pickle.load(filehandler)\n",
    "#     filehandler.close()\n",
    "\n",
    "#     #### clear up some space        \n",
    "#     truncate_date    = np.logical_and(mgII_data['Date'].year>=2000 , mgII_data['Date'].year<=2011 )\n",
    "#     truncate_date3hr =  np.logical_and(mgII_data['Date_3hrAp'].year>=2000 , mgII_data['Date_3hrAp'].year<=2011 )#np.logical_and(mgII_data['Date_3hrAp'].year==year , mgII_data['Date_3hrAp'].dayofyear==day )\n",
    "\n",
    "#     mgII_data['Date_3hrAp'] = mgII_data['Date_3hrAp'][truncate_date3hr]\n",
    "#     mgII_data['Ap']         = np.array(mgII_data['Ap'])[truncate_date3hr]\n",
    "#     mgII_data['Date']       = mgII_data['Date'][truncate_date]\n",
    "#     mgII_data['Ap_dailyavg'] =  np.array(mgII_data['Ap_dailyavg'])[truncate_date]\n",
    "#     mgII_data['f107d_earth'] = np.array(mgII_data['f107d_earth'])[truncate_date]\n",
    "#     mgII_data['f107a_earth'] = np.array(mgII_data['f107a_earth'])[truncate_date]\n",
    "\n",
    "#     del mgII_data['DOY']\n",
    "#     del mgII_data['kp']\n",
    "#     del mgII_data['f107d']\n",
    "#     del mgII_data['f107a']\n",
    "#     del mgII_data['year_day']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #     noaa = pd.read_pickle('/space/DNR_data/noaa_2002_2010_pickle')\n",
    "#     #     noaa['f107d'][noaa['f107d'].astype(float) <= 60] = noaa['f107a'][noaa['f107d'].astype(float) <= 60]\n",
    "\n",
    "#     tleng = 0\n",
    "#     time_full= []\n",
    "#     Year         = []\n",
    "#     Doy          = []\n",
    "#     Hours        = []\n",
    "#     Lon          = []\n",
    "#     Lat          = []\n",
    "#     LatBin       = []\n",
    "#     Height       = []\n",
    "#     LocTim       = []\n",
    "#     CHAMPDensity = []\n",
    "#     D400_msis00  = []  # normalized quantity\n",
    "#     rhosat_msis00= []\n",
    "#     rhosat_msis2 = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "#     rho400_msis2 = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "#     D400_msis2   = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "\n",
    "#     rhosat_tiegcm = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "#     rho400_tiegcm = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "#     D400_tiegcm   = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "\n",
    "#     #     Cd           = []\n",
    "#     # Ap_dayvals   = []\n",
    "#     # f107a_dayvals= []\n",
    "#     # f107d_dayvals= []\n",
    "#     date = []\n",
    "\n",
    "#     i = 0\n",
    "#     for iyear,year in enumerate(years):\n",
    "#         for iday,day in enumerate(days):\n",
    "#                 ####---------------------------------------------\n",
    "#                 #### Gather the Necessary Flux and Ap Information\n",
    "#                 index_date = np.logical_and(mgII_data['Date'].year==year , mgII_data['Date'].dayofyear==day )\n",
    "#                 f107a = float(np.squeeze(np.asarray(mgII_data['f107a_earth'])[index_date]))\n",
    "#                 f107d = float(np.squeeze(np.asarray(mgII_data['f107d_earth'])[index_date]))\n",
    "#                 Ap_daily_avg = float(np.squeeze(np.asarray(mgII_data['Ap_dailyavg'])[index_date]))\n",
    "\n",
    "\n",
    "#                 ### Construct the necessary 3hr Ap Array to go into MSIS\n",
    "# #                 print(f\"----------------------------------------------------------\")\n",
    "\n",
    "#                 print(f\"Year: {year} / Day: {day}\")\n",
    "#                 for it, itime in enumerate( champ['Hours'][:leng]):\n",
    "\n",
    "#                     lon   = champ['Lon'][it]\n",
    "#                     lat   = champ['Lat'][it]\n",
    "#                     dates = datetime(year, 1, 1) + timedelta(float(day) - 1) + timedelta(hours = champ['Hours'][it]) \n",
    "#                     f107din = [f107d]\n",
    "#                     f107ain = [f107a]\n",
    "#                     time_sat = itime\n",
    "\n",
    "#                     if str(interpolate_tiegcm(TIEGCM, lon, lat, time_sat, 400,                 'DEN')*1e3)=='-inf':\n",
    "#                         print('-inf FOUND')\n",
    "#                         continue\n",
    "#                     elif str(interpolate_tiegcm(TIEGCM, lon, lat, time_sat, 400,                 'DEN')*1e3)=='inf':\n",
    "#                         print('inf FOUND')\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         date.append(datetime(year, 1, 1) + timedelta(days = float(day)-1,  hours = itime )) \n",
    "\n",
    "#                         index_date3hr = np.logical_and(mgII_data['Date_3hrAp'].year==year , mgII_data['Date_3hrAp'].dayofyear==day )\n",
    "#                         indexvals =  [i for i, x in enumerate(index_date3hr) if x]\n",
    "#                         Ap_doy_windows = mgII_data['Date_3hrAp'][indexvals]\n",
    "\n",
    "#                         #### Find the Current 3hr Kp window:A\n",
    "#                         Ap_windw_hrs = [i.hour for i in Ap_doy_windows]\n",
    "#                         Ap_windw_hrs = np.append(np.array(Ap_windw_hrs),24)  ## add the final window edge\n",
    "\n",
    "#                         index_current_Ap = int(np.digitize([dates.hour],Ap_windw_hrs))\n",
    "#                         if index_current_Ap==8:\n",
    "#                             index_current_Ap += -1\n",
    "#                         indexglobal_currentAp = indexvals[index_current_Ap]\n",
    "\n",
    "#                         Ap_3HR_current        = mgII_data['Ap'][indexglobal_currentAp]\n",
    "#                         Ap_3HR_prior          = mgII_data['Ap'][indexglobal_currentAp-1]\n",
    "#                         Ap_6HR_prior          = mgII_data['Ap'][indexglobal_currentAp-2]\n",
    "#                         Ap_9HR_prior          = mgII_data['Ap'][indexglobal_currentAp-3]\n",
    "#                         Ap_12hr_33hr_priorAVG = np.mean(mgII_data['Ap'][indexglobal_currentAp-11 :indexglobal_currentAp-3 ] ) ### 33hrs to 12 hours\n",
    "#                         Ap_36hr_57hr_priorAVG = np.mean(mgII_data['Ap'][indexglobal_currentAp-19 :indexglobal_currentAp-11 ])  ### 36hrs to 57 hours\n",
    "\n",
    "#                         apsin = [[Ap_daily_avg,          # (1) DAILY AP\n",
    "#                                   Ap_3HR_current,        # (2) 3 HR AP INDEX FOR CURRENT TIME\n",
    "#                                   Ap_3HR_prior,          # (3) 3 HR AP INDEX FOR 3 HRS BEFORE CURRENT TIME\n",
    "#                                   Ap_6HR_prior,          # (4) 3 HR AP INDEX FOR 6 HRS BEFORE CURRENT TIME\n",
    "#                                   Ap_9HR_prior,          # (5) 3 HR AP INDEX FOR 9 HRS BEFORE CURRENT TIME\n",
    "#                                   Ap_12hr_33hr_priorAVG, # (6) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 12 TO 33 HRS PRIOR\n",
    "#                                   Ap_36hr_57hr_priorAVG]]# (7) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 36 TO 57 HRS PRIOR\n",
    "\n",
    "#                         output2_sat = msis.run(dates, lon, lat, champ['Height'][it], f107din, f107ain, apsin, version = 2, options=SWI_option)\n",
    "#                         output2_400 = msis.run(dates, lon, lat, 400, f107din, f107ain, apsin                , version = 2, options=SWI_option)\n",
    "\n",
    "\n",
    "\n",
    "#                         ### add the values to the growing lists\n",
    "#                         rhosat_msis2[tleng+it] = output2_sat[0,0,0,0][0]\n",
    "#                         rho400_msis2[tleng+it] = output2_400[0,0,0,0][0]\n",
    "#                         ### RENORMALIZE with MSIS2\n",
    "#                         D400_msis2[tleng+it]   = champ['Density'][it] * (output2_400[0,0,0,0][0] / output2_sat[0,0,0,0][0])   # normalized density to 400km with MSIS2\n",
    "\n",
    "#                         ##### Interpolate the TIEGCM Data to the Satellites Position and Time\n",
    "# #                         print(time_sat)\n",
    "# #                         print(lon)\n",
    "# #                         print(lat)\n",
    "# #                         print(interpolate_tiegcm(TIEGCM, lon, lat, time_sat, champ['Height'][it], 'DEN')*1e3)\n",
    "# #                         print(interpolate_tiegcm(TIEGCM, lon, lat, time_sat, 400,                 'DEN')*1e3)\n",
    "#                         val_tiegcm_sat = interpolate_tiegcm(TIEGCM, lon, lat, time_sat, champ['Height'][it], 'DEN')*1e3\n",
    "#                         val_tiegcm_400 = interpolate_tiegcm(TIEGCM, lon, lat, time_sat, 400,                 'DEN')*1e3\n",
    "#                         rhosat_tiegcm[tleng+it] = val_tiegcm_sat\n",
    "#                         rho400_tiegcm[tleng+it] = val_tiegcm_400\n",
    "#                         ### RENORMALIZE with TIEGCM\n",
    "# #                         print('msis2', champ['Density'][it] * (output2_400[0,0,0,0][0] / output2_sat[0,0,0,0][0]))\n",
    "# #                         print('val_tiegcm_sat', val_tiegcm_sat)\n",
    "# #                         print('val_tiegcm_400', val_tiegcm_400)\n",
    "# #                         print('tiegcm', champ['Density'][it] * (val_tiegcm_400 / val_tiegcm_sat))\n",
    "#                         D400_tiegcm[tleng+it]   = champ['Density'][it] * (val_tiegcm_400 / val_tiegcm_sat)   # normalized density to 400km with TIEGCM\n",
    "\n",
    "\n",
    "\n",
    "#                 tleng = tleng + leng\n",
    "#     #                 print(year,'/',day)\n",
    "#                 i+=1\n",
    "#                 print('Closing', sec_file)\n",
    "#                 ncid =  Dataset(sec_file,\"r+\", format=\"NETCDF4\")        \n",
    "#                 ncid.close()\n",
    "#                 gc.collect()\n",
    "\n",
    "\n",
    "#             elif breakloop == True:\n",
    "#                 i+=1\n",
    "#                 continue\n",
    "\n",
    "#     #### REMOVE NANS\n",
    "#     # From Msis lists\n",
    "#     rhosat_msis2 = rhosat_msis2[~np.isnan(rhosat_msis2)]\n",
    "#     rho400_msis2 = rho400_msis2[~np.isnan(rho400_msis2)]\n",
    "#     D400_msis2   = D400_msis2[~np.isnan(D400_msis2)]\n",
    "#     # From tiegcm lists\n",
    "#     rhosat_tiegcm = rhosat_tiegcm[~np.isnan(rhosat_tiegcm)]\n",
    "#     rho400_tiegcm = rho400_tiegcm[~np.isnan(rho400_tiegcm)]\n",
    "#     D400_tiegcm   = D400_tiegcm[~np.isnan(D400_tiegcm)]\n",
    "\n",
    "\n",
    "#     df = pd.DataFrame(data={'Date' :date ,\n",
    "#                             'Year'          : Year,\n",
    "#                             'Doy'           : Doy,\n",
    "#                             'Hours'         : Hours,\n",
    "#                             'Lon'           : Lon,\n",
    "#                             'Lat'           : Lat, \n",
    "#                             'LatBin'        : LatBin,\n",
    "#                             'Height'        : Height,\n",
    "#                             'LocTim'        : LocTim,\n",
    "#                             'CHAMPDensity'  : CHAMPDensity,\n",
    "#                             'rhosat_msis00' : rhosat_msis00, ### MSIS00 density @Satellite Altitude\n",
    "#                             'rhosat_msis2'  : rhosat_msis2,  ### MSIS2 density @Satellite Altitude\n",
    "#                             'D400_msis00'   : D400_msis00,   ### density normalized to 400km with msis00\n",
    "#                             'rho400_msis2'  : rho400_msis2,  ### MSIS2 density @400km \n",
    "#                             'D400_msis2'    : D400_msis2,    ### density normalized to 400km with msis2\n",
    "#                             #\n",
    "#                             'rhosat_tiegcm'  : rhosat_tiegcm,  ### Tiegcm density @Satellite Altitude\n",
    "#                             'rho400_tiegcm'  : rho400_tiegcm,  ### Tiegcm density @400km \n",
    "#                             'D400_tiegcm'    : D400_tiegcm,    ### density normalized to 400km with Tiegcm\n",
    "#                             #                             'Cd'            : Cd,\n",
    "#     #                         'Ap_dayvals'    : Ap_dayvals,\n",
    "#     #                         'f107a_dayvals' : f107a_dayvals,\n",
    "#     #                         'f107d_dayvals' : f107d_dayvals,\n",
    "#                   } )\n",
    "\n",
    "#     df.to_pickle('parallelize/RenormChampWithMSIS2_'+str(year))\n",
    "\n",
    "    \n",
    "#     return(df)    \n",
    "\n",
    "\n",
    "\n",
    "# # import pandas as pd\n",
    "# # import numpy as np \n",
    "# # import sys  \n",
    "# # from scipy.io import loadmat  #allows us to read in .mat files\n",
    "# # from datetime import datetime, timedelta\n",
    "\n",
    "# # #### MAKE MSIS Take the 3HR Ap values\n",
    "# # from pymsis import msis\n",
    "# # SWI_option = [1.0]*25\n",
    "# # SWI_option[8] = -1.0\n",
    "# #             #  C    AP - MAGNETIC INDEX(DAILY) OR WHEN SW(9)=-1. :\n",
    "# #             #  C      - ARRAY CONTAINING:\n",
    "# #             #  C       (1) DAILY AP\n",
    "# #             #  C       (2) 3 HR AP INDEX FOR CURRENT TIME\n",
    "# #             #  C       (3) 3 HR AP INDEX FOR 3 HRS BEFORE CURRENT TIME\n",
    "# #             #  C       (4) 3 HR AP INDEX FOR 6 HRS BEFORE CURRENT TIME\n",
    "# #             #  C       (5) 3 HR AP INDEX FOR 9 HRS BEFORE CURRENT TIME\n",
    "# #             #  C       (6) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 12 TO 33 HRS PRIOR\n",
    "# #             #  C          TO CURRENT TIME\n",
    "# #             #  C       (7) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 36 TO 57 HRS PRIOR\n",
    "# #             #  C          TO CURRENT TIME\n",
    "\n",
    "# # import sys  \n",
    "# # sys.path.insert(0, 'util_funcs/')\n",
    "# # from read_CHAMP_data import get_CHAMP_data\n",
    "\n",
    "# # # #################################\n",
    "# # # years =  [2002]#, 2003, 2004, 2005, 2006, 2007, 2008, 2009]\n",
    "# # # days = np.arange(1,2)\n",
    "# # # path_champ = '/space/DNR_data/CHAMP_2002_2010/'\n",
    "# # # #################################\n",
    "\n",
    "\n",
    "# # def make_champ_timeseries_v2_redorenorm(years, days):\n",
    "    \n",
    "# #     \"\"\"This function redoes the construction of the CHAMP timeseries for the orbit.\n",
    "    \n",
    "# #     Changes from the original include:\n",
    "# #         - Use F10.7 indicies that are scaled with MgII and re-referenced to the Earth's locatiion (instead of at 1AU)\n",
    "# #         - Use MSIS2.0 for the normalization to 400km\n",
    "# #         - Use a 3Hour Ap input as required by MSIS for the more granular Stormtime forcing option.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# #     \"\"\"    \n",
    "\n",
    "    \n",
    "# #     path_champ = '/space/DNR_data/CHAMP_2002_2010/'\n",
    "    \n",
    "    \n",
    "# #     #### Load MgII Scaled F10.7 Values:\n",
    "# #     import pickle\n",
    "# #     dir_save = '/space/DNR_data/'\n",
    "# #     filehandler = open(dir_save+'MgII_F107_KpAp'+'.pkl', 'rb') \n",
    "# #     mgII_data = pickle.load(filehandler)\n",
    "# #     filehandler.close()\n",
    "        \n",
    "# #     #### clear up some space        \n",
    "# #     truncate_date    = np.logical_and(mgII_data['Date'].year>=2000 , mgII_data['Date'].year<=2011 )\n",
    "# #     truncate_date3hr =  np.logical_and(mgII_data['Date_3hrAp'].year>=2000 , mgII_data['Date_3hrAp'].year<=2011 )#np.logical_and(mgII_data['Date_3hrAp'].year==year , mgII_data['Date_3hrAp'].dayofyear==day )\n",
    "\n",
    "# #     mgII_data['Date_3hrAp'] = mgII_data['Date_3hrAp'][truncate_date3hr]\n",
    "# #     mgII_data['Ap']         = np.array(mgII_data['Ap'])[truncate_date3hr]\n",
    "# #     mgII_data['Date']       = mgII_data['Date'][truncate_date]\n",
    "# #     mgII_data['Ap_dailyavg'] =  np.array(mgII_data['Ap_dailyavg'])[truncate_date]\n",
    "# #     mgII_data['f107d_earth'] = np.array(mgII_data['f107d_earth'])[truncate_date]\n",
    "# #     mgII_data['f107a_earth'] = np.array(mgII_data['f107a_earth'])[truncate_date]\n",
    "\n",
    "# #     del mgII_data['DOY']\n",
    "# #     del mgII_data['kp']\n",
    "# #     del mgII_data['f107d']\n",
    "# #     del mgII_data['f107a']\n",
    "# #     del mgII_data['year_day']\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# #     #     noaa = pd.read_pickle('/space/DNR_data/noaa_2002_2010_pickle')\n",
    "# #     #     noaa['f107d'][noaa['f107d'].astype(float) <= 60] = noaa['f107a'][noaa['f107d'].astype(float) <= 60]\n",
    "\n",
    "# #     tleng = 0\n",
    "# #     time_full= []\n",
    "# #     Year         = []\n",
    "# #     Doy          = []\n",
    "# #     Hours        = []\n",
    "# #     Lon          = []\n",
    "# #     Lat          = []\n",
    "# #     LatBin       = []\n",
    "# #     Height       = []\n",
    "# #     LocTim       = []\n",
    "# #     CHAMPDensity = []\n",
    "# #     D400_msis00  = []  # normalized quantity\n",
    "# #     rhosat_msis00= []\n",
    "# #     rhosat_msis2 = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "# #     rho400_msis2 = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "# #     D400_msis2   = np.ones(2000*np.size(years)*np.size(days))*np.nan\n",
    "# # #     Cd           = []\n",
    "# #     # Ap_dayvals   = []\n",
    "# #     # f107a_dayvals= []\n",
    "# #     # f107d_dayvals= []\n",
    "# #     date = []\n",
    "\n",
    "# #     i = 0\n",
    "# #     for iyear,year in enumerate(years):\n",
    "# #         for iday,day in enumerate(days):\n",
    "# #             champ, breakloop = get_CHAMP_data(path_champ, year, day) \n",
    "# #             if breakloop == False:\n",
    "# #                 leng = np.size(champ,0)\n",
    "# #     #                 date_index = datetime(year, 1, 1) + timedelta(float(day)) \n",
    "# #     #                 Ap_old    = float(noaa['Ap'][date_index])\n",
    "# #     #                 f107a_old = float(noaa['f107a'][date_index])\n",
    "# #     #                 f107d_old = float(noaa['f107d'][date_index])\n",
    "# #                 leng2 = leng+tleng\n",
    "\n",
    "# #                 Year[tleng:leng2]          = champ['Year'][:leng]\n",
    "# #                 Doy[tleng:leng2]           = champ['Doy'][:leng]\n",
    "# #                 Hours[tleng:leng2]         = champ['Hours'][:leng]\n",
    "# #                 Lon[tleng:leng2]           = champ['Lon'][:leng]\n",
    "# #                 Lat[tleng:leng2]           = champ['Lat'][:leng]\n",
    "# #                 LatBin[tleng:leng2]        = champ['LatBin'][:leng]\n",
    "# #                 Height[tleng:leng2]        = champ['Height'][:leng]\n",
    "# #                 LocTim[tleng:leng2]        = champ['LocTim'][:leng]\n",
    "# #                 CHAMPDensity[tleng:leng2]  = champ['Density'][:leng]\n",
    "# #                 D400_msis00[tleng:leng2]   = champ['D400'][:leng]\n",
    "# #                 rhosat_msis00[tleng:leng2] = champ['Dmsis'][:leng]\n",
    "# # #                 Cd[tleng:leng2]            = champ['Cd'][:leng]\n",
    "\n",
    "\n",
    "# #                 ####---------------------------------------------\n",
    "# #                 #### Gather the Necessary Flux and Ap Information\n",
    "# #                 index_date = np.logical_and(mgII_data['Date'].year==year , mgII_data['Date'].dayofyear==day )\n",
    "# #                 f107a = float(np.squeeze(np.asarray(mgII_data['f107a_earth'])[index_date]))\n",
    "# #                 f107d = float(np.squeeze(np.asarray(mgII_data['f107d_earth'])[index_date]))\n",
    "# #                 Ap_daily_avg = float(np.squeeze(np.asarray(mgII_data['Ap_dailyavg'])[index_date]))\n",
    "\n",
    "\n",
    "# #                 ### Construct the necessary 3hr Ap Array to go into MSIS\n",
    "# #     #             print('---------------------------------------------')\n",
    "# #                 print(f\"Year: {year} / Day: {day}\")\n",
    "# #     #             daily_avg_ap = np.mean(Ap)\n",
    "# #                 for it, itime in enumerate( champ['Hours'][:leng]):\n",
    "# #                     date.append(datetime(year, 1, 1) + timedelta(days = float(day)-1,  hours = itime )) \n",
    "\n",
    "# #                     lon   = champ['Lon'][it]\n",
    "# #                     lat   = champ['Lat'][it]\n",
    "# #                     dates = datetime(year, 1, 1) + timedelta(float(day) - 1) + timedelta(hours = champ['Hours'][it]) \n",
    "# #                     f107din = [f107d]\n",
    "# #                     f107ain = [f107a]\n",
    "\n",
    "# #                     index_date3hr = np.logical_and(mgII_data['Date_3hrAp'].year==year , mgII_data['Date_3hrAp'].dayofyear==day )\n",
    "# #                     indexvals =  [i for i, x in enumerate(index_date3hr) if x]\n",
    "# #                     Ap_doy_windows = mgII_data['Date_3hrAp'][indexvals]\n",
    "\n",
    "# #                     #### Find the Current 3hr Kp window:A\n",
    "# #                     Ap_windw_hrs = [i.hour for i in Ap_doy_windows]\n",
    "# #                     Ap_windw_hrs = np.append(np.array(Ap_windw_hrs),24)  ## add the final window edge\n",
    "\n",
    "# #                     index_current_Ap = int(np.digitize([dates.hour],Ap_windw_hrs))\n",
    "# #                     if index_current_Ap==8:\n",
    "# #                         index_current_Ap += -1\n",
    "# #                     indexglobal_currentAp = indexvals[index_current_Ap]\n",
    "\n",
    "# #                     Ap_3HR_current        = mgII_data['Ap'][indexglobal_currentAp]\n",
    "# #                     Ap_3HR_prior          = mgII_data['Ap'][indexglobal_currentAp-1]\n",
    "# #                     Ap_6HR_prior          = mgII_data['Ap'][indexglobal_currentAp-2]\n",
    "# #                     Ap_9HR_prior          = mgII_data['Ap'][indexglobal_currentAp-3]\n",
    "# #                     Ap_12hr_33hr_priorAVG = np.mean(mgII_data['Ap'][indexglobal_currentAp-11 :indexglobal_currentAp-3 ] ) ### 33hrs to 12 hours\n",
    "# #                     Ap_36hr_57hr_priorAVG = np.mean(mgII_data['Ap'][indexglobal_currentAp-19 :indexglobal_currentAp-11 ])  ### 36hrs to 57 hours\n",
    "\n",
    "# #                     #### CHECK THE DATES!!!!\n",
    "# #                     # print(f'      Current Date: {dates}')\n",
    "# #                     # print('3HR_current ', mgII_data['Date_3hrAp'][indexglobal_currentAp])\n",
    "# #                     # print('3HR_prior ', mgII_data['Date_3hrAp'][indexglobal_currentAp-1])\n",
    "# #                     # print('6HR_prior ', mgII_data['Date_3hrAp'][indexglobal_currentAp-2])\n",
    "# #                     # print('9HR_prior ', mgII_data['Date_3hrAp'][indexglobal_currentAp-3])\n",
    "# #                     # print('12hr_33hr_priorAVG ', mgII_data['Date_3hrAp'][indexglobal_currentAp-11 :indexglobal_currentAp-3 ]  )\n",
    "# #                     # print('36hr_57hr_priorAVG ', mgII_data['Date_3hrAp'][indexglobal_currentAp-19 :indexglobal_currentAp-11 ] )\n",
    "\n",
    "# #                     apsin = [[Ap_daily_avg,          # (1) DAILY AP\n",
    "# #                               Ap_3HR_current,        # (2) 3 HR AP INDEX FOR CURRENT TIME\n",
    "# #                               Ap_3HR_prior,          # (3) 3 HR AP INDEX FOR 3 HRS BEFORE CURRENT TIME\n",
    "# #                               Ap_6HR_prior,          # (4) 3 HR AP INDEX FOR 6 HRS BEFORE CURRENT TIME\n",
    "# #                               Ap_9HR_prior,          # (5) 3 HR AP INDEX FOR 9 HRS BEFORE CURRENT TIME\n",
    "# #                               Ap_12hr_33hr_priorAVG, # (6) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 12 TO 33 HRS PRIOR\n",
    "# #                               Ap_36hr_57hr_priorAVG]]# (7) AVERAGE OF EIGHT 3 HR AP INDICIES FROM 36 TO 57 HRS PRIOR\n",
    "\n",
    "# #                     output2_sat = msis.run(dates, lon, lat, champ['Height'][it], f107din, f107ain, apsin, version = 2, options=SWI_option)\n",
    "# #                     output2_400 = msis.run(dates, lon, lat, 400, f107din, f107ain, apsin                , version = 2, options=SWI_option)\n",
    "\n",
    "\n",
    "# #                     ### add the values to the growing lists\n",
    "# #                     rhosat_msis2[tleng+it] = output2_sat[0,0,0,0][0]\n",
    "# #                     rho400_msis2[tleng+it] = output2_400[0,0,0,0][0]\n",
    "# #                     D400_msis2[tleng+it]   = champ['Density'][it] * (output2_400[0,0,0,0][0] / output2_sat[0,0,0,0][0])   # normalized density to 400km\n",
    "\n",
    "# #                 tleng = tleng + leng\n",
    "# #     #                 print(year,'/',day)\n",
    "# #                 i+=1\n",
    "\n",
    "# #             elif breakloop == True:\n",
    "# #                 i+=1\n",
    "# #                 continue\n",
    "\n",
    "# #     rhosat_msis2 = rhosat_msis2[~np.isnan(rhosat_msis2)]\n",
    "# #     rho400_msis2 = rho400_msis2[~np.isnan(rho400_msis2)]\n",
    "# #     D400_msis2 = D400_msis2[~np.isnan(D400_msis2)]\n",
    "\n",
    "# #     df = pd.DataFrame(data={'Date' :date ,\n",
    "# #                             'Year'          : Year,\n",
    "# #                             'Doy'           : Doy,\n",
    "# #                             'Hours'         : Hours,\n",
    "# #                             'Lon'           : Lon,\n",
    "# #                             'Lat'           : Lat, \n",
    "# #                             'LatBin'        : LatBin,\n",
    "# #                             'Height'        : Height,\n",
    "# #                             'LocTim'        : LocTim,\n",
    "# #                             'CHAMPDensity'  : CHAMPDensity,\n",
    "# #                             'rhosat_msis00' : rhosat_msis00, ### MSIS00 density @Satellite Altitude\n",
    "# #                             'rhosat_msis2'  : rhosat_msis2,  ### MSIS2 density @Satellite Altitude\n",
    "# #                             'D400_msis00'   : D400_msis00,   ### density normalized to 400km with msis00\n",
    "# #                             'rho400_msis2'  : rho400_msis2,  ### MSIS2 density @400km \n",
    "# #                             'D400_msis2'    : D400_msis2,    ### density normalized to 400km with msis2\n",
    "# # #                             'Cd'            : Cd,\n",
    "# #     #                         'Ap_dayvals'    : Ap_dayvals,\n",
    "# #     #                         'f107a_dayvals' : f107a_dayvals,\n",
    "# #     #                         'f107d_dayvals' : f107d_dayvals,\n",
    "# #                   } )\n",
    "\n",
    "# #     df.to_pickle('parallelize/RenormChampWithMSIS2_'+str(year))\n",
    "# # #     df.to_pickle('constructed_files/RenormChampWithMSIS2_'+year)\n",
    "\n",
    "    \n",
    "# #     return    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "428.812px",
    "left": "822.875px",
    "right": "20px",
    "top": "202px",
    "width": "504px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
